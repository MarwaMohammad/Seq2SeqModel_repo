{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence Models:\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Sequence to Sequence Architecture is one of the important architectures in the neural network. It helps in tasks like Machine translation, text generation, chatbot, ..etc.\n",
    "\n",
    "The Sequence to Sequence architecture is named as Seq2Seq modeling because it tries to convert one sequence into another.\n",
    "\n",
    "Let's explain an example to understand more, in Machine Translation, we feed to the network input an Arabic sentence and obtain an English sentence from the output. So, here is the Seq2Seq modeling output English sentence from Arabic sentence. Although the Arabic letters are in another format which is completely different from English letters. How the Seq2Seq knew the sentence in English from Arabic?\n",
    "\n",
    "Good question! let's go deeper to know how Seq2Seq does that for us. To answer this question we should know the Seq2Seq architecture.\n",
    "    \n",
    "## Seq2Seq Architecture\n",
    "\n",
    "Seq2Seq architecture consists of:\n",
    "- Encoder and its output context vector.\n",
    "- Decoder.\n",
    "\n",
    "The Encoder and decoder together have been the following shape:\n",
    "\n",
    "![title](img/encoder_decoder_arch.png)\n",
    "\n",
    "\n",
    "\n",
    "### Encoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Job:\n",
    "\n",
    "This is the 1st component in the encoder-decoder architecture.  It builds the representation of the input vector and embeds the input meaning inside the context vector.\n",
    "\n",
    "- Architecture:\n",
    "\n",
    "It can be built using the LSTM, RNN, GRU, BiLSTM. The last hidden state of the encoder will hold the context of the entire input sentence. The encoder tries to build and embedding for the entire input sentence.\n",
    "\n",
    "- How does it work?\n",
    "\n",
    "To be able to imagine how it works, please look at the image below:\n",
    "\n",
    "![title](img/encoder.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the above image, the embedding of input words is fed into LSTM stack sequentially, then each cell hidden state is fed into the next LSTM until we reach to the last one where we obtain the context vector from the last cell hidden state.\n",
    "\n",
    "- Using Keras: \n",
    "\n",
    "Feeding the encoder last hidden state using keras, just enable the return_state to enable the LSTM statck to return the last hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "- Job:\n",
    "\n",
    "Generally, 'it decodes the context vector to have the same required output'. In out example, It builds the English representation from the Arabic words encoded inside the context vector. \n",
    "\n",
    "- Architecture:\n",
    "\n",
    "Also, It can be built using RNN, LSTM, GRU.\n",
    "\n",
    "- How does it work?\n",
    "\n",
    "To be able to understand the decoder well, we should realize that the decoder has two methods of working according to the  stage. There are two stages in any machine learning algorithm training, inference or prediction. The decoder work is a little bit difrent in training and inference. \n",
    "\n",
    "#### Decorder in Training\n",
    "\n",
    "\n",
    "![title](img/decoder_Train.png)\n",
    "\n",
    "\n",
    "As in the above image, the decoder the decoder receives the context vector from the encoder, and start token to start its work to predict the 1st word according to the the received context vector. The start token will be input to the 1st cell in the decoder. But the 2nd LSTM cell as in above image will receive the correct token from training data not from the previous LSTM output as in infernece 'As we will see later'. This input token from training data will help the 2nd LSTM with the previous hidden state and cell state to predict the 2nd word correctly. Then then the 2nd LSTM output the predcited 3rd token and its feed its hidden and cell state to the 3rd LSTM. This senario will be repeated unitl the decoder see end token or the reach to the maximum length.\n",
    "\n",
    "\n",
    "#### Decoder in inference or prediction\n",
    "\n",
    "![title](img/decoder_inference.png)\n",
    "\n",
    "As in the above image, the decoder receives the context vector from the encoder. The decoder 1st cell input starts, once the cell sees start token inside the input, it starts to predict the 1st word according to the received context vector. Then this word is fed to the next LSTM as input to help it decode the second word with the help of the hidden state that comes from the previous LSTM. The decoder works with this way until it reaches to the max_length or until it finds the word 'end'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Translation using Seq2Seq Modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting camel-tools\n",
      "  Downloading camel_tools-1.2.0.tar.gz (58 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting future\n",
      "  Using cached future-0.18.2-py3-none-any.whl\n",
      "Requirement already satisfied: six in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from camel-tools) (1.15.0)\n",
      "Collecting docopt\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: cachetools in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from camel-tools) (4.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from camel-tools) (1.19.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from camel-tools) (1.6.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\marwa\\appdata\\roaming\\python\\python38\\site-packages (from camel-tools) (1.1.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from camel-tools) (0.24.1)\n",
      "Collecting dill\n",
      "  Using cached dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "Requirement already satisfied: torch>=1.3 in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from camel-tools) (1.8.1)\n",
      "Requirement already satisfied: transformers>=3.0.2 in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from camel-tools) (4.15.0)\n",
      "Collecting editdistance\n",
      "  Downloading editdistance-0.6.0-cp38-cp38-win_amd64.whl (24 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\marwa\\appdata\\roaming\\python\\python38\\site-packages (from camel-tools) (2.25.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torch>=1.3->camel-tools) (4.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers>=3.0.2->camel-tools) (3.4.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers>=3.0.2->camel-tools) (2021.4.4)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers>=3.0.2->camel-tools) (0.10.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers>=3.0.2->camel-tools) (4.54.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers>=3.0.2->camel-tools) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers>=3.0.2->camel-tools) (21.3)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers>=3.0.2->camel-tools) (0.0.47)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers>=3.0.2->camel-tools) (0.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas->camel-tools) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\marwa\\appdata\\roaming\\python\\python38\\site-packages (from pandas->camel-tools) (2020.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\marwa\\appdata\\roaming\\python\\python38\\site-packages (from requests->camel-tools) (1.26.2)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\marwa\\appdata\\roaming\\python\\python38\\site-packages (from requests->camel-tools) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->camel-tools) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\marwa\\appdata\\roaming\\python\\python38\\site-packages (from requests->camel-tools) (2.10)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn->camel-tools) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn->camel-tools) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from packaging>=20.0->transformers>=3.0.2->camel-tools) (2.4.7)\n",
      "Requirement already satisfied: click in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from sacremoses->transformers>=3.0.2->camel-tools) (8.0.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from click->sacremoses->transformers>=3.0.2->camel-tools) (0.4.4)\n",
      "Building wheels for collected packages: camel-tools, docopt\n",
      "  Building wheel for camel-tools (setup.py): started\n",
      "  Building wheel for camel-tools (setup.py): finished with status 'done'\n",
      "  Created wheel for camel-tools: filename=camel_tools-1.2.0-py3-none-any.whl size=99029 sha256=0c626af20eb25c7722f2c8d404adbe823fc00f6b26ed67f7e56c6989cadd53e0\n",
      "  Stored in directory: c:\\users\\marwa\\appdata\\local\\pip\\cache\\wheels\\b8\\3b\\9f\\910d7d11709d8be2fb2fe4ced865d3b5a7d0c5de2fa10b4823\n",
      "  Building wheel for docopt (setup.py): started\n",
      "  Building wheel for docopt (setup.py): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=0646051b2aefec8475a9e022ce341994ae41b40768081ec283a16cacc765e1fa\n",
      "  Stored in directory: c:\\users\\marwa\\appdata\\local\\pip\\cache\\wheels\\56\\ea\\58\\ead137b087d9e326852a851351d1debf4ada529b6ac0ec4e8c\n",
      "Successfully built camel-tools docopt\n",
      "Installing collected packages: future, editdistance, docopt, dill, camel-tools\n",
      "Successfully installed camel-tools-1.2.0 dill-0.3.4 docopt-0.6.2 editdistance-0.6.0 future-0.18.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 22.0.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "#pip install camel-tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense,LSTM,Embedding,Input\n",
    "import io\n",
    "from string import digits\n",
    "import string\n",
    "import tkseem as tk\n",
    "# instantiate the Maximum Likelihood Disambiguator\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "from camel_tools.disambig.mle import MLEDisambiguator\n",
    "from camel_tools.tokenizers.morphological import MorphologicalTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining the dataset for Arabic and English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file):\n",
    "    file_data = []\n",
    "    data = []\n",
    "    Arabic_data = []\n",
    "    english_data = []\n",
    "    # Read the file lines\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        file_data = f.readlines()\n",
    "    # separate the lines using '\\t'\n",
    "    for line in (file_data):\n",
    "        english_sent, arabic_sent, _ = line.split('\\t')\n",
    "        Arabic_data.append(arabic_sent)\n",
    "        english_data.append(english_sent)\n",
    "        \n",
    "    return english_data, Arabic_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'datasets/ara-eng/ara.txt'\n",
    "english_data,  Arabic_data= read_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi.',\n",
       " 'Run!',\n",
       " 'Duck!',\n",
       " 'Duck!',\n",
       " 'Duck!',\n",
       " 'Help!',\n",
       " 'Jump!',\n",
       " 'Stop!',\n",
       " 'Stop!',\n",
       " 'Wait!']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['إلى اللقاء',\n",
       " 'إنتظر',\n",
       " 'لقد أتى.',\n",
       " 'هو يجري',\n",
       " 'ساعدني!',\n",
       " 'النجدة! ساعدني!',\n",
       " 'ساعدوني',\n",
       " 'انتظر.',\n",
       " 'أنا موافق',\n",
       " 'أنا حزين.',\n",
       " 'أنا أيضاً.',\n",
       " 'اخرس!',\n",
       " 'اصمت!',\n",
       " 'اسكت!',\n",
       " 'أغلق فمك!',\n",
       " 'أوقفه',\n",
       " 'خذه',\n",
       " 'أخبرني',\n",
       " 'توم فاز.',\n",
       " 'لقد ربح توم.',\n",
       " 'استيقظ!',\n",
       " 'أهلاً و سهلاً!',\n",
       " 'مرحباً بك!',\n",
       " 'اهلا وسهلا',\n",
       " 'مرحبا!',\n",
       " 'من فاز؟',\n",
       " 'من الذي ربح؟',\n",
       " 'لم لا؟',\n",
       " 'لما لا؟',\n",
       " 'لا فكرة لدي',\n",
       " 'استمتع بوقتك.',\n",
       " 'أسرعا.',\n",
       " 'لقد نسيت.',\n",
       " 'فهمتُهُ.',\n",
       " 'فهمتُها.',\n",
       " 'فَهمتُ ذلك.',\n",
       " 'أستخدمه.',\n",
       " 'سأدفع أنا.',\n",
       " 'أنا مشغول.',\n",
       " 'إنني مشغول.',\n",
       " 'أشعر بالبرد.',\n",
       " 'أنا حُرّ.',\n",
       " 'أنا هنا',\n",
       " 'لقد عدت إلى البيت',\n",
       " 'أنا فقير.',\n",
       " 'أنا ثري.',\n",
       " 'هذا مؤلم',\n",
       " 'انها جافه',\n",
       " 'الجو حار',\n",
       " 'إنه جديد']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Arabic_data[50:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12158"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Arabic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12158"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 12158 line in both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame({'Arabic_input':Arabic_data, 'English_target':english_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Arabic_input</th>\n",
       "      <th>English_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>مرحبًا.</td>\n",
       "      <td>Hi.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>اركض!</td>\n",
       "      <td>Run!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>اخفض رأسك!</td>\n",
       "      <td>Duck!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>اخفضي رأسك!</td>\n",
       "      <td>Duck!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>اخفضوا رؤوسكم!</td>\n",
       "      <td>Duck!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Arabic_input English_target\n",
       "0         مرحبًا.            Hi.\n",
       "1           اركض!           Run!\n",
       "2      اخفض رأسك!          Duck!\n",
       "3     اخفضي رأسك!          Duck!\n",
       "4  اخفضوا رؤوسكم!          Duck!"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arabic Normalization\n",
    "The Arabic has a special preprocessing, why?\n",
    "\n",
    "Arabic has difirrent characteristics like:\n",
    "- The word in Arabic can mean a complete sentence in other languages. So, it requires a special segmentation step which considers the Arabic language rules. \n",
    "- Arabic has diactrics which should be normalized. \n",
    "- Some Aabic letters has more than one shape so, it should be unified.\n",
    "\n",
    "If you would like to know more about Arabic charasteristics, please read this part  \"Arabic Challenges in the Context of NER\" in the the following paper:\n",
    "\n",
    "https://thescipub.com/pdf/jcssp.2020.117.125.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the following repo: https://github.com/motazsaad/process-arabic-text/blob/master/clean_arabic_text.py\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = arabic_punctuations + english_punctuations\n",
    "\n",
    "arabic_diacritics = re.compile(\"\"\"\n",
    "                             ّ    | # Tashdid\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "\n",
    "\n",
    "def normalize_arabic(text):\n",
    "    #text = re.sub(\"[إأآا]\", \"ا\", text)\n",
    "\n",
    "    #text = re.sub(\"ى\", \"ي\", text)\n",
    "    #text = re.sub(\"ؤ\", \"ء\", text)\n",
    "    #text = re.sub(\"ئ\", \"ء\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    text = re.sub(\"گ\", \"ك\", text)\n",
    "    return text\n",
    "\n",
    "def remove_digits(text):\n",
    "    text = re.sub(r\"[1234567890١٢٣٤٥٦٧٨٩٠]+\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def remove_english_characters(text):\n",
    "    text = re.sub(r'[a-zA-Z]+','',text)\n",
    "    return text\n",
    "    \n",
    "\n",
    "def remove_diacritics(text):\n",
    "    text = re.sub(arabic_diacritics, '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Arabic_normalization(Arabic_sentence_list):\n",
    "    Arabic_data_list = []\n",
    "    for item in Arabic_sentence_list:\n",
    "        text = remove_english_characters(item)\n",
    "        text = remove_digits(text)\n",
    "        text = normalize_arabic(text)\n",
    "        text = remove_diacritics(text)\n",
    "        text = remove_punctuations(text)\n",
    "        Arabic_data_list.append(text)\n",
    "        \n",
    "    return Arabic_data_list    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Arabic_input'] = Arabic_normalization(dataset.Arabic_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            مرحبا\n",
       "1             اركض\n",
       "2        اخفض رأسك\n",
       "3       اخفضي رأسك\n",
       "4    اخفضوا رؤوسكم\n",
       "5           النجده\n",
       "6             اقفز\n",
       "7               قف\n",
       "8            توقف \n",
       "9            إنتظر\n",
       "Name: Arabic_input, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Arabic_input'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English Normalization:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def English_normalization(English_target_ls):\n",
    "    English_data_list = []\n",
    "    # Since we work on word level, if we normalize the text to lower case, this will reduce the vocabulary. \n",
    "    #It's easy to recover the case later. \n",
    "    English_data_list = English_target_ls.apply(lambda x: x.lower())\n",
    "\n",
    "    # Clean up punctuations and digits. Such special chars are common to both domains, and can just be copied with no error.\n",
    "    exclude = set(string.punctuation)\n",
    "    English_data_list = English_data_list.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    English_data_list = English_data_list.apply(lambda x: x.translate(remove_digits))\n",
    "    \n",
    "    return English_data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['English_target'] = English_normalization(dataset.English_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Arabic_input</th>\n",
       "      <th>English_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>مرحبا</td>\n",
       "      <td>hi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>اركض</td>\n",
       "      <td>run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>اخفض رأسك</td>\n",
       "      <td>duck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>اخفضي رأسك</td>\n",
       "      <td>duck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>اخفضوا رؤوسكم</td>\n",
       "      <td>duck</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Arabic_input English_target\n",
       "0          مرحبا             hi\n",
       "1           اركض            run\n",
       "2      اخفض رأسك           duck\n",
       "3     اخفضي رأسك           duck\n",
       "4  اخفضوا رؤوسكم           duck"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation:\n",
    "In this step we start to convert the data for training. \n",
    "- Adding the start and end token to the target language.Since the english is our target, so we will add the start and tokens to it. lets see how can we do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_tok = 'START_'\n",
    "end_tok = '_END'\n",
    "def data_prep():\n",
    "    dataset.English_target = dataset.English_target.apply(lambda x : st_tok + ' ' + x + ' ' + end_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Arabic_input</th>\n",
       "      <th>English_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>مرحبا</td>\n",
       "      <td>START_ hi _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>اركض</td>\n",
       "      <td>START_ run _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>اخفض رأسك</td>\n",
       "      <td>START_ duck _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>اخفضي رأسك</td>\n",
       "      <td>START_ duck _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>اخفضوا رؤوسكم</td>\n",
       "      <td>START_ duck _END</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Arabic_input    English_target\n",
       "0          مرحبا    START_ hi _END\n",
       "1           اركض   START_ run _END\n",
       "2      اخفض رأسك  START_ duck _END\n",
       "3     اخفضي رأسك  START_ duck _END\n",
       "4  اخفضوا رؤوسكم  START_ duck _END"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenization\n",
    "Arabic tokenization is completely diffrent from English tokenization. English tokenization depends on spaces, but in Arabic this is not valid. Since the token in Arabic can be used to mean a complete sentence in another languages.\n",
    "\n",
    "Note, I used her the camel_tools which is not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arabic Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "\n",
    "def tokenize_Arabic():\n",
    "    # The tokenizer expects pre-tokenized text\n",
    "    Arabic_input_ls = dataset.Arabic_input.apply(simple_word_tokenize)\n",
    "\n",
    "    # Load a pretrained disambiguator to use with a tokenizer\n",
    "    mle = MLEDisambiguator.pretrained('calima-msa-r13')\n",
    "\n",
    "    # By specifying `split=True`, the morphological tokens are output as seperate\n",
    "    # strings.\n",
    "    tokenizer = MorphologicalTokenizer(mle,scheme='d3tok', split=True)\n",
    "    Arabic_input_ls = Arabic_input_ls.apply(tokenizer.tokenize)\n",
    "    \n",
    "    return Arabic_input_ls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_plus(tokens):\n",
    "    sentence_ls = []\n",
    "    for token in tokens:\n",
    "        if '+' in token:\n",
    "            token_without_plus = token.replace('+','')\n",
    "            sentence_ls.append(token_without_plus) \n",
    "        else:\n",
    "            sentence_ls.append(token) \n",
    "\n",
    "            \n",
    "    return sentence_ls\n",
    "#Arabic_input_ls = Arabic_input_ls.apply(remove_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Arabic_input</th>\n",
       "      <th>English_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>مرحبا</td>\n",
       "      <td>START_ hi _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>اركض</td>\n",
       "      <td>START_ run _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>اخفض رأسك</td>\n",
       "      <td>START_ duck _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>اخفضي رأسك</td>\n",
       "      <td>START_ duck _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>اخفضوا رؤوسكم</td>\n",
       "      <td>START_ duck _END</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Arabic_input    English_target\n",
       "0          مرحبا    START_ hi _END\n",
       "1           اركض   START_ run _END\n",
       "2      اخفض رأسك  START_ duck _END\n",
       "3     اخفضي رأسك  START_ duck _END\n",
       "4  اخفضوا رؤوسكم  START_ duck _END"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds_copy = dataset.copy()\n",
    "tokenized_ds_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The camel tool returned the hamza letter again but in a unified way for all the words. I mean the same word can't exist in two difrrent spellings.\n",
    "#### English tokenization\n",
    "English is tokenized according to spaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_split_word2word(data):\n",
    "    return data.split(' ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_stats(tokenized_ds_copy):\n",
    "    #Obtain the tokenized words in Arabic\n",
    "    tokenized_ds_copy['Arabic_input'] = tokenize_Arabic()\n",
    "    # The tokenization output has + in the separated token, which should be removed\n",
    "    tokenized_ds_copy['Arabic_input'] = tokenized_ds_copy.Arabic_input.apply(remove_plus)\n",
    "    \n",
    "    #create a set to hold all Arabic words uniquely.\n",
    "    input_tokens=set()\n",
    "    for item in tokenized_ds_copy.Arabic_input:\n",
    "        for tok in item:\n",
    "            input_tokens.add(tok)\n",
    "    \n",
    "    #Obtain the tokenized words in English dataset\n",
    "    tokenized_ds_copy['English_target'] = tokenized_ds_copy.English_target.apply(tok_split_word2word)\n",
    "    \n",
    "    #create a set to hold all English words uniquely.\n",
    "    target_tokens=set()\n",
    "    for item in tokenized_ds_copy.English_target:\n",
    "        for tok in item:\n",
    "            target_tokens.add(tok)\n",
    "        \n",
    "    input_tokens = sorted(list(input_tokens))\n",
    "    target_tokens = sorted(list(target_tokens))\n",
    "\n",
    "\n",
    "    \n",
    "    num_encoder_tokens = len(input_tokens)\n",
    "    num_decoder_tokens = len(target_tokens)\n",
    "    \n",
    "    #To obtin the maximum number of words inside Arabic and English dataset.\n",
    "    max_encoder_seq_length = np.max([len(l) for l in tokenized_ds_copy.Arabic_input])\n",
    "    max_decoder_seq_length = np.max([len(l) for l in tokenized_ds_copy.English_target])\n",
    "\n",
    "    return input_tokens, target_tokens, num_encoder_tokens, num_decoder_tokens, max_encoder_seq_length, max_decoder_seq_length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens, target_tokens, num_encoder_tokens, num_decoder_tokens, max_encoder_seq_length, max_decoder_seq_length  = data_stats(tokenized_ds_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 12158\n",
      "Number of unique input tokens: 7205\n",
      "Number of unique output tokens: 4298\n",
      "Max sequence length for inputs: 52\n",
      "Max sequence length for outputs: 36\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples:', len(dataset))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "\n",
    "In this step we will build our vocab2int table which will be used to map between words and their indices.'Machine Learning can't work directly with words since computer doen't understand words, so it should be converted into numbers'.\n",
    "\n",
    "Note that the pad and separation should be considered during obtaining the vocab2int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_tok = 'PAD'\n",
    "sep_tok = ' '\n",
    "special_tokens = [pad_tok, sep_tok, st_tok, end_tok] \n",
    "\n",
    "#Increase the number of token by the number of special characters.\n",
    "num_encoder_tokens += len(special_tokens)\n",
    "num_decoder_tokens += len(special_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab(input_tokens, target_tokens):\n",
    "    input_token_index = {}\n",
    "    target_token_index = {}\n",
    "    for i,tok in enumerate(special_tokens):\n",
    "        input_token_index[tok] = i\n",
    "        target_token_index[tok] = i \n",
    "\n",
    "    offset = len(special_tokens)\n",
    "    for i, tok in enumerate(input_tokens):\n",
    "        input_token_index[tok] = i+offset\n",
    "\n",
    "    for i, tok in enumerate(target_tokens):\n",
    "        target_token_index[tok] = i+offset\n",
    "   \n",
    "    # Reverse-lookup token index to decode sequences back to something readable.\n",
    "    reverse_input_tok_index = dict(\n",
    "        (i, tok) for tok, i in input_token_index.items())\n",
    "    reverse_target_tok_index = dict(\n",
    "        (i, tok) for tok, i in target_token_index.items())\n",
    "    return input_token_index, target_token_index, reverse_input_tok_index, reverse_target_tok_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index, target_token_index, reverse_input_tok_index, reverse_target_tok_index = vocab(input_tokens, target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PAD': 0,\n",
       " ' ': 1,\n",
       " 'START_': 2,\n",
       " '_END': 3,\n",
       " 'NOAN': 4,\n",
       " 'آب': 5,\n",
       " 'آباء': 6,\n",
       " 'آبد': 7,\n",
       " 'آبقو': 8,\n",
       " 'آت': 9,\n",
       " 'آتون': 10,\n",
       " 'آتي': 11,\n",
       " 'آثار': 12,\n",
       " 'آخذ': 13,\n",
       " 'آخر': 14,\n",
       " 'آخرة': 15,\n",
       " 'آخرعلي': 16,\n",
       " 'آخرون': 17,\n",
       " 'آخرين': 18,\n",
       " 'آدم': 19,\n",
       " 'آذار': 20,\n",
       " 'آذان': 21,\n",
       " 'آذيتم': 22,\n",
       " 'آراء': 23,\n",
       " 'آرية': 24,\n",
       " 'آسفون': 25,\n",
       " 'آسيا': 26,\n",
       " 'آفاق': 27,\n",
       " 'آكل': 28,\n",
       " 'آلاف': 29,\n",
       " 'آلام': 30,\n",
       " 'آلة': 31,\n",
       " 'آلن': 32,\n",
       " 'آلي': 33,\n",
       " 'آليا': 34,\n",
       " 'آمال': 35,\n",
       " 'آمل': 36,\n",
       " 'آمن': 37,\n",
       " 'آمنة': 38,\n",
       " 'آن': 39,\n",
       " 'آنا': 40,\n",
       " 'آنذاك': 41,\n",
       " 'آنس': 42,\n",
       " 'آني': 43,\n",
       " 'آية': 44,\n",
       " 'أ': 45,\n",
       " 'أأريتها': 46,\n",
       " 'أأشتري': 47,\n",
       " 'أأنت': 48,\n",
       " 'أؤجر': 49,\n",
       " 'أؤذي': 50,\n",
       " 'أؤكد': 51,\n",
       " 'أؤلف': 52,\n",
       " 'أؤمن': 53,\n",
       " 'أإلى': 54,\n",
       " 'أاصيب': 55,\n",
       " 'أب': 56,\n",
       " 'أبإمكانك': 57,\n",
       " 'أبا': 58,\n",
       " 'أبتاع': 59,\n",
       " 'أبتز': 60,\n",
       " 'أبتسم': 61,\n",
       " 'أبتل': 62,\n",
       " 'أبحار': 63,\n",
       " 'أبحث': 64,\n",
       " 'أبد': 65,\n",
       " 'أبدأ': 66,\n",
       " 'أبدا': 67,\n",
       " 'أبدو': 68,\n",
       " 'أبدوا': 69,\n",
       " 'أبدين': 70,\n",
       " 'أبذل': 71,\n",
       " 'أبر': 72,\n",
       " 'أبرد': 73,\n",
       " 'أبريل': 74,\n",
       " 'أبشع': 75,\n",
       " 'أبعد': 76,\n",
       " 'أبق': 77,\n",
       " 'أبقار': 78,\n",
       " 'أبقى': 79,\n",
       " 'أبقيت': 80,\n",
       " 'أبقينا': 81,\n",
       " 'أبكر': 82,\n",
       " 'أبلغ': 83,\n",
       " 'أبلي': 84,\n",
       " 'أبليت': 85,\n",
       " 'أبناء': 86,\n",
       " 'أبو': 87,\n",
       " 'أبواب': 88,\n",
       " 'أبوي': 89,\n",
       " 'أبي': 90,\n",
       " 'أبيض': 91,\n",
       " 'أبيضا': 92,\n",
       " 'أبيع': 93,\n",
       " 'أبيه': 94,\n",
       " 'أتأخر': 95,\n",
       " 'أتأسف': 96,\n",
       " 'أتأكد': 97,\n",
       " 'أتابع': 98,\n",
       " 'أتبع': 99,\n",
       " 'أتت': 100,\n",
       " 'أتتطلع': 101,\n",
       " 'أتحب': 102,\n",
       " 'أتحبان': 103,\n",
       " 'أتحبني': 104,\n",
       " 'أتحبينني': 105,\n",
       " 'أتحدث': 106,\n",
       " 'أتحدثت': 107,\n",
       " 'أتحس': 108,\n",
       " 'أتخذ': 109,\n",
       " 'أتخرج': 110,\n",
       " 'أتخصص': 111,\n",
       " 'أتخطى': 112,\n",
       " 'أتخيل': 113,\n",
       " 'أتدرس': 114,\n",
       " 'أتذكر': 115,\n",
       " 'أترجم': 116,\n",
       " 'أترع': 117,\n",
       " 'أترك': 118,\n",
       " 'أتريد': 119,\n",
       " 'أتزوج': 120,\n",
       " 'أتساءل': 121,\n",
       " 'أتستطيع': 122,\n",
       " 'أتسكن': 123,\n",
       " 'أتسلق': 124,\n",
       " 'أتسلى': 125,\n",
       " 'أتسمح': 126,\n",
       " 'أتسوق': 127,\n",
       " 'أتصفح': 128,\n",
       " 'أتصل': 129,\n",
       " 'أتضح': 130,\n",
       " 'أتضن': 131,\n",
       " 'أتضور': 132,\n",
       " 'أتطلع': 133,\n",
       " 'أتظن': 134,\n",
       " 'أتظنها': 135,\n",
       " 'أتعامل': 136,\n",
       " 'أتعب': 137,\n",
       " 'أتعتقد': 138,\n",
       " 'أتعجب': 139,\n",
       " 'أتعديني': 140,\n",
       " 'أتعرف': 141,\n",
       " 'أتعلم': 142,\n",
       " 'أتعمد': 143,\n",
       " 'أتغدى': 144,\n",
       " 'أتفاجئ': 145,\n",
       " 'أتفرج': 146,\n",
       " 'أتفضل': 147,\n",
       " 'أتفق': 148,\n",
       " 'أتقبلون': 149,\n",
       " 'أتكلم': 150,\n",
       " 'أتل': 151,\n",
       " 'أتلعب': 152,\n",
       " 'أتلقف': 153,\n",
       " 'أتم': 154,\n",
       " 'أتمتلك': 155,\n",
       " 'أتمشى': 156,\n",
       " 'أتملك': 157,\n",
       " 'أتممنا': 158,\n",
       " 'أتمنتى': 159,\n",
       " 'أتمنى': 160,\n",
       " 'أتناول': 161,\n",
       " 'أتنسى': 162,\n",
       " 'أتهم': 163,\n",
       " 'أتوا': 164,\n",
       " 'أتواصل': 165,\n",
       " 'أتوتر': 166,\n",
       " 'أتوجد': 167,\n",
       " 'أتود': 168,\n",
       " 'أتوسل': 169,\n",
       " 'أتوقع': 170,\n",
       " 'أتوقف': 171,\n",
       " 'أتولى': 172,\n",
       " 'أتون': 173,\n",
       " 'أتى': 174,\n",
       " 'أتيت': 175,\n",
       " 'أتينا': 176,\n",
       " 'أتی': 177,\n",
       " 'أثاث': 178,\n",
       " 'أثار': 179,\n",
       " 'أثر': 180,\n",
       " 'أثرت': 181,\n",
       " 'أثق': 182,\n",
       " 'أثقل': 183,\n",
       " 'أثلجت': 184,\n",
       " 'أثناء': 185,\n",
       " 'أجاب': 186,\n",
       " 'أجابت': 187,\n",
       " 'أجانب': 188,\n",
       " 'أجب': 189,\n",
       " 'أجبت': 190,\n",
       " 'أجبرت': 191,\n",
       " 'أجبروا': 192,\n",
       " 'أجبن': 193,\n",
       " 'أجتاح': 194,\n",
       " 'أجد': 195,\n",
       " 'أجداد': 196,\n",
       " 'أجدد': 197,\n",
       " 'أجر': 198,\n",
       " 'أجرب': 199,\n",
       " 'أجرح': 200,\n",
       " 'أجريت': 201,\n",
       " 'أجزاء': 202,\n",
       " 'أجعل': 203,\n",
       " 'أجل': 204,\n",
       " 'أجلا': 205,\n",
       " 'أجلب': 206,\n",
       " 'أجلت': 207,\n",
       " 'أجلس': 208,\n",
       " 'أجمع': 209,\n",
       " 'أجمل': 210,\n",
       " 'أجنبية': 211,\n",
       " 'أجني': 212,\n",
       " 'أجهزة': 213,\n",
       " 'أجهشت': 214,\n",
       " 'أجيب': 215,\n",
       " 'أجيد': 216,\n",
       " 'أح': 217,\n",
       " 'أحافظ': 218,\n",
       " 'أحال': 219,\n",
       " 'أحاول': 220,\n",
       " 'أحب': 221,\n",
       " 'أحببت': 222,\n",
       " 'أحبت': 223,\n",
       " 'أحتاج': 224,\n",
       " 'أحترق': 225,\n",
       " 'أحترم': 226,\n",
       " 'أحتمل': 227,\n",
       " 'أحجز': 228,\n",
       " 'أحد': 229,\n",
       " 'أحدا': 230,\n",
       " 'أحدثت': 231,\n",
       " 'أحذ': 232,\n",
       " 'أحذر': 233,\n",
       " 'أحذية': 234,\n",
       " 'أحر': 235,\n",
       " 'أحراج': 236,\n",
       " 'أحرجت': 237,\n",
       " 'أحرز': 238,\n",
       " 'أحزن': 239,\n",
       " 'أحس': 240,\n",
       " 'أحسب': 241,\n",
       " 'أحسد': 242,\n",
       " 'أحسست': 243,\n",
       " 'أحسن': 244,\n",
       " 'أحسنت': 245,\n",
       " 'أحصل': 246,\n",
       " 'أحضر': 247,\n",
       " 'أحضرت': 248,\n",
       " 'أحفظ': 249,\n",
       " 'أحقا': 250,\n",
       " 'أحكام': 251,\n",
       " 'أحكى': 252,\n",
       " 'أحكي': 253,\n",
       " 'أحل': 254,\n",
       " 'أحلام': 255,\n",
       " 'أحلاما': 256,\n",
       " 'أحلم': 257,\n",
       " 'أحلى': 258,\n",
       " 'أحمر': 259,\n",
       " 'أحمرا': 260,\n",
       " 'أحمق': 261,\n",
       " 'أحمقا': 262,\n",
       " 'أحمل': 263,\n",
       " 'أحمي': 264,\n",
       " 'أحوال': 265,\n",
       " 'أحول': 266,\n",
       " 'أحي': 267,\n",
       " 'أحياء': 268,\n",
       " 'أحيان': 269,\n",
       " 'أحيانا': 270,\n",
       " 'أخ': 271,\n",
       " 'أخا': 272,\n",
       " 'أخاف': 273,\n",
       " 'أخافت': 274,\n",
       " 'أخبار': 275,\n",
       " 'أخبر': 276,\n",
       " 'أخبرت': 277,\n",
       " 'أخبرنا': 278,\n",
       " 'أخبروا': 279,\n",
       " 'أخبريني': 280,\n",
       " 'أخبز': 281,\n",
       " 'أخت': 282,\n",
       " 'أختان': 283,\n",
       " 'أختتم': 284,\n",
       " 'أختر': 285,\n",
       " 'أخترع': 286,\n",
       " 'أختفي': 287,\n",
       " 'أختلف': 288,\n",
       " 'أخجل': 289,\n",
       " 'أخدود': 290,\n",
       " 'أخذ': 291,\n",
       " 'أخذت': 292,\n",
       " 'أخذل': 293,\n",
       " 'أخذنا': 294,\n",
       " 'أخرج': 295,\n",
       " 'أخرجت': 296,\n",
       " 'أخرس': 297,\n",
       " 'أخرق': 298,\n",
       " 'أخرني': 299,\n",
       " 'أخرى': 300,\n",
       " 'أخسر': 301,\n",
       " 'أخشى': 302,\n",
       " 'أخضر': 303,\n",
       " 'أخطأ': 304,\n",
       " 'أخطأت': 305,\n",
       " 'أخطاء': 306,\n",
       " 'أخطط': 307,\n",
       " 'أخطيء': 308,\n",
       " 'أخفض': 309,\n",
       " 'أخفف': 310,\n",
       " 'أخفي': 311,\n",
       " 'أخلد': 312,\n",
       " 'أخلط': 313,\n",
       " 'أخلع': 314,\n",
       " 'أخلف': 315,\n",
       " 'أخمص': 316,\n",
       " 'أخمن': 317,\n",
       " 'أخو': 318,\n",
       " 'أخوات': 319,\n",
       " 'أخوض': 320,\n",
       " 'أخون': 321,\n",
       " 'أخيب': 322,\n",
       " 'أخير': 323,\n",
       " 'أخيرا': 324,\n",
       " 'أخيرة': 325,\n",
       " 'أداء': 326,\n",
       " 'أدب': 327,\n",
       " 'أدبا': 328,\n",
       " 'أدخل': 329,\n",
       " 'أدخلت': 330,\n",
       " 'أدخن': 331,\n",
       " 'أدرس': 332,\n",
       " 'أدرك': 333,\n",
       " 'أدركت': 334,\n",
       " 'أدري': 335,\n",
       " 'أدع': 336,\n",
       " 'أدعم': 337,\n",
       " 'أدعى': 338,\n",
       " 'أدفأ': 339,\n",
       " 'أدفع': 340,\n",
       " 'أدلة': 341,\n",
       " 'أدلى': 342,\n",
       " 'أدنی': 343,\n",
       " 'أدو': 344,\n",
       " 'أدوات': 345,\n",
       " 'أدوار': 346,\n",
       " 'أدوية': 347,\n",
       " 'أدير': 348,\n",
       " 'أدين': 349,\n",
       " 'أذاكر': 350,\n",
       " 'أذاهب': 351,\n",
       " 'أذق': 352,\n",
       " 'أذكر': 353,\n",
       " 'أذكى': 354,\n",
       " 'أذهب': 355,\n",
       " 'أذهبت': 356,\n",
       " 'أذى': 357,\n",
       " 'أر': 358,\n",
       " 'أرأى': 359,\n",
       " 'أراد': 360,\n",
       " 'أرادت': 361,\n",
       " 'أراسل': 362,\n",
       " 'أرافق': 363,\n",
       " 'أراقب': 364,\n",
       " 'أرانب': 365,\n",
       " 'أربط': 366,\n",
       " 'أربع': 367,\n",
       " 'أربعا': 368,\n",
       " 'أربعاء': 369,\n",
       " 'أربعة': 370,\n",
       " 'أربعون': 371,\n",
       " 'أربعين': 372,\n",
       " 'أرتاح': 373,\n",
       " 'أرتقي': 374,\n",
       " 'أرتكب': 375,\n",
       " 'أرجح': 376,\n",
       " 'أرجع': 377,\n",
       " 'أرجو': 378,\n",
       " 'أرحل': 379,\n",
       " 'أرخص': 380,\n",
       " 'أرد': 381,\n",
       " 'أردت': 382,\n",
       " 'أردنا': 383,\n",
       " 'أردي': 384,\n",
       " 'أرز': 385,\n",
       " 'أرسل': 386,\n",
       " 'أرسلت': 387,\n",
       " 'أرسم': 388,\n",
       " 'أرض': 389,\n",
       " 'أرضا': 390,\n",
       " 'أرضي': 391,\n",
       " 'أرضية': 392,\n",
       " 'أرغب': 393,\n",
       " 'أرغم': 394,\n",
       " 'أرغمت': 395,\n",
       " 'أرفع': 396,\n",
       " 'أرقص': 397,\n",
       " 'أركب': 398,\n",
       " 'أركز': 399,\n",
       " 'أركض': 400,\n",
       " 'أرم': 401,\n",
       " 'أرمل': 402,\n",
       " 'أرملة': 403,\n",
       " 'أرنب': 404,\n",
       " 'أرني': 405,\n",
       " 'أرهقت': 406,\n",
       " 'أروع': 407,\n",
       " 'أرى': 408,\n",
       " 'أري': 409,\n",
       " 'أرياف': 410,\n",
       " 'أريتني': 411,\n",
       " 'أريد': 412,\n",
       " 'أريكة': 413,\n",
       " 'أريكتي': 414,\n",
       " 'أريني': 415,\n",
       " 'أزاحت': 416,\n",
       " 'أزال': 417,\n",
       " 'أزالت': 418,\n",
       " 'أزرق': 419,\n",
       " 'أزرقا': 420,\n",
       " 'أزعج': 421,\n",
       " 'أزل': 422,\n",
       " 'أزلت': 423,\n",
       " 'أزمات': 424,\n",
       " 'أزمة': 425,\n",
       " 'أزمت': 426,\n",
       " 'أزهار': 427,\n",
       " 'أزور': 428,\n",
       " 'أزياء': 429,\n",
       " 'أس': 430,\n",
       " 'أسأت': 431,\n",
       " 'أسأل': 432,\n",
       " 'أسئل': 433,\n",
       " 'أسئلة': 434,\n",
       " 'أساء': 435,\n",
       " 'أسابيع': 436,\n",
       " 'أساسي': 437,\n",
       " 'أساعد': 438,\n",
       " 'أسافر': 439,\n",
       " 'أساكوسا': 440,\n",
       " 'أسامحك': 441,\n",
       " 'أسامحه': 442,\n",
       " 'أسباب': 443,\n",
       " 'أسبح': 444,\n",
       " 'أسبوع': 445,\n",
       " 'أسبوعا': 446,\n",
       " 'أسبوعيا': 447,\n",
       " 'أسبوعين': 448,\n",
       " 'أستاذ': 449,\n",
       " 'أستاذة': 450,\n",
       " 'أستحق': 451,\n",
       " 'أستحم': 452,\n",
       " 'أستخدم': 453,\n",
       " 'أستدر': 454,\n",
       " 'أستدعي': 455,\n",
       " 'أستذهب': 456,\n",
       " 'أستراليا': 457,\n",
       " 'أستساعدهم': 458,\n",
       " 'أستطع': 459,\n",
       " 'أستطيع': 460,\n",
       " 'أستعد': 461,\n",
       " 'أستعمل': 462,\n",
       " 'أستعوب': 463,\n",
       " 'أستعير': 464,\n",
       " 'أستغرق': 465,\n",
       " 'أستقيل': 466,\n",
       " 'أستلقي': 467,\n",
       " 'أستلم': 468,\n",
       " 'أستمتع': 469,\n",
       " 'أستمطر': 470,\n",
       " 'أستمع': 471,\n",
       " 'أستيقظ': 472,\n",
       " 'أسحب': 473,\n",
       " 'أسد': 474,\n",
       " 'أسدد': 475,\n",
       " 'أسرة': 476,\n",
       " 'أسرتي': 477,\n",
       " 'أسرع': 478,\n",
       " 'أسرعا': 479,\n",
       " 'أسرق': 480,\n",
       " 'أسطوانة': 481,\n",
       " 'أسطورة': 482,\n",
       " 'أسطوري': 483,\n",
       " 'أسعار': 484,\n",
       " 'أسعد': 485,\n",
       " 'أسعدت': 486,\n",
       " 'أسعدنياجعلني': 487,\n",
       " 'أسف': 488,\n",
       " 'أسفل': 489,\n",
       " 'أسق': 490,\n",
       " 'أسقطت': 491,\n",
       " 'أسكت': 492,\n",
       " 'أسكتنا': 493,\n",
       " 'أسكن': 494,\n",
       " 'أسلب': 495,\n",
       " 'أسلحة': 496,\n",
       " 'أسماء': 497,\n",
       " 'أسماك': 498,\n",
       " 'أسمح': 499,\n",
       " 'أسمع': 500,\n",
       " 'أسمعت': 501,\n",
       " 'أسنان': 502,\n",
       " 'أسهم': 503,\n",
       " 'أسوأ': 504,\n",
       " 'أسود': 505,\n",
       " 'أسودا': 506,\n",
       " 'أسورة': 507,\n",
       " 'أشأ': 508,\n",
       " 'أشار': 509,\n",
       " 'أشارك': 510,\n",
       " 'أشاهد': 511,\n",
       " 'أشباح': 512,\n",
       " 'أشتاق': 513,\n",
       " 'أشتري': 514,\n",
       " 'أشتكي': 515,\n",
       " 'أشتية': 516,\n",
       " 'أشجار': 517,\n",
       " 'أشخاص': 518,\n",
       " 'أشرب': 519,\n",
       " 'أشرت': 520,\n",
       " 'أشرح': 521,\n",
       " 'أشرق': 522,\n",
       " 'أشعار': 523,\n",
       " 'أشعارا': 524,\n",
       " 'أشعة': 525,\n",
       " 'أشعر': 526,\n",
       " 'أشعل': 527,\n",
       " 'أشقاء': 528,\n",
       " 'أشك': 529,\n",
       " 'أشكال': 530,\n",
       " 'أشكر': 531,\n",
       " 'أشلاء': 532,\n",
       " 'أشم': 533,\n",
       " 'أشهر': 534,\n",
       " 'أشياء': 535,\n",
       " 'أصاب': 536,\n",
       " 'أصبت': 537,\n",
       " 'أصبح': 538,\n",
       " 'أصبحا': 539,\n",
       " 'أصبحت': 540,\n",
       " 'أصبحنا': 541,\n",
       " 'أصبحوا': 542,\n",
       " 'أصحاب': 543,\n",
       " 'أصحو': 544,\n",
       " 'أصحيح': 545,\n",
       " 'أصدر': 546,\n",
       " 'أصدق': 547,\n",
       " 'أصدقاء': 548,\n",
       " 'أصر': 549,\n",
       " 'أصطاد': 550,\n",
       " 'أصطحب': 551,\n",
       " 'أصعب': 552,\n",
       " 'أصغر': 553,\n",
       " 'أصغيتم': 554,\n",
       " 'أصفر': 555,\n",
       " 'أصل': 556,\n",
       " 'أصلا': 557,\n",
       " 'أصلح': 558,\n",
       " 'أصلحت': 559,\n",
       " 'أصلع': 560,\n",
       " 'أصليين': 561,\n",
       " 'أصم': 562,\n",
       " 'أصمت': 563,\n",
       " 'أصنع': 564,\n",
       " 'أصوات': 565,\n",
       " 'أصوت': 566,\n",
       " 'أصوليين': 567,\n",
       " 'أصيب': 568,\n",
       " 'أصيبت': 569,\n",
       " 'أصيبوا': 570,\n",
       " 'أصير': 571,\n",
       " 'أضحت': 572,\n",
       " 'أضحك': 573,\n",
       " 'أضرب': 574,\n",
       " 'أضطر': 575,\n",
       " 'أضع': 576,\n",
       " 'أضعاف': 577,\n",
       " 'أضعت': 578,\n",
       " 'أضغط': 579,\n",
       " 'أضف': 580,\n",
       " 'أضفت': 581,\n",
       " 'أضمن': 582,\n",
       " 'أضن': 583,\n",
       " 'أضواء': 584,\n",
       " 'أضيع': 585,\n",
       " 'أضيف': 586,\n",
       " 'أطباء': 587,\n",
       " 'أطباق': 588,\n",
       " 'أطبخ': 589,\n",
       " 'أطرح': 590,\n",
       " 'أطع': 591,\n",
       " 'أطعم': 592,\n",
       " 'أطعمة': 593,\n",
       " 'أطعمت': 594,\n",
       " 'أطفأ': 595,\n",
       " 'أطفأت': 596,\n",
       " 'أطفئ': 597,\n",
       " 'أطفال': 598,\n",
       " 'أطفالا': 599,\n",
       " 'أطلب': 600,\n",
       " 'أطلق': 601,\n",
       " 'أطلقت': 602,\n",
       " 'أطمئن': 603,\n",
       " 'أطهو': 604,\n",
       " 'أطول': 605,\n",
       " 'أطيب': 606,\n",
       " 'أطيق': 607,\n",
       " 'أظل': 608,\n",
       " 'أظن': 609,\n",
       " 'أظنن': 610,\n",
       " 'أظنني': 611,\n",
       " 'أظهر': 612,\n",
       " 'أظهرت': 613,\n",
       " 'أظهروا': 614,\n",
       " 'أعاد': 615,\n",
       " 'أعار': 616,\n",
       " 'أعارض': 617,\n",
       " 'أعاود': 618,\n",
       " 'أعتبر': 619,\n",
       " 'أعتذر': 620,\n",
       " 'أعترف': 621,\n",
       " 'أعتقد': 622,\n",
       " 'أعتمد': 623,\n",
       " 'أعتن': 624,\n",
       " 'أعتني': 625,\n",
       " 'أعثر': 626,\n",
       " 'أعجبت': 627,\n",
       " 'أعجوبة': 628,\n",
       " 'أعد': 629,\n",
       " 'أعداء': 630,\n",
       " 'أعداد': 631,\n",
       " 'أعدت': 632,\n",
       " 'أعددت': 633,\n",
       " 'أعدو': 634,\n",
       " 'أعذر': 635,\n",
       " 'أعر': 636,\n",
       " 'أعرت': 637,\n",
       " 'أعرف': 638,\n",
       " 'أعرني': 639,\n",
       " 'أعز': 640,\n",
       " 'أعزب': 641,\n",
       " 'أعزف': 642,\n",
       " 'أعشاب': 643,\n",
       " 'أعشق': 644,\n",
       " 'أعصاب': 645,\n",
       " 'أعضاء': 646,\n",
       " 'أعط': 647,\n",
       " 'أعطانيه': 648,\n",
       " 'أعطت': 649,\n",
       " 'أعطس': 650,\n",
       " 'أعطني': 651,\n",
       " 'أعطه': 652,\n",
       " 'أعطوا': 653,\n",
       " 'أعطى': 654,\n",
       " 'أعطي': 655,\n",
       " 'أعطيت': 656,\n",
       " 'أعظم': 657,\n",
       " 'أعلام': 658,\n",
       " 'أعلق': 659,\n",
       " 'أعلم': 660,\n",
       " 'أعلميني': 661,\n",
       " 'أعلن': 662,\n",
       " 'أعلنت': 663,\n",
       " 'أعلى': 664,\n",
       " 'أعماق': 665,\n",
       " 'أعمال': 666,\n",
       " 'أعمامي': 667,\n",
       " 'أعمق': 668,\n",
       " 'أعمل': 669,\n",
       " 'أعمى': 670,\n",
       " 'أعندك': 671,\n",
       " 'أعني': 672,\n",
       " 'أعوام': 673,\n",
       " 'أعواما': 674,\n",
       " 'أعود': 675,\n",
       " 'أعور': 676,\n",
       " 'أعي': 677,\n",
       " 'أعياد': 678,\n",
       " 'أعيد': 679,\n",
       " 'أعير': 680,\n",
       " 'أعيش': 681,\n",
       " 'أعين': 682,\n",
       " 'أغادر': 683,\n",
       " 'أغبى': 684,\n",
       " 'أغراض': 685,\n",
       " 'أغرب': 686,\n",
       " 'أغرق': 687,\n",
       " 'أغسل': 688,\n",
       " 'أغضب': 689,\n",
       " 'أغضبت': 690,\n",
       " 'أغلب': 691,\n",
       " 'أغلبي': 692,\n",
       " 'أغلق': 693,\n",
       " 'أغلقت': 694,\n",
       " 'أغلى': 695,\n",
       " 'أغني': 696,\n",
       " 'أغنية': 697,\n",
       " 'أغيب': 698,\n",
       " 'أغير': 699,\n",
       " 'أفاجئ': 700,\n",
       " 'أفادت': 701,\n",
       " 'أفاعي': 702,\n",
       " 'أفتح': 703,\n",
       " 'أفترض': 704,\n",
       " 'أفتقد': 705,\n",
       " 'أفحص': 706,\n",
       " 'أفراد': 707,\n",
       " 'أفرش': 708,\n",
       " 'أفسح': 709,\n",
       " 'أفسد': 710,\n",
       " 'أفضل': 711,\n",
       " 'أفطرت': 712,\n",
       " 'أفعال': 713,\n",
       " 'أفعل': 714,\n",
       " 'أفكار': 715,\n",
       " 'أفكر': 716,\n",
       " 'أفكرت': 717,\n",
       " 'أفلام': 718,\n",
       " 'أفهم': 719,\n",
       " 'أفواه': 720,\n",
       " 'أفوز': 721,\n",
       " 'أفون': 722,\n",
       " 'أقابل': 723,\n",
       " 'أقارب': 724,\n",
       " 'أقاضي': 725,\n",
       " 'أقاطع': 726,\n",
       " 'أقام': 727,\n",
       " 'أقبل': 728,\n",
       " 'أقترب': 729,\n",
       " 'أقترح': 730,\n",
       " 'أقترض': 731,\n",
       " 'أقترف': 732,\n",
       " 'أقتل': 733,\n",
       " 'أقدام': 734,\n",
       " 'أقدر': 735,\n",
       " 'أقدم': 736,\n",
       " 'أقرأ': 737,\n",
       " 'أقرا': 738,\n",
       " 'أقراص': 739,\n",
       " 'أقرب': 740,\n",
       " 'أقربون': 741,\n",
       " 'أقرر': 742,\n",
       " 'أقرض': 743,\n",
       " 'أقرضت': 744,\n",
       " 'أقساط': 745,\n",
       " 'أقسم': 746,\n",
       " 'أقشر': 747,\n",
       " 'أقصد': 748,\n",
       " 'أقصر': 749,\n",
       " 'أقصوصة': 750,\n",
       " 'أقصى': 751,\n",
       " 'أقضي': 752,\n",
       " 'أقطع': 753,\n",
       " 'أقعد': 754,\n",
       " 'أقف': 755,\n",
       " 'أقفز': 756,\n",
       " 'أقفل': 757,\n",
       " 'أقفلت': 758,\n",
       " 'أقل': 759,\n",
       " 'أقلام': 760,\n",
       " 'أقلب': 761,\n",
       " 'أقلعت': 762,\n",
       " 'أقم': 763,\n",
       " 'أقمت': 764,\n",
       " 'أقنع': 765,\n",
       " 'أقنعت': 766,\n",
       " 'أقو': 767,\n",
       " 'أقوال': 768,\n",
       " 'أقود': 769,\n",
       " 'أقول': 770,\n",
       " 'أقوم': 771,\n",
       " 'أقوى': 772,\n",
       " 'أقوياء': 773,\n",
       " 'أقيس': 774,\n",
       " 'أكاد': 775,\n",
       " 'أكبر': 776,\n",
       " 'أكتاف': 777,\n",
       " 'أكتب': 778,\n",
       " 'أكتبت': 779,\n",
       " 'أكتوبر': 780,\n",
       " 'أكثر': 781,\n",
       " 'أكره': 782,\n",
       " 'أكسجين': 783,\n",
       " 'أكل': 784,\n",
       " 'أكلت': 785,\n",
       " 'أكلم': 786,\n",
       " 'أكلنا': 787,\n",
       " 'أكلوا': 788,\n",
       " 'أكمل': 789,\n",
       " 'أكملت': 790,\n",
       " 'أكن': 791,\n",
       " 'أكنت': 792,\n",
       " 'أكو': 793,\n",
       " 'أكون': 794,\n",
       " 'أكيد': 795,\n",
       " 'أكيدة': 796,\n",
       " 'أكيليز': 797,\n",
       " 'أل': 798,\n",
       " 'ألبرت': 799,\n",
       " 'ألبس': 800,\n",
       " 'ألتقط': 801,\n",
       " 'ألتمس': 802,\n",
       " 'ألحق': 803,\n",
       " 'ألحقت': 804,\n",
       " 'ألدي': 805,\n",
       " 'ألزمت': 806,\n",
       " 'ألعاب': 807,\n",
       " 'ألعب': 808,\n",
       " 'ألغ': 809,\n",
       " 'ألغوا': 810,\n",
       " 'ألغيت': 811,\n",
       " 'ألف': 812,\n",
       " 'ألفت': 813,\n",
       " 'ألفي': 814,\n",
       " 'ألفين': 815,\n",
       " 'ألق': 816,\n",
       " 'ألقت': 817,\n",
       " 'ألقوا': 818,\n",
       " 'ألقى': 819,\n",
       " 'ألقي': 820,\n",
       " 'أللقصه': 821,\n",
       " 'ألم': 822,\n",
       " 'ألما': 823,\n",
       " 'ألماس': 824,\n",
       " 'ألمانيا': 825,\n",
       " 'ألمانية': 826,\n",
       " 'ألنا': 827,\n",
       " 'ألوان': 828,\n",
       " 'ألوب': 829,\n",
       " 'ألوت': 830,\n",
       " 'ألوم': 831,\n",
       " 'ألياف': 832,\n",
       " 'أليافا': 833,\n",
       " 'أليست': 834,\n",
       " 'أليفا': 835,\n",
       " 'أليفة': 836,\n",
       " 'أم': 837,\n",
       " 'أما': 838,\n",
       " 'أمازالوا': 839,\n",
       " 'أماكن': 840,\n",
       " 'أمام': 841,\n",
       " 'أمامي': 842,\n",
       " 'أمامية': 843,\n",
       " 'أمان': 844,\n",
       " 'أمانع': 845,\n",
       " 'أماه': 846,\n",
       " 'أمتأكد': 847,\n",
       " 'أمتا': 848,\n",
       " 'أمتار': 849,\n",
       " 'أمتلك': 850,\n",
       " 'أمثال': 851,\n",
       " 'أمدرس': 852,\n",
       " 'أمر': 853,\n",
       " 'أمرا': 854,\n",
       " 'أمرت': 855,\n",
       " 'أمرض': 856,\n",
       " 'أمريكا': 857,\n",
       " 'أمريكي': 858,\n",
       " 'أمريكية': 859,\n",
       " 'أمريكيون': 860,\n",
       " 'أمريكيين': 861,\n",
       " 'أمزح': 862,\n",
       " 'أمس': 863,\n",
       " 'أمسكت': 864,\n",
       " 'أمسية': 865,\n",
       " 'أمش': 866,\n",
       " 'أمشط': 867,\n",
       " 'أمشى': 868,\n",
       " 'أمض': 869,\n",
       " 'أمضت': 870,\n",
       " 'أمضغ': 871,\n",
       " 'أمضوا': 872,\n",
       " 'أمضي': 873,\n",
       " 'أمضيت': 874,\n",
       " 'أمضينا': 875,\n",
       " 'أمطار': 876,\n",
       " 'أمطرت': 877,\n",
       " 'أمكن': 878,\n",
       " 'أمكننا': 879,\n",
       " 'أمكنني': 880,\n",
       " 'أمل': 881,\n",
       " 'أملئ': 882,\n",
       " 'أملا': 883,\n",
       " 'أملت': 884,\n",
       " 'أملك': 885,\n",
       " 'أملى': 886,\n",
       " 'أمهات': 887,\n",
       " 'أمواج': 888,\n",
       " 'أموال': 889,\n",
       " 'أموت': 890,\n",
       " 'أمور': 891,\n",
       " 'أميال': 892,\n",
       " 'أميرة': 893,\n",
       " 'أميرتي': 894,\n",
       " 'أميركي': 895,\n",
       " 'أمين': 896,\n",
       " 'أمينا': 897,\n",
       " 'أمينة': 898,\n",
       " 'أن': 899,\n",
       " 'أنا': 900,\n",
       " 'أنابيب': 901,\n",
       " 'أناس': 902,\n",
       " 'أناقش': 903,\n",
       " 'أنام': 904,\n",
       " 'أناناس': 905,\n",
       " 'أنانيين': 906,\n",
       " 'أنت': 907,\n",
       " 'أنتبه': 908,\n",
       " 'أنتحر': 909,\n",
       " 'أنتظر': 910,\n",
       " 'أنتقل': 911,\n",
       " 'أنتم': 912,\n",
       " 'أنته': 913,\n",
       " 'أنتهي': 914,\n",
       " 'أنجح': 915,\n",
       " 'أنجز': 916,\n",
       " 'أنجزت': 917,\n",
       " 'أنجلترا': 918,\n",
       " 'أنحاء': 919,\n",
       " 'أنحني': 920,\n",
       " 'أنذر': 921,\n",
       " 'أنذهب': 922,\n",
       " 'أنذهل': 923,\n",
       " 'أنزع': 924,\n",
       " 'أنزل': 925,\n",
       " 'أنس': 926,\n",
       " 'أنسجم': 927,\n",
       " 'أنسخ': 928,\n",
       " 'أنسى': 929,\n",
       " 'أنشئت': 930,\n",
       " 'أنصت': 931,\n",
       " 'أنصح': 932,\n",
       " 'أنضم': 933,\n",
       " 'أنظر': 934,\n",
       " 'أنظف': 935,\n",
       " 'أنف': 936,\n",
       " 'أنفاس': 937,\n",
       " 'أنفاق': 938,\n",
       " 'أنفس': 939,\n",
       " 'أنفق': 940,\n",
       " 'أنفي': 941,\n",
       " 'أنقذ': 942,\n",
       " 'أنقر': 943,\n",
       " 'أنقلب': 944,\n",
       " 'أنكر': 945,\n",
       " 'أنكسر': 946,\n",
       " 'أنم': 947,\n",
       " 'أنهار': 948,\n",
       " 'أنهت': 949,\n",
       " 'أنهض': 950,\n",
       " 'أنهى': 951,\n",
       " 'أنهي': 952,\n",
       " 'أنهيت': 953,\n",
       " 'أنهينا': 954,\n",
       " 'أنوار': 955,\n",
       " 'أنواع': 956,\n",
       " 'أنوف': 957,\n",
       " 'أنوي': 958,\n",
       " 'أنيقة': 959,\n",
       " 'أنی': 960,\n",
       " 'أهتم': 961,\n",
       " 'أهدأ': 962,\n",
       " 'أهداف': 963,\n",
       " 'أهدت': 964,\n",
       " 'أهذ': 965,\n",
       " 'أهذا': 966,\n",
       " 'أهرب': 967,\n",
       " 'أهزم': 968,\n",
       " 'أهل': 969,\n",
       " 'أهلا': 970,\n",
       " 'أهم': 971,\n",
       " 'أهمية': 972,\n",
       " 'أهنئ': 973,\n",
       " 'أهنا': 974,\n",
       " 'أهو': 975,\n",
       " 'أهي': 976,\n",
       " 'أو': 977,\n",
       " 'أواجه': 978,\n",
       " 'أوافق': 979,\n",
       " 'أوافي': 980,\n",
       " 'أوامر': 981,\n",
       " 'أوان': 982,\n",
       " 'أوبخ': 983,\n",
       " 'أوبرا': 984,\n",
       " 'أوتان': 985,\n",
       " 'أوتظنني': 986,\n",
       " 'أوتوماتيكي': 987,\n",
       " 'أوج': 988,\n",
       " 'أوجدت': 989,\n",
       " 'أود': 990,\n",
       " 'أوذيك': 991,\n",
       " 'أوراق': 992,\n",
       " 'أوروبا': 993,\n",
       " 'أوروبيين': 994,\n",
       " 'أوساكا': 995,\n",
       " 'أوشكت': 996,\n",
       " 'أوصل': 997,\n",
       " 'أوصلت': 998,\n",
       " 'أوقات': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_token_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will put the max_encoder_seq for both decoder and encoder with 64 since the max number of tokens in Arabic is 52 and in English is 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_encoder_seq_length = 64\n",
    "max_decoder_seq_length = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Arabic_input</th>\n",
       "      <th>English_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>مرحبا</td>\n",
       "      <td>START_ hi _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>اركض</td>\n",
       "      <td>START_ run _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>اخفض رأسك</td>\n",
       "      <td>START_ duck _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>اخفضي رأسك</td>\n",
       "      <td>START_ duck _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>اخفضوا رؤوسكم</td>\n",
       "      <td>START_ duck _END</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Arabic_input    English_target\n",
       "0          مرحبا    START_ hi _END\n",
       "1           اركض   START_ run _END\n",
       "2      اخفض رأسك  START_ duck _END\n",
       "3     اخفضي رأسك  START_ duck _END\n",
       "4  اخفضوا رؤوسكم  START_ duck _END"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_input_target(dataset, max_encoder_seq_length, max_decoder_seq_length, num_decoder_tokens):\n",
    "    # The input setence to the encoder is 64 leghth.\n",
    "    encoder_input_data = np.zeros( (len(dataset.Arabic_input), max_encoder_seq_length),dtype='float32')\n",
    "    # The input setence to the decoder is 64 leghth.\n",
    "    decoder_input_data = np.zeros((len(dataset.English_target), max_decoder_seq_length), dtype='float32')\n",
    "    # The output setence of the decoder is 64 x all_tokens_inside_encoder. Since the decoder will do softmax for each token.\n",
    "    decoder_target_data = np.zeros((len(dataset.English_target), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "    \n",
    "    return encoder_input_data, decoder_input_data, decoder_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data, decoder_input_data, decoder_target_data = init_input_target(dataset, max_encoder_seq_length, max_decoder_seq_length, num_decoder_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(tokenized_ds_copy, max_encoder_seq_length, max_decoder_seq_length, num_decoder_tokens):\n",
    "\n",
    "    for i, (input_text_ls, target_text_ls) in enumerate(zip(tokenized_ds_copy.Arabic_input, tokenized_ds_copy.English_target)):\n",
    "        # preparing the encoder inputs\n",
    "        for t, tok in enumerate(input_text_ls):\n",
    "            #To obtain the ids of encoder sentence's tokens from input_token_index\n",
    "            encoder_input_data[i, t] = input_token_index[tok]\n",
    "            \n",
    "        encoder_input_data[i, t+1:] = input_token_index[pad_tok]\n",
    "        \n",
    "        # This loop is used to prepare the input and output of the decoder\n",
    "        for t, tok in enumerate(target_text_ls):\n",
    "            #1- prepare the decoder input\n",
    "            #To obtain the ids of decoder sentence's tokens from target_token_index\n",
    "            decoder_input_data[i, t] = target_token_index[tok]    \n",
    "            \n",
    "            # To obtain the decoder output\n",
    "            if t > 0:\n",
    "                # decoder_target_data will be ahead by one timestep\n",
    "                # and will not include the start character.\n",
    "                #We put 1 in the place of the expected word\n",
    "                decoder_target_data[i, t - 1, target_token_index[tok]] = 1.\n",
    "        decoder_input_data[i, t+1:] = target_token_index[pad_tok] \n",
    "        decoder_target_data[i, t:, target_token_index[pad_tok]] = 1.          \n",
    "              \n",
    "    return encoder_input_data, decoder_input_data, decoder_target_data              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data, decoder_input_data, decoder_target_data = vectorize(tokenized_ds_copy, max_encoder_seq_length, max_decoder_seq_length, num_decoder_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5490.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5490"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_token_index['مرحبا']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \n",
    "\n",
    "The 1st sentence in the Arabic_input column is 'مرحبا', and there is no any other word inside the sentence. If we printed the 1st place in encoder_input_data, we will find that there is only one word, this word took the same number of 'مرحبا "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   5., 1802.,    6.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1802"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_token_index['hi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_token_index['START_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_token_index['_END']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "    The 1st sentence in the decoder input is START_ hi _END, so there are only three tokens, each token has id. If we check the tokens ids inside the target_token_index, then we find that the decoder_input_data has the similar number in the location 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The _END location\n",
    "decoder_target_data[0][1][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The hi location\n",
    "decoder_target_data[0][0][1800:1810]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "what should appear in the output of the decoder in this case is 'hi _END', we will find hi on gate 0 in the 3rd location, and _END on gate  1 n the 6th location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mdeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will implement the encoder separated from decoder to simplify the operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_training_encoder(num_encoder_tokens, emb_sz, lstm_sz, mask_zero):\n",
    "    # 1- Define the input to the encoder\n",
    "    encoder_inputs = Input(shape=(None,))\n",
    "    \n",
    "    # 2- Define the embedding layer. This layer is built on the previous layer the input to the encoder.\n",
    "    # This embedding layer need the following parameters 1- all the expected encoder tokens number. 2- embedding size of each word\n",
    "    # The vector of each word output from embedding layer has this word charcteristics.\n",
    "    en_x=  Embedding(num_encoder_tokens, emb_sz,mask_zero=mask_zero)(encoder_inputs)\n",
    "    \n",
    "    # 3- Define the 1st LSTM layer, \n",
    "    #This layer parametrs are:\n",
    "    # 1- reurn_satate which enable us to output the cell state and the hidden state\n",
    "    # 2- lstm_sz: which is the number of hidden units inside the LSTM layer.\n",
    "    encoder = LSTM(lstm_sz, return_state=True)\n",
    "    \n",
    "    # 4- Put the encoder LSTM on the embedding layer output \n",
    "    #and take the output of this LSTM which will be used to build the context vector\n",
    "    encoder_outputs, state_h, state_c = encoder(en_x)\n",
    "    \n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]\n",
    "  \n",
    "    # Encoder model\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "    print('\\n The encoder model \\n')\n",
    "    encoder_model.summary()\n",
    "  \n",
    "    return encoder_model, encoder_states, encoder_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "    \n",
    "def build_training_decoder(num_decoder_tokens, emb_sz, lstm_sz, encoder_states, encoder_inputs, mask_zero):\n",
    "        \n",
    "    #1- define the input layer to the decoder.\n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "    # 2- define the embeddding layer for the decoder/ In training it will take the expected tokens from the dataset\n",
    "    # The embedding layer parameters are:\n",
    "    # 1- All the expected tokens to the decoder.\n",
    "    # 2- the embedding size\n",
    "    decoder_embedding=  Embedding(num_decoder_tokens, emb_sz,mask_zero=True)\n",
    "\n",
    "    # 3- put the layer of embedding on the layer of decoder inputs. \n",
    "    embedding_output= decoder_embedding(decoder_inputs)\n",
    "\n",
    "    \n",
    "    #4- Define the LSTM which should output the hidden state and cell state and cell output for each cell.\n",
    "    # the hidden state and cell state which can be done enabling return_state\n",
    "    #  The output sequence of all the input sequence tokens can be done by enabling the return_sequences \n",
    "    decoder_lstm = LSTM(lstm_sz, return_sequences=True, return_state=True)\n",
    "\n",
    "    \n",
    "    # 5- Put the LSTM on the top of embedding layer and feeed the context vector to the LSTM 'encoder_states'\n",
    "    decoder_outputs, _, _ = decoder_lstm(embedding_output, initial_state=encoder_states)\n",
    "\n",
    "    #6- Define the fully connected layer which predicts the output token.\n",
    "    # This layer will output a vector. This vector has a probability for each expected token to output. \n",
    "    #Then this layer feeds this to an activation function 'softmax' to decide which word sould output at each timestep.\n",
    "    decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "\n",
    "    # 6-Put the dense/fully connected layer on the top of lstm \n",
    "    #and feed the lstm output vector for all input tokens to the dense.\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    # 7-Here build the combined model which takes the training tokens to the encoder and decoder and output the decoder output. \n",
    "    # This decoder output comes after the softmax of the dense layer\n",
    "    combined_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    \n",
    "    print('\\n The combined model \\n')\n",
    "    combined_model.summary()\n",
    "    \n",
    "    return combined_model, decoder_inputs,embedding_output, decoder_lstm, decoder_dense\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will use the same architecture of decoder in the training pahse, so we will use the followiing\n",
    "# 1- decoder_input.\n",
    "# 2- decoder_embedding_output. \n",
    "# 3- decoder_lstm\n",
    "#4- decoder_dense\n",
    "#why will we use the same architecture? since we will use the same cells but by using the inference methodology which is mentioned above.\n",
    "def build_inference_decoder(num_decoder_tokens, lstm_sz, emb_sz, embedding_output, decoder_inputs, decoder_lstm, decoder_dense):\n",
    "    \n",
    "    # Decoder model: Re-build based on explicit state inputs. Needed for step-by-step inference:\n",
    "    \n",
    "    # define the hidden state of the context vector which will come from the encoder in the prediction\n",
    "    decoder_state_input_h = Input(shape=(lstm_sz,))\n",
    "    # define the cell state of the context vector which will come from the encoder in the prediction\n",
    "    decoder_state_input_c = Input(shape=(lstm_sz,))\n",
    "    \n",
    "    #define the conext vector which will feed into the decoder to initialize it. The values of this will be feed in the prediction.\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    # feed the decoder LSTM with embedding output, and inilize its state to decoder_states_inputs \n",
    "    #which will be fed after that with the encoder context vector.\n",
    "    decoder_outputs2, state_h2, state_c2 = decoder_lstm(embedding_output, initial_state=decoder_states_inputs)\n",
    "    \n",
    "    # Define the input to the dense layer\n",
    "    decoder_states2 = [state_h2, state_c2]\n",
    "    \n",
    "    # Feed the hidden and cell state to the dense layer which will predict the output.\n",
    "    decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "    #define the decoder_model which will take take the decoder inputs and initial state for the decoder.\n",
    "    # this model will output the decoder output token in addition to the hidden and cell states.\n",
    "    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs2] + decoder_states2) \n",
    "    \n",
    "    return decoder_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "emb_sz = 50\n",
    "lstm_sz = 64\n",
    " \n",
    "def model_seq_to_seq(batch_size, epochs, mask_zero):\n",
    "    \n",
    "    encoder_model, encoder_states, encoder_inputs = build_training_encoder(num_encoder_tokens, emb_sz, lstm_sz, mask_zero)\n",
    "    combined_model, decoder_inputs,embedding_output, decoder_lstm, decoder_dense = build_training_decoder(num_decoder_tokens,\n",
    "                                                                                                          emb_sz, lstm_sz, \n",
    "                                                                                                          encoder_states, \n",
    "                                                                                                          encoder_inputs,mask_zero)\n",
    "    \n",
    "    decoder_model = build_inference_decoder(num_decoder_tokens, lstm_sz, emb_sz, embedding_output, decoder_inputs,\n",
    "                                            decoder_lstm, decoder_dense)\n",
    "\n",
    "    \n",
    "    # 8- compile the combined model in training phase\n",
    "    combined_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "    combined_model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=batch_size,\n",
    "                       epochs=epochs, validation_split=0.05)\n",
    "    \n",
    "    return combined_model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The encoder model \n",
      "\n",
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_12 (Embedding)     (None, None, 50)          360450    \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               [(None, 64), (None, 64),  29440     \n",
      "=================================================================\n",
      "Total params: 389,890\n",
      "Trainable params: 389,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      " The combined model \n",
      "\n",
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        (None, None, 50)     360450      input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)        (None, None, 50)     215100      input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_12 (LSTM)                  [(None, 64), (None,  29440       embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_13 (LSTM)                  [(None, None, 64), ( 29440       embedding_13[0][0]               \n",
      "                                                                 lstm_12[0][1]                    \n",
      "                                                                 lstm_12[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, None, 4302)   279630      lstm_13[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 914,060\n",
      "Trainable params: 914,060\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/30\n",
      "181/181 [==============================] - 52s 253ms/step - loss: 3.3147 - acc: 0.8719 - val_loss: 1.2910 - val_acc: 0.8069\n",
      "Epoch 2/30\n",
      "181/181 [==============================] - 44s 244ms/step - loss: 0.5464 - acc: 0.9167 - val_loss: 1.2179 - val_acc: 0.8225\n",
      "Epoch 3/30\n",
      "181/181 [==============================] - 46s 256ms/step - loss: 0.5171 - acc: 0.9196 - val_loss: 1.2314 - val_acc: 0.8229\n",
      "Epoch 4/30\n",
      "181/181 [==============================] - 49s 274ms/step - loss: 0.5029 - acc: 0.9208 - val_loss: 1.2085 - val_acc: 0.8242\n",
      "Epoch 5/30\n",
      "181/181 [==============================] - 47s 259ms/step - loss: 0.4881 - acc: 0.9219 - val_loss: 1.1860 - val_acc: 0.8271\n",
      "Epoch 6/30\n",
      "181/181 [==============================] - 47s 259ms/step - loss: 0.4737 - acc: 0.9235 - val_loss: 1.1839 - val_acc: 0.8276\n",
      "Epoch 7/30\n",
      "181/181 [==============================] - 45s 247ms/step - loss: 0.4653 - acc: 0.9239 - val_loss: 1.1713 - val_acc: 0.8300\n",
      "Epoch 8/30\n",
      "181/181 [==============================] - 49s 271ms/step - loss: 0.4523 - acc: 0.9253 - val_loss: 1.1676 - val_acc: 0.8312\n",
      "Epoch 9/30\n",
      "181/181 [==============================] - 48s 266ms/step - loss: 0.4422 - acc: 0.9261 - val_loss: 1.1408 - val_acc: 0.8331\n",
      "Epoch 10/30\n",
      "181/181 [==============================] - 48s 266ms/step - loss: 0.4339 - acc: 0.9267 - val_loss: 1.1228 - val_acc: 0.8344\n",
      "Epoch 11/30\n",
      "181/181 [==============================] - 49s 272ms/step - loss: 0.4240 - acc: 0.9274 - val_loss: 1.1349 - val_acc: 0.8338\n",
      "Epoch 12/30\n",
      "181/181 [==============================] - 48s 265ms/step - loss: 0.4163 - acc: 0.9283 - val_loss: 1.1282 - val_acc: 0.8338\n",
      "Epoch 13/30\n",
      "181/181 [==============================] - 46s 255ms/step - loss: 0.4124 - acc: 0.9283 - val_loss: 1.1267 - val_acc: 0.8346\n",
      "Epoch 14/30\n",
      "181/181 [==============================] - 44s 245ms/step - loss: 0.4062 - acc: 0.9289 - val_loss: 1.1249 - val_acc: 0.8354\n",
      "Epoch 15/30\n",
      "181/181 [==============================] - 45s 247ms/step - loss: 0.4032 - acc: 0.9291 - val_loss: 1.1257 - val_acc: 0.8353\n",
      "Epoch 16/30\n",
      "181/181 [==============================] - 46s 254ms/step - loss: 0.3976 - acc: 0.9295 - val_loss: 1.1120 - val_acc: 0.8362\n",
      "Epoch 17/30\n",
      "181/181 [==============================] - 45s 250ms/step - loss: 0.3927 - acc: 0.9300 - val_loss: 1.1284 - val_acc: 0.8359\n",
      "Epoch 18/30\n",
      "181/181 [==============================] - 44s 244ms/step - loss: 0.3868 - acc: 0.9308 - val_loss: 1.1112 - val_acc: 0.8366\n",
      "Epoch 19/30\n",
      "181/181 [==============================] - 45s 251ms/step - loss: 0.3836 - acc: 0.9313 - val_loss: 1.1226 - val_acc: 0.8367\n",
      "Epoch 20/30\n",
      "181/181 [==============================] - 45s 247ms/step - loss: 0.3743 - acc: 0.9323 - val_loss: 1.0905 - val_acc: 0.8376\n",
      "Epoch 21/30\n",
      "181/181 [==============================] - 45s 246ms/step - loss: 0.3715 - acc: 0.9329 - val_loss: 1.1046 - val_acc: 0.8390\n",
      "Epoch 22/30\n",
      "181/181 [==============================] - 45s 251ms/step - loss: 0.3644 - acc: 0.9339 - val_loss: 1.1174 - val_acc: 0.8386\n",
      "Epoch 23/30\n",
      "181/181 [==============================] - 46s 253ms/step - loss: 0.3631 - acc: 0.9339 - val_loss: 1.1193 - val_acc: 0.8396\n",
      "Epoch 24/30\n",
      "181/181 [==============================] - 47s 260ms/step - loss: 0.3564 - acc: 0.9347 - val_loss: 1.1090 - val_acc: 0.8401\n",
      "Epoch 25/30\n",
      "181/181 [==============================] - 46s 252ms/step - loss: 0.3536 - acc: 0.9355 - val_loss: 1.1253 - val_acc: 0.8397\n",
      "Epoch 26/30\n",
      "181/181 [==============================] - 46s 255ms/step - loss: 0.3468 - acc: 0.9364 - val_loss: 1.1039 - val_acc: 0.8403\n",
      "Epoch 27/30\n",
      "181/181 [==============================] - 48s 263ms/step - loss: 0.3427 - acc: 0.9372 - val_loss: 1.1034 - val_acc: 0.8409\n",
      "Epoch 28/30\n",
      "181/181 [==============================] - 47s 259ms/step - loss: 0.3390 - acc: 0.9376 - val_loss: 1.1111 - val_acc: 0.8407\n",
      "Epoch 29/30\n",
      "181/181 [==============================] - 46s 257ms/step - loss: 0.3352 - acc: 0.9383 - val_loss: 1.1176 - val_acc: 0.8418\n",
      "Epoch 30/30\n",
      "181/181 [==============================] - 47s 258ms/step - loss: 0.3292 - acc: 0.9393 - val_loss: 1.1053 - val_acc: 0.8414\n"
     ]
    }
   ],
   "source": [
    "combined_model, encoder_model, decoder_model = model_seq_to_seq(batch_size=64, epochs=30, mask_zero=False)\n",
    "combined_model.save_weights(\"translatemodel.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, sep = ' '):\n",
    "    # to obtain the encoder model\n",
    "    # 1- Encode the input to obtain the context vector from the encoder.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # Generate empty target sequence of length 1 like this [[0]]\n",
    "    target_seq = np.zeros((1,1))\n",
    "    \n",
    "    # Populate the first character of target sequence with the start character to be like [[5]]\n",
    "    target_seq[0, 0] = target_token_index[st_tok]\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    # if you don't find end token\n",
    "    while not stop_condition:\n",
    "        #feed the predict() with the input to the decoder model which is target_seq and\n",
    "        #the context vector from the encode which is states_value\n",
    "        # output_tokens will hold the last output of the LSTM\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        \n",
    "\n",
    "        # Sample a token \n",
    "        # It returns the index of the maximum item in the 1st array last row\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        # return the word as letters from its index\n",
    "        sampled_tok = reverse_target_tok_index[sampled_token_index]\n",
    "        # form the sentence which consists of words and separatoe\n",
    "        decoded_sentence += sep + sampled_tok\n",
    "\n",
    "        # Exit condition: either hit max length which is 64\n",
    "        # or find stop character.\n",
    "        if (sampled_tok == end_tok or len(decoded_sentence) > 64):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1) with the index of the output token.\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states to be fed to the prediction\n",
    "        states_value = [h, c]\n",
    " \n",
    "    return decoded_sentence\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: 0    مرحبا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of him _END\n",
      "-\n",
      "Input sentence: 1    اركض\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot _END\n",
      "-\n",
      "Input sentence: 2    اخفض رأسك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of the table _END\n",
      "-\n",
      "Input sentence: 3    اخفضي رأسك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  this is a beautiful country _END\n",
      "-\n",
      "Input sentence: 4    اخفضوا رؤوسكم\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  this is a good car _END\n",
      "-\n",
      "Input sentence: 5    النجده\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot _END\n",
      "-\n",
      "Input sentence: 6    اقفز\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of the table _END\n",
      "-\n",
      "Input sentence: 7    قف\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of him _END\n",
      "-\n",
      "Input sentence: 8    توقف \n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot _END\n",
      "-\n",
      "Input sentence: 9    إنتظر\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of the table _END\n",
      "-\n",
      "Input sentence: 10    داوم\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  this house is beautiful _END\n",
      "-\n",
      "Input sentence: 11    استمر\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  this house is beautiful _END\n",
      "-\n",
      "Input sentence: 12    مرحبا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of him _END\n",
      "-\n",
      "Input sentence: 13    تعجل\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of the table _END\n",
      "-\n",
      "Input sentence: 14    استعجل\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of the table _END\n",
      "-\n",
      "Input sentence: 15    انا اري\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im not a doctor _END\n",
      "-\n",
      "Input sentence: 16    أنا فزت\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i am a lot _END\n",
      "-\n",
      "Input sentence: 17    استرح\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of the table _END\n",
      "-\n",
      "Input sentence: 18    ابتسم\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  this is a car _END\n",
      "-\n",
      "Input sentence: 19    في صحتك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  what do you have _END\n",
      "-\n",
      "Input sentence: 20    هل فهمت\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  do you have a doctor _END\n",
      "-\n",
      "Input sentence: 21    ركض\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot _END\n",
      "-\n",
      "Input sentence: 22    أعرف\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i know _END\n",
      "-\n",
      "Input sentence: 23    أعلم ذلك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i know _END\n",
      "-\n",
      "Input sentence: 24    أنا أعلم\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i know _END\n",
      "-\n",
      "Input sentence: 25    أنا في \n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im a good student _END\n",
      "-\n",
      "Input sentence: 26    أنا بخير\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im a good student _END\n",
      "-\n",
      "Input sentence: 27    استمع\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of the table _END\n",
      "-\n",
      "Input sentence: 28    غير معقول\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  what are you _END\n",
      "-\n",
      "Input sentence: 29    حقا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im not a good _END\n",
      "-\n",
      "Input sentence: 30    شكرا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of him _END\n",
      "-\n",
      "Input sentence: 31    شكرا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of him _END\n",
      "-\n",
      "Input sentence: 32    لماذا أنا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i dont have _END\n",
      "-\n",
      "Input sentence: 33    رائع\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a good _END\n",
      "-\n",
      "Input sentence: 34    خذ راحتك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  my mother is a lot of tom _END\n",
      "-\n",
      "Input sentence: 35    أغرب عن وجهي\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is very fast _END\n",
      "-\n",
      "Input sentence: 36    هاتفني\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of the table _END\n",
      "-\n",
      "Input sentence: 37    اتصل بي\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  lets get out _END\n",
      "-\n",
      "Input sentence: 38    تفضل بالدخول\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  please come _END\n",
      "-\n",
      "Input sentence: 39    تعال إلى الداخل\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  my father is a lot of tom _END\n",
      "-\n",
      "Input sentence: 40    بالله عليك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is not a lot _END\n",
      "-\n",
      "Input sentence: 41    هيا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of him _END\n",
      "-\n",
      "Input sentence: 42    هيا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of him _END\n",
      "-\n",
      "Input sentence: 43    اخرج من هنا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is not a lot of the table _END\n",
      "-\n",
      "Input sentence: 44    أخرج\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of the table _END\n",
      "-\n",
      "Input sentence: 45    اخرج\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of the table _END\n",
      "-\n",
      "Input sentence: 46    اتركني و شأني\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  lets go _END\n",
      "-\n",
      "Input sentence: 47    اذهب بعيدا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of the table _END\n",
      "-\n",
      "Input sentence: 48    ارحل\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  my brother is a good country _END\n",
      "-\n",
      "Input sentence: 49    مع السلامه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of the table _END\n",
      "-\n",
      "Input sentence: 50    إلى اللقاء\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i have a lot of water _END\n",
      "-\n",
      "Input sentence: 51    إنتظر\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of the table _END\n",
      "-\n",
      "Input sentence: 52    لقد أتى\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i am a lot _END\n",
      "-\n",
      "Input sentence: 53    هو يجري\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of him _END\n",
      "-\n",
      "Input sentence: 54    ساعدني\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of the table _END\n",
      "-\n",
      "Input sentence: 55    النجده ساعدني\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  my father is a lot of tom _END\n",
      "-\n",
      "Input sentence: 56    ساعدوني\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  my father is a lot of tom _END\n",
      "-\n",
      "Input sentence: 57    انتظر\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of the table _END\n",
      "-\n",
      "Input sentence: 58    أنا موافق\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i am a good car _END\n",
      "-\n",
      "Input sentence: 59    أنا حزين\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im a good student _END\n",
      "-\n",
      "Input sentence: 60    أنا أيضا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i am a good idea _END\n",
      "-\n",
      "Input sentence: 61    اخرس\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of the table _END\n",
      "-\n",
      "Input sentence: 62    اصمت\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  this house is very _END\n",
      "-\n",
      "Input sentence: 63    اسكت\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of the table _END\n",
      "-\n",
      "Input sentence: 64    أغلق فمك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of the table _END\n",
      "-\n",
      "Input sentence: 65    أوقفه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of him _END\n",
      "-\n",
      "Input sentence: 66    خذه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of him _END\n",
      "-\n",
      "Input sentence: 67    أخبرني\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  dont be a doctor _END\n",
      "-\n",
      "Input sentence: 68    توم فاز\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  tom is a good student _END\n",
      "-\n",
      "Input sentence: 69    لقد ربح توم\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  tom is a lot of tom _END\n",
      "-\n",
      "Input sentence: 70    استيقظ\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of the table _END\n",
      "-\n",
      "Input sentence: 71    أهلا و سهلا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  this is a good country _END\n",
      "-\n",
      "Input sentence: 72    مرحبا بك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  where are the book _END\n",
      "-\n",
      "Input sentence: 73    اهلا وسهلا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  this is a good country _END\n",
      "-\n",
      "Input sentence: 74    مرحبا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of him _END\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: 75    من فاز\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of him _END\n",
      "-\n",
      "Input sentence: 76    من الذي ربح\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of him _END\n",
      "-\n",
      "Input sentence: 77    لم لا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  why do you want _END\n",
      "-\n",
      "Input sentence: 78    لما لا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i dont have _END\n",
      "-\n",
      "Input sentence: 79    لا فكره لدي\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i dont have a doctor _END\n",
      "-\n",
      "Input sentence: 80    استمتع بوقتك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i am going to the hospital _END\n",
      "-\n",
      "Input sentence: 81    أسرعا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  this house is very _END\n",
      "-\n",
      "Input sentence: 82    لقد نسيت\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i am not going _END\n",
      "-\n",
      "Input sentence: 83    فهمته\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i am very hungry _END\n",
      "-\n",
      "Input sentence: 84    فهمتها\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i am a lot _END\n",
      "-\n",
      "Input sentence: 85    فهمت ذلك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im not a doctor _END\n",
      "-\n",
      "Input sentence: 86    أستخدمه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i was a lot of the table _END\n",
      "-\n",
      "Input sentence: 87    سأدفع أنا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i am not going _END\n",
      "-\n",
      "Input sentence: 88    أنا مشغول\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im not a doctor _END\n",
      "-\n",
      "Input sentence: 89    إنني مشغول\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im not a doctor _END\n",
      "-\n",
      "Input sentence: 90    أشعر بالبرد\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i am a good idea _END\n",
      "-\n",
      "Input sentence: 91    أنا حر\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im a good student _END\n",
      "-\n",
      "Input sentence: 92    أنا هنا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im not a good _END\n",
      "-\n",
      "Input sentence: 93    لقد عدت إلى البيت\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i have a lot of water _END\n",
      "-\n",
      "Input sentence: 94    أنا فقير\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im a good student _END\n",
      "-\n",
      "Input sentence: 95    أنا ثري\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im not a doctor _END\n",
      "-\n",
      "Input sentence: 96    هذا مؤلم\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i am a lot _END\n",
      "-\n",
      "Input sentence: 97    انها جافه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i have a lot of water _END\n",
      "-\n",
      "Input sentence: 98    الجو حار\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i have a lot _END\n",
      "-\n",
      "Input sentence: 99    إنه جديد\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  its a good _END\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(100): #[14077,20122,40035,40064, 40056, 40068, 40090, 40095, 40100, 40119, 40131, 40136, 40150, 40153]:\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', dataset.Arabic_input[seq_index: seq_index + 1])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### changing the epochs and notice its effect on the decoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_sz = 50\n",
    "lstm_sz = 256\n",
    " \n",
    "def model_seq_to_seq(batch_size, epochs,mask_zero):\n",
    "    \n",
    "    encoder_model, encoder_states, encoder_inputs = build_training_encoder(num_encoder_tokens, emb_sz, lstm_sz,mask_zero)\n",
    "    combined_model, decoder_inputs,embedding_output, decoder_lstm, decoder_dense = build_training_decoder(num_decoder_tokens,\n",
    "                                                                                                          emb_sz, lstm_sz, \n",
    "                                                                                                          encoder_states, \n",
    "                                                                                                          encoder_inputs, \n",
    "                                                                                                          mask_zero)\n",
    "    \n",
    "    decoder_model = build_inference_decoder(num_decoder_tokens, lstm_sz, emb_sz, embedding_output, decoder_inputs,\n",
    "                                            decoder_lstm, decoder_dense)\n",
    "\n",
    "    \n",
    "    # 8- compile the combined model in training phase\n",
    "    combined_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "    combined_model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "    \n",
    "    return combined_model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The encoder model \n",
      "\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, None, 50)          360450    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                [(None, 256), (None, 256) 314368    \n",
      "=================================================================\n",
      "Total params: 674,818\n",
      "Trainable params: 674,818\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      " The combined model \n",
      "\n",
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, None, 50)     360450      input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, None, 50)     215100      input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, 256), (None, 314368      embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   [(None, None, 256),  314368      embedding_5[0][0]                \n",
      "                                                                 lstm_4[0][1]                     \n",
      "                                                                 lstm_4[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 4302)   1105614     lstm_5[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,309,900\n",
      "Trainable params: 2,309,900\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "152/152 [==============================] - 80s 484ms/step - loss: 0.5928 - acc: 0.2081 - val_loss: 0.9569 - val_acc: 0.2080\n",
      "Epoch 2/100\n",
      "152/152 [==============================] - 77s 510ms/step - loss: 0.4560 - acc: 0.3364 - val_loss: 0.9359 - val_acc: 0.2242\n",
      "Epoch 3/100\n",
      "152/152 [==============================] - 78s 515ms/step - loss: 0.4254 - acc: 0.3541 - val_loss: 0.9056 - val_acc: 0.2371\n",
      "Epoch 4/100\n",
      "152/152 [==============================] - 78s 512ms/step - loss: 0.4039 - acc: 0.3691 - val_loss: 0.8684 - val_acc: 0.2513\n",
      "Epoch 5/100\n",
      "152/152 [==============================] - 82s 541ms/step - loss: 0.3821 - acc: 0.3824 - val_loss: 0.8910 - val_acc: 0.2487\n",
      "Epoch 6/100\n",
      "152/152 [==============================] - 79s 522ms/step - loss: 0.3686 - acc: 0.3956 - val_loss: 0.8759 - val_acc: 0.2637\n",
      "Epoch 7/100\n",
      "152/152 [==============================] - 82s 538ms/step - loss: 0.3537 - acc: 0.4176 - val_loss: 0.8755 - val_acc: 0.2685\n",
      "Epoch 8/100\n",
      "152/152 [==============================] - 78s 515ms/step - loss: 0.3380 - acc: 0.4368 - val_loss: 0.8611 - val_acc: 0.2764\n",
      "Epoch 9/100\n",
      "152/152 [==============================] - 76s 501ms/step - loss: 0.3234 - acc: 0.4511 - val_loss: 0.8505 - val_acc: 0.2860\n",
      "Epoch 10/100\n",
      "152/152 [==============================] - 78s 515ms/step - loss: 0.3089 - acc: 0.4718 - val_loss: 0.8364 - val_acc: 0.2908\n",
      "Epoch 11/100\n",
      "152/152 [==============================] - 81s 535ms/step - loss: 0.2949 - acc: 0.4897 - val_loss: 0.8628 - val_acc: 0.2933\n",
      "Epoch 12/100\n",
      "152/152 [==============================] - 80s 526ms/step - loss: 0.2793 - acc: 0.5086 - val_loss: 0.8524 - val_acc: 0.2987\n",
      "Epoch 13/100\n",
      "152/152 [==============================] - 80s 529ms/step - loss: 0.2683 - acc: 0.5220 - val_loss: 0.8634 - val_acc: 0.3022\n",
      "Epoch 14/100\n",
      "152/152 [==============================] - 78s 516ms/step - loss: 0.2571 - acc: 0.5387 - val_loss: 0.8481 - val_acc: 0.3057\n",
      "Epoch 15/100\n",
      "152/152 [==============================] - 82s 540ms/step - loss: 0.2488 - acc: 0.5519 - val_loss: 0.8587 - val_acc: 0.3070\n",
      "Epoch 16/100\n",
      "152/152 [==============================] - 82s 542ms/step - loss: 0.2394 - acc: 0.5636 - val_loss: 0.8623 - val_acc: 0.3110\n",
      "Epoch 17/100\n",
      "152/152 [==============================] - 81s 531ms/step - loss: 0.2289 - acc: 0.5798 - val_loss: 0.8826 - val_acc: 0.3085\n",
      "Epoch 18/100\n",
      "152/152 [==============================] - 79s 519ms/step - loss: 0.2196 - acc: 0.5929 - val_loss: 0.8800 - val_acc: 0.3092\n",
      "Epoch 19/100\n",
      "152/152 [==============================] - 81s 532ms/step - loss: 0.2112 - acc: 0.6059 - val_loss: 0.8796 - val_acc: 0.3118\n",
      "Epoch 20/100\n",
      "152/152 [==============================] - 82s 541ms/step - loss: 0.2022 - acc: 0.6205 - val_loss: 0.8917 - val_acc: 0.3129\n",
      "Epoch 21/100\n",
      "152/152 [==============================] - 82s 537ms/step - loss: 0.1951 - acc: 0.6369 - val_loss: 0.9050 - val_acc: 0.3108\n",
      "Epoch 22/100\n",
      "152/152 [==============================] - 82s 539ms/step - loss: 0.1858 - acc: 0.6523 - val_loss: 0.9084 - val_acc: 0.3145\n",
      "Epoch 23/100\n",
      "152/152 [==============================] - 81s 534ms/step - loss: 0.1791 - acc: 0.6639 - val_loss: 0.8968 - val_acc: 0.3153\n",
      "Epoch 24/100\n",
      "152/152 [==============================] - 79s 522ms/step - loss: 0.1703 - acc: 0.6798 - val_loss: 0.9332 - val_acc: 0.3110\n",
      "Epoch 25/100\n",
      "152/152 [==============================] - 78s 515ms/step - loss: 0.1632 - acc: 0.6938 - val_loss: 0.9232 - val_acc: 0.3159\n",
      "Epoch 26/100\n",
      "152/152 [==============================] - 78s 511ms/step - loss: 0.1569 - acc: 0.7080 - val_loss: 0.9327 - val_acc: 0.3140\n",
      "Epoch 27/100\n",
      "152/152 [==============================] - 77s 508ms/step - loss: 0.1486 - acc: 0.7211 - val_loss: 0.9288 - val_acc: 0.3149\n",
      "Epoch 28/100\n",
      "152/152 [==============================] - 78s 515ms/step - loss: 0.1443 - acc: 0.7330 - val_loss: 0.9580 - val_acc: 0.3128\n",
      "Epoch 29/100\n",
      "152/152 [==============================] - 79s 519ms/step - loss: 0.1367 - acc: 0.7457 - val_loss: 0.9781 - val_acc: 0.3110\n",
      "Epoch 30/100\n",
      "152/152 [==============================] - 79s 522ms/step - loss: 0.1310 - acc: 0.7559 - val_loss: 0.9677 - val_acc: 0.3124\n",
      "Epoch 31/100\n",
      "152/152 [==============================] - 79s 523ms/step - loss: 0.1246 - acc: 0.7701 - val_loss: 0.9713 - val_acc: 0.3132\n",
      "Epoch 32/100\n",
      "152/152 [==============================] - 77s 507ms/step - loss: 0.1196 - acc: 0.7818 - val_loss: 0.9773 - val_acc: 0.3129\n",
      "Epoch 33/100\n",
      "152/152 [==============================] - 99s 651ms/step - loss: 0.1126 - acc: 0.7954 - val_loss: 0.9862 - val_acc: 0.3129\n",
      "Epoch 34/100\n",
      "152/152 [==============================] - 121s 796ms/step - loss: 0.1080 - acc: 0.8040 - val_loss: 0.9866 - val_acc: 0.3136\n",
      "Epoch 35/100\n",
      "152/152 [==============================] - 118s 776ms/step - loss: 0.1042 - acc: 0.8153 - val_loss: 1.0110 - val_acc: 0.3104\n",
      "Epoch 36/100\n",
      "152/152 [==============================] - 124s 814ms/step - loss: 0.0990 - acc: 0.8244 - val_loss: 1.0079 - val_acc: 0.3127\n",
      "Epoch 37/100\n",
      "152/152 [==============================] - 120s 790ms/step - loss: 0.0948 - acc: 0.8303 - val_loss: 1.0266 - val_acc: 0.3122\n",
      "Epoch 38/100\n",
      "152/152 [==============================] - 117s 770ms/step - loss: 0.0903 - acc: 0.8414 - val_loss: 1.0190 - val_acc: 0.3117\n",
      "Epoch 39/100\n",
      "152/152 [==============================] - 111s 731ms/step - loss: 0.0861 - acc: 0.8540 - val_loss: 1.0231 - val_acc: 0.3121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/100\n",
      "152/152 [==============================] - 105s 693ms/step - loss: 0.0812 - acc: 0.8637 - val_loss: 1.0392 - val_acc: 0.3093\n",
      "Epoch 41/100\n",
      "152/152 [==============================] - 105s 692ms/step - loss: 0.0773 - acc: 0.8716 - val_loss: 1.0259 - val_acc: 0.3119\n",
      "Epoch 42/100\n",
      "152/152 [==============================] - 108s 711ms/step - loss: 0.0749 - acc: 0.8756 - val_loss: 1.0482 - val_acc: 0.3123\n",
      "Epoch 43/100\n",
      "152/152 [==============================] - 105s 695ms/step - loss: 0.0707 - acc: 0.8840 - val_loss: 1.0633 - val_acc: 0.3091\n",
      "Epoch 44/100\n",
      "152/152 [==============================] - 106s 698ms/step - loss: 0.0674 - acc: 0.8900 - val_loss: 1.0581 - val_acc: 0.3084\n",
      "Epoch 45/100\n",
      "152/152 [==============================] - 105s 694ms/step - loss: 0.0644 - acc: 0.8963 - val_loss: 1.0675 - val_acc: 0.3090\n",
      "Epoch 46/100\n",
      "152/152 [==============================] - 107s 703ms/step - loss: 0.0611 - acc: 0.9046 - val_loss: 1.0664 - val_acc: 0.3089\n",
      "Epoch 47/100\n",
      "152/152 [==============================] - 107s 707ms/step - loss: 0.0578 - acc: 0.9114 - val_loss: 1.0651 - val_acc: 0.3103\n",
      "Epoch 48/100\n",
      "152/152 [==============================] - 105s 690ms/step - loss: 0.0554 - acc: 0.9157 - val_loss: 1.0776 - val_acc: 0.3084\n",
      "Epoch 49/100\n",
      "152/152 [==============================] - 107s 707ms/step - loss: 0.0524 - acc: 0.9220 - val_loss: 1.0917 - val_acc: 0.3077\n",
      "Epoch 50/100\n",
      "152/152 [==============================] - 110s 726ms/step - loss: 0.0505 - acc: 0.9245 - val_loss: 1.0930 - val_acc: 0.3086\n",
      "Epoch 51/100\n",
      "152/152 [==============================] - 106s 702ms/step - loss: 0.0472 - acc: 0.9316 - val_loss: 1.0967 - val_acc: 0.3076\n",
      "Epoch 52/100\n",
      "152/152 [==============================] - 116s 765ms/step - loss: 0.0447 - acc: 0.9352 - val_loss: 1.1069 - val_acc: 0.3089\n",
      "Epoch 53/100\n",
      "152/152 [==============================] - 106s 694ms/step - loss: 0.0431 - acc: 0.9375 - val_loss: 1.1151 - val_acc: 0.3057\n",
      "Epoch 54/100\n",
      "152/152 [==============================] - 102s 668ms/step - loss: 0.0409 - acc: 0.9417 - val_loss: 1.1285 - val_acc: 0.3084\n",
      "Epoch 55/100\n",
      "152/152 [==============================] - 102s 674ms/step - loss: 0.0390 - acc: 0.9464 - val_loss: 1.1106 - val_acc: 0.3082\n",
      "Epoch 56/100\n",
      "152/152 [==============================] - 103s 677ms/step - loss: 0.0365 - acc: 0.9492 - val_loss: 1.1322 - val_acc: 0.3095\n",
      "Epoch 57/100\n",
      "152/152 [==============================] - 102s 672ms/step - loss: 0.0352 - acc: 0.9517 - val_loss: 1.1367 - val_acc: 0.3081\n",
      "Epoch 58/100\n",
      "152/152 [==============================] - 102s 669ms/step - loss: 0.0333 - acc: 0.9545 - val_loss: 1.1346 - val_acc: 0.3069\n",
      "Epoch 59/100\n",
      "152/152 [==============================] - 101s 663ms/step - loss: 0.0315 - acc: 0.9576 - val_loss: 1.1495 - val_acc: 0.3075\n",
      "Epoch 60/100\n",
      "152/152 [==============================] - 103s 676ms/step - loss: 0.0303 - acc: 0.9600 - val_loss: 1.1551 - val_acc: 0.3070\n",
      "Epoch 61/100\n",
      "152/152 [==============================] - 105s 688ms/step - loss: 0.0290 - acc: 0.9612 - val_loss: 1.1508 - val_acc: 0.3068\n",
      "Epoch 62/100\n",
      "152/152 [==============================] - 101s 666ms/step - loss: 0.0277 - acc: 0.9639 - val_loss: 1.1514 - val_acc: 0.3087\n",
      "Epoch 63/100\n",
      "152/152 [==============================] - 102s 671ms/step - loss: 0.0262 - acc: 0.9662 - val_loss: 1.1616 - val_acc: 0.3062\n",
      "Epoch 64/100\n",
      "152/152 [==============================] - 100s 661ms/step - loss: 0.0248 - acc: 0.9680 - val_loss: 1.1672 - val_acc: 0.3087\n",
      "Epoch 65/100\n",
      "152/152 [==============================] - 103s 678ms/step - loss: 0.0239 - acc: 0.9699 - val_loss: 1.1907 - val_acc: 0.3064\n",
      "Epoch 66/100\n",
      "152/152 [==============================] - 103s 677ms/step - loss: 0.0226 - acc: 0.9716 - val_loss: 1.1843 - val_acc: 0.3073\n",
      "Epoch 67/100\n",
      "152/152 [==============================] - 102s 672ms/step - loss: 0.0219 - acc: 0.9734 - val_loss: 1.1761 - val_acc: 0.3081\n",
      "Epoch 68/100\n",
      "152/152 [==============================] - 105s 690ms/step - loss: 0.0207 - acc: 0.9736 - val_loss: 1.1926 - val_acc: 0.3076\n",
      "Epoch 69/100\n",
      "152/152 [==============================] - 106s 697ms/step - loss: 0.0196 - acc: 0.9757 - val_loss: 1.2009 - val_acc: 0.3056\n",
      "Epoch 70/100\n",
      "152/152 [==============================] - 112s 738ms/step - loss: 0.0192 - acc: 0.9762 - val_loss: 1.1946 - val_acc: 0.3045\n",
      "Epoch 71/100\n",
      "152/152 [==============================] - 105s 691ms/step - loss: 0.0175 - acc: 0.9790 - val_loss: 1.2062 - val_acc: 0.3056\n",
      "Epoch 72/100\n",
      "152/152 [==============================] - 105s 692ms/step - loss: 0.0166 - acc: 0.9802 - val_loss: 1.2078 - val_acc: 0.3043\n",
      "Epoch 73/100\n",
      "152/152 [==============================] - 105s 690ms/step - loss: 0.0163 - acc: 0.9810 - val_loss: 1.2265 - val_acc: 0.3072\n",
      "Epoch 74/100\n",
      "152/152 [==============================] - 105s 694ms/step - loss: 0.0153 - acc: 0.9821 - val_loss: 1.2213 - val_acc: 0.3075\n",
      "Epoch 75/100\n",
      "152/152 [==============================] - 108s 713ms/step - loss: 0.0147 - acc: 0.9819 - val_loss: 1.2212 - val_acc: 0.3063\n",
      "Epoch 76/100\n",
      "152/152 [==============================] - 106s 697ms/step - loss: 0.0138 - acc: 0.9837 - val_loss: 1.2292 - val_acc: 0.3072\n",
      "Epoch 77/100\n",
      "152/152 [==============================] - 105s 694ms/step - loss: 0.0132 - acc: 0.9849 - val_loss: 1.2359 - val_acc: 0.3052\n",
      "Epoch 78/100\n",
      "152/152 [==============================] - 107s 702ms/step - loss: 0.0131 - acc: 0.9839 - val_loss: 1.2318 - val_acc: 0.3042\n",
      "Epoch 79/100\n",
      "152/152 [==============================] - 107s 704ms/step - loss: 0.0124 - acc: 0.9846 - val_loss: 1.2161 - val_acc: 0.3062\n",
      "Epoch 80/100\n",
      "152/152 [==============================] - 110s 723ms/step - loss: 0.0118 - acc: 0.9859 - val_loss: 1.2376 - val_acc: 0.3039\n",
      "Epoch 81/100\n",
      "152/152 [==============================] - 106s 697ms/step - loss: 0.0113 - acc: 0.9863 - val_loss: 1.2350 - val_acc: 0.3033\n",
      "Epoch 82/100\n",
      "152/152 [==============================] - 106s 700ms/step - loss: 0.0107 - acc: 0.9866 - val_loss: 1.2456 - val_acc: 0.3053\n",
      "Epoch 83/100\n",
      "152/152 [==============================] - 108s 709ms/step - loss: 0.0103 - acc: 0.9877 - val_loss: 1.2453 - val_acc: 0.3042\n",
      "Epoch 84/100\n",
      "152/152 [==============================] - 109s 717ms/step - loss: 0.0097 - acc: 0.9878 - val_loss: 1.2519 - val_acc: 0.3028\n",
      "Epoch 85/100\n",
      "152/152 [==============================] - 106s 697ms/step - loss: 0.0092 - acc: 0.9880 - val_loss: 1.2664 - val_acc: 0.3034\n",
      "Epoch 86/100\n",
      "152/152 [==============================] - 106s 698ms/step - loss: 0.0088 - acc: 0.9887 - val_loss: 1.2641 - val_acc: 0.3028\n",
      "Epoch 87/100\n",
      "152/152 [==============================] - 106s 700ms/step - loss: 0.0085 - acc: 0.9892 - val_loss: 1.2625 - val_acc: 0.3054\n",
      "Epoch 88/100\n",
      "152/152 [==============================] - 106s 701ms/step - loss: 0.0081 - acc: 0.9891 - val_loss: 1.2728 - val_acc: 0.3026\n",
      "Epoch 89/100\n",
      "152/152 [==============================] - 108s 714ms/step - loss: 0.0076 - acc: 0.9892 - val_loss: 1.2723 - val_acc: 0.3037\n",
      "Epoch 90/100\n",
      "152/152 [==============================] - 107s 705ms/step - loss: 0.0074 - acc: 0.9897 - val_loss: 1.2783 - val_acc: 0.3035\n",
      "Epoch 91/100\n",
      "152/152 [==============================] - 110s 722ms/step - loss: 0.0073 - acc: 0.9894 - val_loss: 1.2847 - val_acc: 0.3038\n",
      "Epoch 92/100\n",
      "152/152 [==============================] - 109s 717ms/step - loss: 0.0069 - acc: 0.9904 - val_loss: 1.2974 - val_acc: 0.3035\n",
      "Epoch 93/100\n",
      "152/152 [==============================] - 106s 700ms/step - loss: 0.0065 - acc: 0.9908 - val_loss: 1.2993 - val_acc: 0.3022\n",
      "Epoch 94/100\n",
      "152/152 [==============================] - 111s 730ms/step - loss: 0.0062 - acc: 0.9906 - val_loss: 1.2924 - val_acc: 0.3025\n",
      "Epoch 95/100\n",
      "152/152 [==============================] - 107s 703ms/step - loss: 0.0060 - acc: 0.9912 - val_loss: 1.3067 - val_acc: 0.3037\n",
      "Epoch 96/100\n",
      "152/152 [==============================] - 106s 698ms/step - loss: 0.0056 - acc: 0.9912 - val_loss: 1.3054 - val_acc: 0.3033\n",
      "Epoch 97/100\n",
      "152/152 [==============================] - 106s 699ms/step - loss: 0.0056 - acc: 0.9905 - val_loss: 1.3047 - val_acc: 0.3020\n",
      "Epoch 98/100\n",
      "152/152 [==============================] - 109s 716ms/step - loss: 0.0053 - acc: 0.9912 - val_loss: 1.3187 - val_acc: 0.3032\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 109s 720ms/step - loss: 0.0050 - acc: 0.9915 - val_loss: 1.3140 - val_acc: 0.3029\n",
      "Epoch 100/100\n",
      "152/152 [==============================] - 106s 701ms/step - loss: 0.0048 - acc: 0.9910 - val_loss: 1.3165 - val_acc: 0.3041\n"
     ]
    }
   ],
   "source": [
    "combined_model, encoder_model, decoder_model = model_seq_to_seq(batch_size=64, epochs=100, mask_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: 0    مرحبا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  hi _END\n",
      "-\n",
      "Input sentence: 1    اركض\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  run _END\n",
      "-\n",
      "Input sentence: 2    اخفض رأسك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  duck _END\n",
      "-\n",
      "Input sentence: 3    اخفضي رأسك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  duck _END\n",
      "-\n",
      "Input sentence: 4    اخفضوا رؤوسكم\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  duck _END\n",
      "-\n",
      "Input sentence: 5    النجده\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  help _END\n",
      "-\n",
      "Input sentence: 6    اقفز\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  jump _END\n",
      "-\n",
      "Input sentence: 7    قف\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  stand up _END\n",
      "-\n",
      "Input sentence: 8    توقف \n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  stop _END\n",
      "-\n",
      "Input sentence: 9    إنتظر\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  wait _END\n",
      "-\n",
      "Input sentence: 10    داوم\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  go on _END\n",
      "-\n",
      "Input sentence: 11    استمر\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  go on _END\n",
      "-\n",
      "Input sentence: 12    مرحبا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  hi _END\n",
      "-\n",
      "Input sentence: 13    تعجل\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  hurry _END\n",
      "-\n",
      "Input sentence: 14    استعجل\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  hurry _END\n",
      "-\n",
      "Input sentence: 15    انا اري\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i see _END\n",
      "-\n",
      "Input sentence: 16    أنا فزت\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i won _END\n",
      "-\n",
      "Input sentence: 17    استرح\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  take a rest _END\n",
      "-\n",
      "Input sentence: 18    ابتسم\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  smile _END\n",
      "-\n",
      "Input sentence: 19    في صحتك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  cheers _END\n",
      "-\n",
      "Input sentence: 20    هل فهمت\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  got it _END\n",
      "-\n",
      "Input sentence: 21    ركض\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he ran _END\n",
      "-\n",
      "Input sentence: 22    أعرف\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i know _END\n",
      "-\n",
      "Input sentence: 23    أعلم ذلك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i know _END\n",
      "-\n",
      "Input sentence: 24    أنا أعلم\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i know _END\n",
      "-\n",
      "Input sentence: 25    أنا في \n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im  _END\n",
      "-\n",
      "Input sentence: 26    أنا بخير\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im ok _END\n",
      "-\n",
      "Input sentence: 27    استمع\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  listen _END\n",
      "-\n",
      "Input sentence: 28    غير معقول\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  no way _END\n",
      "-\n",
      "Input sentence: 29    حقا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  really _END\n",
      "-\n",
      "Input sentence: 30    شكرا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  thanks _END\n",
      "-\n",
      "Input sentence: 31    شكرا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  thanks _END\n",
      "-\n",
      "Input sentence: 32    لماذا أنا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  why me _END\n",
      "-\n",
      "Input sentence: 33    رائع\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  terrific _END\n",
      "-\n",
      "Input sentence: 34    خذ راحتك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  be cool _END\n",
      "-\n",
      "Input sentence: 35    أغرب عن وجهي\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  beat it _END\n",
      "-\n",
      "Input sentence: 36    هاتفني\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  call me _END\n",
      "-\n",
      "Input sentence: 37    اتصل بي\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  call me _END\n",
      "-\n",
      "Input sentence: 38    تفضل بالدخول\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  please come in _END\n",
      "-\n",
      "Input sentence: 39    تعال إلى الداخل\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  come in _END\n",
      "-\n",
      "Input sentence: 40    بالله عليك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  come on _END\n",
      "-\n",
      "Input sentence: 41    هيا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  come on _END\n",
      "-\n",
      "Input sentence: 42    هيا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  come on _END\n",
      "-\n",
      "Input sentence: 43    اخرج من هنا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  get out of here _END\n",
      "-\n",
      "Input sentence: 44    أخرج\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  get out _END\n",
      "-\n",
      "Input sentence: 45    اخرج\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  get out _END\n",
      "-\n",
      "Input sentence: 46    اتركني و شأني\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  leave me alone _END\n",
      "-\n",
      "Input sentence: 47    اذهب بعيدا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  go away _END\n",
      "-\n",
      "Input sentence: 48    ارحل\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  go away _END\n",
      "-\n",
      "Input sentence: 49    مع السلامه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  goodbye _END\n",
      "-\n",
      "Input sentence: 50    إلى اللقاء\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  see you again _END\n",
      "-\n",
      "Input sentence: 51    إنتظر\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  wait _END\n",
      "-\n",
      "Input sentence: 52    لقد أتى\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he came _END\n",
      "-\n",
      "Input sentence: 53    هو يجري\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he runs _END\n",
      "-\n",
      "Input sentence: 54    ساعدني\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  help me _END\n",
      "-\n",
      "Input sentence: 55    النجده ساعدني\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  help me _END\n",
      "-\n",
      "Input sentence: 56    ساعدوني\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  help me _END\n",
      "-\n",
      "Input sentence: 57    انتظر\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  wait _END\n",
      "-\n",
      "Input sentence: 58    أنا موافق\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i agree _END\n",
      "-\n",
      "Input sentence: 59    أنا حزين\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im sad _END\n",
      "-\n",
      "Input sentence: 60    أنا أيضا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  me too _END\n",
      "-\n",
      "Input sentence: 61    اخرس\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  shut up _END\n",
      "-\n",
      "Input sentence: 62    اصمت\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  shut up _END\n",
      "-\n",
      "Input sentence: 63    اسكت\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  shut up _END\n",
      "-\n",
      "Input sentence: 64    أغلق فمك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  shut up _END\n",
      "-\n",
      "Input sentence: 65    أوقفه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  stop it _END\n",
      "-\n",
      "Input sentence: 66    خذه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  take it _END\n",
      "-\n",
      "Input sentence: 67    أخبرني\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  tell me _END\n",
      "-\n",
      "Input sentence: 68    توم فاز\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  tom won _END\n",
      "-\n",
      "Input sentence: 69    لقد ربح توم\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  tom won _END\n",
      "-\n",
      "Input sentence: 70    استيقظ\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  wake up _END\n",
      "-\n",
      "Input sentence: 71    أهلا و سهلا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  welcome _END\n",
      "-\n",
      "Input sentence: 72    مرحبا بك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  welcome _END\n",
      "-\n",
      "Input sentence: 73    اهلا وسهلا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  welcome _END\n",
      "-\n",
      "Input sentence: 74    مرحبا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  hi _END\n",
      "-\n",
      "Input sentence: 75    من فاز\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  who won _END\n",
      "-\n",
      "Input sentence: 76    من الذي ربح\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  who won _END\n",
      "-\n",
      "Input sentence: 77    لم لا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  why not _END\n",
      "-\n",
      "Input sentence: 78    لما لا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  why not _END\n",
      "-\n",
      "Input sentence: 79    لا فكره لدي\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  no one is reading _END\n",
      "-\n",
      "Input sentence: 80    استمتع بوقتك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  have fun _END\n",
      "-\n",
      "Input sentence: 81    أسرعا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  hurry up _END\n",
      "-\n",
      "Input sentence: 82    لقد نسيت\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i forgot _END\n",
      "-\n",
      "Input sentence: 83    فهمته\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i got it _END\n",
      "-\n",
      "Input sentence: 84    فهمتها\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i got it _END\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: 85    فهمت ذلك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i got it _END\n",
      "-\n",
      "Input sentence: 86    أستخدمه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i use it _END\n",
      "-\n",
      "Input sentence: 87    سأدفع أنا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  ill pay _END\n",
      "-\n",
      "Input sentence: 88    أنا مشغول\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im busy _END\n",
      "-\n",
      "Input sentence: 89    إنني مشغول\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im busy _END\n",
      "-\n",
      "Input sentence: 90    أشعر بالبرد\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im freezing _END\n",
      "-\n",
      "Input sentence: 91    أنا حر\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im free _END\n",
      "-\n",
      "Input sentence: 92    أنا هنا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im here _END\n",
      "-\n",
      "Input sentence: 93    لقد عدت إلى البيت\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im home _END\n",
      "-\n",
      "Input sentence: 94    أنا فقير\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im poor _END\n",
      "-\n",
      "Input sentence: 95    أنا ثري\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im rich _END\n",
      "-\n",
      "Input sentence: 96    هذا مؤلم\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  it hurts _END\n",
      "-\n",
      "Input sentence: 97    انها جافه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  its dry _END\n",
      "-\n",
      "Input sentence: 98    الجو حار\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  its hot _END\n",
      "-\n",
      "Input sentence: 99    إنه جديد\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  its new _END\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(100): #[14077,20122,40035,40064, 40056, 40068, 40090, 40095, 40100, 40119, 40131, 40136, 40150, 40153]:\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', dataset.Arabic_input[seq_index: seq_index + 1])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tring a long sentence and notice what will happen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: 9600    كم ساعه نوم تحتاج\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  how many hours of sleep do you need _END\n",
      "-\n",
      "Input sentence: 9601    إلى كم ساعه من النوم تحتاج\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  how many hours of sleep do you need _END\n",
      "-\n",
      "Input sentence: 9602    كم من الوقت سوف تستغرق بعد\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  how much longer is it going to take _END\n",
      "-\n",
      "Input sentence: 9603    أسرع و إلا فاتتك الطائره\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  hurry up or youll miss your plane _END\n",
      "-\n",
      "Input sentence: 9604    أسرع و ستلحق بالقطار\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  hurry and you will catch the train _END\n",
      "-\n",
      "Input sentence: 9605    أنا موافق ولكن على شرط واحد\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i accept but only on one condition _END\n",
      "-\n",
      "Input sentence: 9606    سأشاهد التلفاز هذا المساء\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i am going to watch tv this evening _END\n",
      "-\n",
      "Input sentence: 9607    ضحكت فجأه عندما رأيته\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i burst out laughing when i saw him _END\n",
      "-\n",
      "Input sentence: 9608    أستطيع أن أحمل تلك الحقائب عنك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i can carry those suitcases for you _END\n",
      "-\n",
      "Input sentence: 9609    أقصى مده انتظار يمكنني تحملها هي أربعه أيام\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i can wait four days at the longest _END\n",
      "-\n",
      "Input sentence: 9610    لا أقدر أن أسامحه على ما فعل\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i cant forgive him for what he did _END\n",
      "-\n",
      "Input sentence: 9611    لا أستطيع أن أفكر في مثال مناسب\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i cant think of a suitable example _END\n",
      "-\n",
      "Input sentence: 9612    لا أستطيع فهم أفكاره على الإطلاق\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i cant understand his ideas at all _END\n",
      "-\n",
      "Input sentence: 9613    لست راغبا جدا بالثروه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i dont have much desire for wealth _END\n",
      "-\n",
      "Input sentence: 9614    لا أعرف لأني لم أكن هناك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i dont know because i wasnt there _END\n",
      "-\n",
      "Input sentence: 9615    لا أعرف كيف أتعامل مع الأطفال\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i dont know how to handle children _END\n",
      "-\n",
      "Input sentence: 9616    لا أعلم لم ينبغي أن نقوم بذلك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i dont know why we have to do that _END\n",
      "-\n",
      "Input sentence: 9617    أنا لا أحب السباحه في المياه المالحه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i dont like swimming in salt water _END\n",
      "-\n",
      "Input sentence: 9618    لا افضل ان اتكلم مع الغرباء\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i dont like talking with strangers _END\n",
      "-\n",
      "Input sentence: 9619    لا أدعي فهم النساء\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i dont pretend to understand women _END\n",
      "-\n",
      "Input sentence: 9620    لا أدعي أنني أفهم النساء\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i dont pretend to understand women _END\n",
      "-\n",
      "Input sentence: 9621    أشك في أن يستطيع أحد فعل ذلك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i dont think anybody could do that _END\n",
      "-\n",
      "Input sentence: 9622    لا أريد القيام بعمله نيابه عنه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i dont want to do his work for him _END\n",
      "-\n",
      "Input sentence: 9623    لا ارغب ان ابدو كسائح \n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i dont want to look like a tourist _END\n",
      "-\n",
      "Input sentence: 9624    لا أريدك أن تنساني\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i dont want you to forget about me _END\n",
      "-\n",
      "Input sentence: 9625    لقد شعرت بلمسه خفيفه على كتفي\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i felt a light touch on my shoulder _END\n",
      "-\n",
      "Input sentence: 9626    أنا منحت توم فرصه للعمل معي\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i gave tom a chance to work with me _END\n",
      "-\n",
      "Input sentence: 9627    تعبت من الرقود في السرير طوال اليوم\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i got tired of lying in bed all day _END\n",
      "-\n",
      "Input sentence: 9628    استيقظت الساعه السادسه و النصف هذا الصباح\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i got up at sixthirty this morning _END\n",
      "-\n",
      "Input sentence: 9629    لعله من الأفضل أن أعود لعملي\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i guess id better get back to work _END\n",
      "-\n",
      "Input sentence: 9630    لم أعتقد أن توم من الممكن أن يكون جدا قاسي\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i had no idea tom could be so cruel _END\n",
      "-\n",
      "Input sentence: 9631    لدي سؤال أريد سؤالك عنه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i have a question i need to ask you _END\n",
      "-\n",
      "Input sentence: 9632    عندي سؤال علي سؤالك عنه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i have a question i need to ask you _END\n",
      "-\n",
      "Input sentence: 9633    آمل أن هذه معلومات ذات مصداقيه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i hope this is accurate information _END\n",
      "-\n",
      "Input sentence: 9634    علمت أن شيئا طريفا قد يحدث\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i knew something funny might happen _END\n",
      "-\n",
      "Input sentence: 9635    أنا أعرف من هو الذي تبحث عنه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i know who it is youre looking for _END\n",
      "-\n",
      "Input sentence: 9636    تعلمت الكثير عن الحضاره الإغريقيه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i learned a lot about greek culture _END\n",
      "-\n",
      "Input sentence: 9637    تركت له الباقي وخرجت\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i left the rest to him and went out _END\n",
      "-\n",
      "Input sentence: 9638    أحب أختها الصغرى كثيرا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i like her younger sister very much _END\n",
      "-\n",
      "Input sentence: 9639    احتاج ان اتحدث معك الآن\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i need to speak with you right away _END\n",
      "-\n",
      "Input sentence: 9640    لم أرغب ابدا في الواقع أن أذهب إلى هناك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i never actually wanted to go there _END\n",
      "-\n",
      "Input sentence: 9641    أقنعته بالتخلي عن الفكره\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i persuaded him to give up the idea _END\n",
      "-\n",
      "Input sentence: 9642    وضعت الهلام في الثلاجه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i put the jello in the refrigerator _END\n",
      "-\n",
      "Input sentence: 9643    رأيت توم يرقص مع فتاه أخرى\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i saw tom dancing with another girl _END\n",
      "-\n",
      "Input sentence: 9644    أتخطى الإعلانات على الفيديوهات كلما أمكن\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i skip ads on videos whenever i can _END\n",
      "-\n",
      "Input sentence: 9645    أظن بأن توم لا يفهم الفرنسيه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i think tom cant understand french _END\n",
      "-\n",
      "Input sentence: 9646    كان القيام بذلك شيئا سخيفا على ما أعتقد\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i think it was a stupid thing to do _END\n",
      "-\n",
      "Input sentence: 9647    أعتقد أنه ينبغي عليك أن تقرأه بنفسك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i think you should read it yourself _END\n",
      "-\n",
      "Input sentence: 9648    أعتقد أنك تعمل خطأ كبير\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i think youre making a big mistake _END\n",
      "-\n",
      "Input sentence: 9649    ظننت هذا معلوما\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i thought this was common knowledge _END\n",
      "-\n",
      "Input sentence: 9650    اعتقدت انك لن تأتي الى هنا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i thought you would never come here _END\n",
      "-\n",
      "Input sentence: 9651    اعتقدت انك لا تريد ان تأتي الى هنا \n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i thought you would never come here _END\n",
      "-\n",
      "Input sentence: 9652    أرغب بأن يحضر عشائي لغرفتي\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i want my dinner brought to my room _END\n",
      "-\n",
      "Input sentence: 9653    أريد أن أعرف من سيأتي معنا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i want to know whos coming with us _END\n",
      "-\n",
      "Input sentence: 9654    أردت شراء لوحه من توم\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i wanted to buy a painting from tom _END\n",
      "-\n",
      "Input sentence: 9655    أزعجني بكاء الطفل\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i was bothered by the babys crying _END\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: 9656    تأخرت بسبب زحمه الطريق الشديده\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i was late because of heavy traffic _END\n",
      "-\n",
      "Input sentence: 9657    ذهبت لشرب البيره مع الأصدقاء\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i went to drink a beer with friends _END\n",
      "-\n",
      "Input sentence: 9658    ذهبت إلى المقهى بالأمس\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i went to the coffee shop yesterday _END\n",
      "-\n",
      "Input sentence: 9659    أتمنى لو أعيش قريبا من منزلك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i wish i could live near your house _END\n",
      "-\n",
      "Input sentence: 9660    لن أتركك تفلت بهذه السهوله\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i wont let you slip away so easily _END\n",
      "-\n",
      "Input sentence: 9661    أعمل كل يوم عدا يوم الأحد\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i work every day except sunday _END\n",
      "-\n",
      "Input sentence: 9662    أعمل كل يوم إلا يوم الأحد\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i work every day except sunday _END\n",
      "-\n",
      "Input sentence: 9663    أود أن تقابل والداي\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i would like you to meet my parents _END\n",
      "-\n",
      "Input sentence: 9664    أريد أن أحجز طاوله لشخصين\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  id like to reserve a table for two _END\n",
      "-\n",
      "Input sentence: 9665    سأسأل عن موعد وصول القطار\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  ill ask when the train will get out _END\n",
      "-\n",
      "Input sentence: 9666    سأجلب لك الفاتوره حالا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  ill bring you the bill immediately _END\n",
      "-\n",
      "Input sentence: 9667    سأخذ أشعه سينيه لأسنانك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  ill take some xrays of your teeth _END\n",
      "-\n",
      "Input sentence: 9668    سأخبرك بما أريد فعله\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  ill tell you what i want you to do _END\n",
      "-\n",
      "Input sentence: 9669    سأنزل عند المحطه القادمه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im getting off at the next station _END\n",
      "-\n",
      "Input sentence: 9670    ساعثر علي مكان خاص بي\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im going to find a place of my own _END\n",
      "-\n",
      "Input sentence: 9671    أنا أقل بقليل من  سم\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im just under  centimeters tall _END\n",
      "-\n",
      "Input sentence: 9672    أنا أقل بقليل من  سنتي متر\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im just under  centimeters tall _END\n",
      "-\n",
      "Input sentence: 9673    أنا أتتطلع لرحله التزلج خاصتنا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im looking forward to our ski trip _END\n",
      "-\n",
      "Input sentence: 9674    أنا حتى لست متأكدا إن كان هذا مفتاحي\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im not even sure if this is my key _END\n",
      "-\n",
      "Input sentence: 9675    لن أذهب إلى المدرسه اليوم\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im not going to go to school today _END\n",
      "-\n",
      "Input sentence: 9676    لن أذهب للمدرسه يوم السبت\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im not going to school on saturday _END\n",
      "-\n",
      "Input sentence: 9677    من الأرجح أنني لن أنس ذلك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im not likely to forget to do that _END\n",
      "-\n",
      "Input sentence: 9678    لن أخذ عطله هذا العام\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im not taking a vacation this year _END\n",
      "-\n",
      "Input sentence: 9679    انا لست عدوك انا صديقك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im not your enemy im your friend _END\n",
      "-\n",
      "Input sentence: 9680    متأكد بأن توم يخطط لفعل ذلك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im sure tom is planning to do that _END\n",
      "-\n",
      "Input sentence: 9681    متأكد بأن توم سيخبرنا بالحقيقه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im sure tom will tell us the truth _END\n",
      "-\n",
      "Input sentence: 9682    أحاول ترك مكان للتحليه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im trying to save room for dessert _END\n",
      "-\n",
      "Input sentence: 9683    أحاول ترك مكان للحلوى\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im trying to save room for dessert _END\n",
      "-\n",
      "Input sentence: 9684    لقد إنتهيت للتو من تناول الإفطار\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  ive just finished eating breakfast _END\n",
      "-\n",
      "Input sentence: 9685    لقد وعدت توم بأن أكون هناك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  ive promised tom that id be there _END\n",
      "-\n",
      "Input sentence: 9686    إن احتجت المال سأطلبه من والدي\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  if i need money ill ask my father _END\n",
      "-\n",
      "Input sentence: 9687    إن لم تذكرني فسأنسى\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  if you dont remind me ill forget _END\n",
      "-\n",
      "Input sentence: 9688    في هذه الحاله أعتقد أنه على حق\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  in this case i think he is correct _END\n",
      "-\n",
      "Input sentence: 9689    أهذا هو الكتاب الذي تبحث عنه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  is this the book youre looking for _END\n",
      "-\n",
      "Input sentence: 9690    حدث ذلك كما قال توم\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  it happened the way tom said it did _END\n",
      "-\n",
      "Input sentence: 9691    ليس من السهل حل المشكله\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  it is not easy to solve the problem _END\n",
      "-\n",
      "Input sentence: 9692    انها ليست بارده هنا كما توقعت\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  it isnt as cold here as i expected _END\n",
      "-\n",
      "Input sentence: 9693    كان إهمالا منها أن تنسى\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  it was careless of her to forget it _END\n",
      "-\n",
      "Input sentence: 9694    من المستحيل القول بشكل قطعي\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  its impossible to tell for certain _END\n",
      "-\n",
      "Input sentence: 9695    اليابان وكوريا الجنوبيه جارتان\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  japan and south korea are neighbors _END\n",
      "-\n",
      "Input sentence: 9696    سجل ما تنفق\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  keep a record of how much you spend _END\n",
      "-\n",
      "Input sentence: 9697    لقد تم اختراع الطائره الورقيه قبل ألفي عام\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  kites were invented  years ago _END\n",
      "-\n",
      "Input sentence: 9698    دعنا لا نستعجل كثيرا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  lets not be in too much of a hurry _END\n",
      "-\n",
      "Input sentence: 9699    لنجلس أين يوجد ظل\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  lets sit where there is some shade _END\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(9800, 9900):\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', dataset.Arabic_input[seq_index: seq_index + 1])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Here although the accuracy is a small value. But if we notice the translation is correct. The accuracy in problems of sequence to sequence can't be cosidered as a metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "- https://github.com/motazsaad/process-arabic-text/blob/master/clean_arabic_text.py\n",
    "- https://thescipub.com/pdf/jcssp.2020.117.125.pdf\n",
    "- Hands -On Python Natural Language Processing Book for Aman Kedia and Mayank Rasu\n",
    "- https://colab.research.google.com/drive/1dhlc3Nt_LvZcxY5tUd-XLU1fvGt4pPuh?usp=sharing\n",
    "- https://github.com/CAMeL-Lab/camel_tools#installing-data\n",
    "- https://colab.research.google.com/drive/1Y3qCbD6Gw1KEw-lixQx1rI6WlyWnrnDS?usp=sharing#scrollTo=9knGLLGg7cnm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
