{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence Models:\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Sequence to Sequence Architecture is one of the important architectures in the neural network. It helps in tasks like Machine translation, text generation, chatbot, ..etc.\n",
    "\n",
    "The Sequence to Sequence architecture is named as Seq2Seq modeling because it tries to convert one sequence into another.\n",
    "\n",
    "Let's explain an example to understand more, in Machine Translation, we feed to the network input an Arabic sentence and obtain an English sentence from the output. So, here is the Seq2Seq modeling output English sentence from Arabic sentence. Although the Arabic letters are in another format which is completely different from English letters. How the Seq2Seq knew the sentence in English from Arabic?\n",
    "\n",
    "Good question! let's go deeper to know how Seq2Seq does that for us. To answer this question we should know the Seq2Seq architecture.\n",
    "    \n",
    "## Seq2Seq Architecture\n",
    "\n",
    "Seq2Seq architecture consists of:\n",
    "- Encoder and its output context vector.\n",
    "- Decoder.\n",
    "\n",
    "The Encoder and decoder together have been the following shape:\n",
    "\n",
    "![title](img/encoder_decoder_arch.png)\n",
    "\n",
    "\n",
    "\n",
    "### Encoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Job:\n",
    "\n",
    "This is the 1st component in the encoder-decoder architecture.  It builds the representation of the input vector and embeds the input meaning inside the context vector.\n",
    "\n",
    "- Architecture:\n",
    "\n",
    "It can be built using the LSTM, RNN, GRU, BiLSTM. The last hidden state of the encoder will hold the context of the entire input sentence. The encoder tries to build and embedding for the entire input sentence.\n",
    "\n",
    "- How does it work?\n",
    "\n",
    "To be able to imagine how it works, please look at the image below:\n",
    "\n",
    "![title](img/encoder.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the above image, the embedding of input words is fed into LSTM stack sequentially, then each cell hidden state is fed into the next LSTM until we reach to the last one where we obtain the context vector from the last cell hidden state.\n",
    "\n",
    "- Using Keras: \n",
    "\n",
    "Feeding the encoder last hidden state using keras, just enable the return_state to enable the LSTM statck to return the last hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "- Job:\n",
    "\n",
    "Generally, 'it decodes the context vector to have the same required output'. In out example, It builds the English representation from the Arabic words encoded inside the context vector. \n",
    "\n",
    "- Architecture:\n",
    "\n",
    "Also, It can be built using RNN, LSTM, GRU.\n",
    "\n",
    "- How does it work?\n",
    "\n",
    "To be able to understand the decoder well, we should realize that the decoder has two methods of working according to the  stage. There are two stages in any machine learning algorithm training, inference or prediction. The decoder work is a little bit difrent in training and inference. \n",
    "\n",
    "#### Decorder in Training\n",
    "\n",
    "\n",
    "![title](img/decoder_Training.png)\n",
    "\n",
    "\n",
    "As in the above image, the decoder the decoder receives the context vector from the encoder, and start token to start its work to predict the 1st word according to the the received context vector. The start token will be input to the 1st cell in the decoder. But the 2nd LSTM cell as in above image will receive the correct token from training data not from the previous LSTM output as in infernece 'As we will see later'. This input token from training data will help the 2nd LSTM with the previous hidden state and cell state to predict the 2nd word correctly. Then then the 2nd LSTM output the predcited 3rd token and its feed its hidden and cell state to the 3rd LSTM. This senario will be repeated unitl the decoder see end token or the reach to the maximum length.\n",
    "\n",
    "\n",
    "#### Decoder in inference or prediction\n",
    "\n",
    "![title](img/decoder_inference.png)\n",
    "\n",
    "As in the above image, the decoder receives the context vector from the encoder. The decoder 1st cell input starts, once the cell sees start token inside the input, it starts to predict the 1st word according to the received context vector. Then this word is fed to the next LSTM as input to help it decode the second word with the help of the hidden state that comes from the previous LSTM. The decoder works with this way until it reaches to the max_length or until it finds the word 'end'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Translation using Seq2Seq Modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting camel-tools\n",
      "  Downloading camel_tools-1.2.0.tar.gz (58 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting future\n",
      "  Using cached future-0.18.2-py3-none-any.whl\n",
      "Requirement already satisfied: six in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from camel-tools) (1.15.0)\n",
      "Collecting docopt\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: cachetools in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from camel-tools) (4.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from camel-tools) (1.19.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from camel-tools) (1.6.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\marwa\\appdata\\roaming\\python\\python38\\site-packages (from camel-tools) (1.1.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from camel-tools) (0.24.1)\n",
      "Collecting dill\n",
      "  Using cached dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "Requirement already satisfied: torch>=1.3 in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from camel-tools) (1.8.1)\n",
      "Requirement already satisfied: transformers>=3.0.2 in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from camel-tools) (4.15.0)\n",
      "Collecting editdistance\n",
      "  Downloading editdistance-0.6.0-cp38-cp38-win_amd64.whl (24 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\marwa\\appdata\\roaming\\python\\python38\\site-packages (from camel-tools) (2.25.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from torch>=1.3->camel-tools) (4.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers>=3.0.2->camel-tools) (3.4.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers>=3.0.2->camel-tools) (2021.4.4)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers>=3.0.2->camel-tools) (0.10.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers>=3.0.2->camel-tools) (4.54.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers>=3.0.2->camel-tools) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers>=3.0.2->camel-tools) (21.3)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers>=3.0.2->camel-tools) (0.0.47)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from transformers>=3.0.2->camel-tools) (0.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas->camel-tools) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\marwa\\appdata\\roaming\\python\\python38\\site-packages (from pandas->camel-tools) (2020.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\marwa\\appdata\\roaming\\python\\python38\\site-packages (from requests->camel-tools) (1.26.2)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\marwa\\appdata\\roaming\\python\\python38\\site-packages (from requests->camel-tools) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests->camel-tools) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\marwa\\appdata\\roaming\\python\\python38\\site-packages (from requests->camel-tools) (2.10)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn->camel-tools) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn->camel-tools) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from packaging>=20.0->transformers>=3.0.2->camel-tools) (2.4.7)\n",
      "Requirement already satisfied: click in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from sacremoses->transformers>=3.0.2->camel-tools) (8.0.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from click->sacremoses->transformers>=3.0.2->camel-tools) (0.4.4)\n",
      "Building wheels for collected packages: camel-tools, docopt\n",
      "  Building wheel for camel-tools (setup.py): started\n",
      "  Building wheel for camel-tools (setup.py): finished with status 'done'\n",
      "  Created wheel for camel-tools: filename=camel_tools-1.2.0-py3-none-any.whl size=99029 sha256=0c626af20eb25c7722f2c8d404adbe823fc00f6b26ed67f7e56c6989cadd53e0\n",
      "  Stored in directory: c:\\users\\marwa\\appdata\\local\\pip\\cache\\wheels\\b8\\3b\\9f\\910d7d11709d8be2fb2fe4ced865d3b5a7d0c5de2fa10b4823\n",
      "  Building wheel for docopt (setup.py): started\n",
      "  Building wheel for docopt (setup.py): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=0646051b2aefec8475a9e022ce341994ae41b40768081ec283a16cacc765e1fa\n",
      "  Stored in directory: c:\\users\\marwa\\appdata\\local\\pip\\cache\\wheels\\56\\ea\\58\\ead137b087d9e326852a851351d1debf4ada529b6ac0ec4e8c\n",
      "Successfully built camel-tools docopt\n",
      "Installing collected packages: future, editdistance, docopt, dill, camel-tools\n",
      "Successfully installed camel-tools-1.2.0 dill-0.3.4 docopt-0.6.2 editdistance-0.6.0 future-0.18.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 22.0.2 is available.\n",
      "You should consider upgrading via the 'c:\\users\\marwa\\appdata\\local\\programs\\python\\python38\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "#pip install camel-tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense,LSTM,Embedding,Input\n",
    "import io\n",
    "from string import digits\n",
    "import string\n",
    "import tkseem as tk\n",
    "# instantiate the Maximum Likelihood Disambiguator\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "from camel_tools.disambig.mle import MLEDisambiguator\n",
    "from camel_tools.tokenizers.morphological import MorphologicalTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining the dataset for Arabic and English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file):\n",
    "    file_data = []\n",
    "    data = []\n",
    "    Arabic_data = []\n",
    "    english_data = []\n",
    "    # Read the file lines\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        file_data = f.readlines()\n",
    "    # separate the lines using '\\t'\n",
    "    for line in (file_data):\n",
    "        english_sent, arabic_sent, _ = line.split('\\t')\n",
    "        Arabic_data.append(arabic_sent)\n",
    "        english_data.append(english_sent)\n",
    "        \n",
    "    return english_data, Arabic_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'datasets/ara-eng/ara.txt'\n",
    "english_data,  Arabic_data= read_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi.',\n",
       " 'Run!',\n",
       " 'Duck!',\n",
       " 'Duck!',\n",
       " 'Duck!',\n",
       " 'Help!',\n",
       " 'Jump!',\n",
       " 'Stop!',\n",
       " 'Stop!',\n",
       " 'Wait!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['إلى اللقاء',\n",
       " 'إنتظر',\n",
       " 'لقد أتى.',\n",
       " 'هو يجري',\n",
       " 'ساعدني!',\n",
       " 'النجدة! ساعدني!',\n",
       " 'ساعدوني',\n",
       " 'انتظر.',\n",
       " 'أنا موافق',\n",
       " 'أنا حزين.',\n",
       " 'أنا أيضاً.',\n",
       " 'اخرس!',\n",
       " 'اصمت!',\n",
       " 'اسكت!',\n",
       " 'أغلق فمك!',\n",
       " 'أوقفه',\n",
       " 'خذه',\n",
       " 'أخبرني',\n",
       " 'توم فاز.',\n",
       " 'لقد ربح توم.',\n",
       " 'استيقظ!',\n",
       " 'أهلاً و سهلاً!',\n",
       " 'مرحباً بك!',\n",
       " 'اهلا وسهلا',\n",
       " 'مرحبا!',\n",
       " 'من فاز؟',\n",
       " 'من الذي ربح؟',\n",
       " 'لم لا؟',\n",
       " 'لما لا؟',\n",
       " 'لا فكرة لدي',\n",
       " 'استمتع بوقتك.',\n",
       " 'أسرعا.',\n",
       " 'لقد نسيت.',\n",
       " 'فهمتُهُ.',\n",
       " 'فهمتُها.',\n",
       " 'فَهمتُ ذلك.',\n",
       " 'أستخدمه.',\n",
       " 'سأدفع أنا.',\n",
       " 'أنا مشغول.',\n",
       " 'إنني مشغول.',\n",
       " 'أشعر بالبرد.',\n",
       " 'أنا حُرّ.',\n",
       " 'أنا هنا',\n",
       " 'لقد عدت إلى البيت',\n",
       " 'أنا فقير.',\n",
       " 'أنا ثري.',\n",
       " 'هذا مؤلم',\n",
       " 'انها جافه',\n",
       " 'الجو حار',\n",
       " 'إنه جديد']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Arabic_data[50:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12158"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Arabic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12158"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 12158 line in both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame({'Arabic_input':Arabic_data, 'English_target':english_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Arabic_input</th>\n",
       "      <th>English_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>مرحبًا.</td>\n",
       "      <td>Hi.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>اركض!</td>\n",
       "      <td>Run!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>اخفض رأسك!</td>\n",
       "      <td>Duck!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>اخفضي رأسك!</td>\n",
       "      <td>Duck!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>اخفضوا رؤوسكم!</td>\n",
       "      <td>Duck!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Arabic_input English_target\n",
       "0         مرحبًا.            Hi.\n",
       "1           اركض!           Run!\n",
       "2      اخفض رأسك!          Duck!\n",
       "3     اخفضي رأسك!          Duck!\n",
       "4  اخفضوا رؤوسكم!          Duck!"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arabic Normalization\n",
    "The Arabic has a special preprocessing, why?\n",
    "\n",
    "Arabic has difirrent characteristics like:\n",
    "- The word in Arabic can mean a complete sentence in other languages. So, it requires a special segmentation step which considers the Arabic language rules. \n",
    "- Arabic has diactrics which should be normalized. \n",
    "- Some Aabic letters has more than one shape so, it should be unified.\n",
    "\n",
    "If you would like to know more about Arabic charasteristics, please read this part  \"Arabic Challenges in the Context of NER\" in the the following paper:\n",
    "\n",
    "https://thescipub.com/pdf/jcssp.2020.117.125.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the following repo: https://github.com/motazsaad/process-arabic-text/blob/master/clean_arabic_text.py\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = arabic_punctuations + english_punctuations\n",
    "\n",
    "arabic_diacritics = re.compile(\"\"\"\n",
    "                             ّ    | # Tashdid\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "\n",
    "\n",
    "def normalize_arabic(text):\n",
    "    #text = re.sub(\"[إأآا]\", \"ا\", text)\n",
    "\n",
    "    #text = re.sub(\"ى\", \"ي\", text)\n",
    "    #text = re.sub(\"ؤ\", \"ء\", text)\n",
    "    #text = re.sub(\"ئ\", \"ء\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    text = re.sub(\"گ\", \"ك\", text)\n",
    "    return text\n",
    "\n",
    "def remove_digits(text):\n",
    "    text = re.sub(r\"[1234567890١٢٣٤٥٦٧٨٩٠]+\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def remove_english_characters(text):\n",
    "    text = re.sub(r'[a-zA-Z]+','',text)\n",
    "    return text\n",
    "    \n",
    "\n",
    "def remove_diacritics(text):\n",
    "    text = re.sub(arabic_diacritics, '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Arabic_normalization(Arabic_sentence_list):\n",
    "    Arabic_data_list = []\n",
    "    for item in Arabic_sentence_list:\n",
    "        text = remove_english_characters(item)\n",
    "        text = remove_digits(text)\n",
    "        text = normalize_arabic(text)\n",
    "        text = remove_diacritics(text)\n",
    "        text = remove_punctuations(text)\n",
    "        Arabic_data_list.append(text)\n",
    "        \n",
    "    return Arabic_data_list    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Arabic_input'] = Arabic_normalization(dataset.Arabic_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            مرحبا\n",
       "1             اركض\n",
       "2        اخفض رأسك\n",
       "3       اخفضي رأسك\n",
       "4    اخفضوا رؤوسكم\n",
       "5           النجده\n",
       "6             اقفز\n",
       "7               قف\n",
       "8            توقف \n",
       "9            إنتظر\n",
       "Name: Arabic_input, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Arabic_input'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English Normalization:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def English_normalization(English_target_ls):\n",
    "    English_data_list = []\n",
    "    # Since we work on word level, if we normalize the text to lower case, this will reduce the vocabulary. \n",
    "    #It's easy to recover the case later. \n",
    "    English_data_list = English_target_ls.apply(lambda x: x.lower())\n",
    "\n",
    "    # Clean up punctuations and digits. Such special chars are common to both domains, and can just be copied with no error.\n",
    "    exclude = set(string.punctuation)\n",
    "    English_data_list = English_data_list.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    English_data_list = English_data_list.apply(lambda x: x.translate(remove_digits))\n",
    "    \n",
    "    return English_data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['English_target'] = English_normalization(dataset.English_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Arabic_input</th>\n",
       "      <th>English_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>مرحبا</td>\n",
       "      <td>hi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>اركض</td>\n",
       "      <td>run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>اخفض رأسك</td>\n",
       "      <td>duck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>اخفضي رأسك</td>\n",
       "      <td>duck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>اخفضوا رؤوسكم</td>\n",
       "      <td>duck</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Arabic_input English_target\n",
       "0          مرحبا             hi\n",
       "1           اركض            run\n",
       "2      اخفض رأسك           duck\n",
       "3     اخفضي رأسك           duck\n",
       "4  اخفضوا رؤوسكم           duck"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation:\n",
    "In this step we start to convert the data for training. \n",
    "- Adding the start and end token to the target language.Since the english is our target, so we will add the start and tokens to it. lets see how can we do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_tok = 'START_'\n",
    "end_tok = '_END'\n",
    "def data_prep():\n",
    "    dataset.English_target = dataset.English_target.apply(lambda x : st_tok + ' ' + x + ' ' + end_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Arabic_input</th>\n",
       "      <th>English_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>مرحبا</td>\n",
       "      <td>START_ hi _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>اركض</td>\n",
       "      <td>START_ run _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>اخفض رأسك</td>\n",
       "      <td>START_ duck _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>اخفضي رأسك</td>\n",
       "      <td>START_ duck _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>اخفضوا رؤوسكم</td>\n",
       "      <td>START_ duck _END</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Arabic_input    English_target\n",
       "0          مرحبا    START_ hi _END\n",
       "1           اركض   START_ run _END\n",
       "2      اخفض رأسك  START_ duck _END\n",
       "3     اخفضي رأسك  START_ duck _END\n",
       "4  اخفضوا رؤوسكم  START_ duck _END"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenization\n",
    "Arabic tokenization is completely diffrent from English tokenization. English tokenization depends on spaces, but in Arabic this is not valid. Since the token in Arabic can be used to mean a complete sentence in another languages.\n",
    "\n",
    "Note, I used her the camel_tools which is not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arabic Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "\n",
    "def tokenize_Arabic():\n",
    "    # The tokenizer expects pre-tokenized text\n",
    "    Arabic_input_ls = dataset.Arabic_input.apply(simple_word_tokenize)\n",
    "\n",
    "    # Load a pretrained disambiguator to use with a tokenizer\n",
    "    mle = MLEDisambiguator.pretrained('calima-msa-r13')\n",
    "\n",
    "    # By specifying `split=True`, the morphological tokens are output as seperate\n",
    "    # strings.\n",
    "    tokenizer = MorphologicalTokenizer(mle,scheme='d3tok', split=True)\n",
    "    Arabic_input_ls = Arabic_input_ls.apply(tokenizer.tokenize)\n",
    "    \n",
    "    return Arabic_input_ls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_plus(tokens):\n",
    "    sentence_ls = []\n",
    "    for token in tokens:\n",
    "        if '+' in token:\n",
    "            token_without_plus = token.replace('+','')\n",
    "            sentence_ls.append(token_without_plus) \n",
    "        else:\n",
    "            sentence_ls.append(token) \n",
    "\n",
    "            \n",
    "    return sentence_ls\n",
    "#Arabic_input_ls = Arabic_input_ls.apply(remove_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Arabic_input</th>\n",
       "      <th>English_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>مرحبا</td>\n",
       "      <td>START_ hi _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>اركض</td>\n",
       "      <td>START_ run _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>اخفض رأسك</td>\n",
       "      <td>START_ duck _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>اخفضي رأسك</td>\n",
       "      <td>START_ duck _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>اخفضوا رؤوسكم</td>\n",
       "      <td>START_ duck _END</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Arabic_input    English_target\n",
       "0          مرحبا    START_ hi _END\n",
       "1           اركض   START_ run _END\n",
       "2      اخفض رأسك  START_ duck _END\n",
       "3     اخفضي رأسك  START_ duck _END\n",
       "4  اخفضوا رؤوسكم  START_ duck _END"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds_copy = dataset.copy()\n",
    "tokenized_ds_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The camel tool returned the hamza letter again but in a unified way for all the words. I mean the same word can't exist in two difrrent spellings.\n",
    "#### English tokenization\n",
    "English is tokenized according to spaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_split_word2word(data):\n",
    "    return data.split(' ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_stats(tokenized_ds_copy):\n",
    "    #Obtain the tokenized words in Arabic\n",
    "    tokenized_ds_copy['Arabic_input'] = tokenize_Arabic()\n",
    "    # The tokenization output has + in the separated token, which should be removed\n",
    "    tokenized_ds_copy['Arabic_input'] = tokenized_ds_copy.Arabic_input.apply(remove_plus)\n",
    "    \n",
    "    #create a set to hold all Arabic words uniquely.\n",
    "    input_tokens=set()\n",
    "    for item in tokenized_ds_copy.Arabic_input:\n",
    "        for tok in item:\n",
    "            input_tokens.add(tok)\n",
    "    \n",
    "    #Obtain the tokenized words in English dataset\n",
    "    tokenized_ds_copy['English_target'] = tokenized_ds_copy.English_target.apply(tok_split_word2word)\n",
    "    \n",
    "    #create a set to hold all English words uniquely.\n",
    "    target_tokens=set()\n",
    "    for item in tokenized_ds_copy.English_target:\n",
    "        for tok in item:\n",
    "            target_tokens.add(tok)\n",
    "        \n",
    "    input_tokens = sorted(list(input_tokens))\n",
    "    target_tokens = sorted(list(target_tokens))\n",
    "\n",
    "\n",
    "    \n",
    "    num_encoder_tokens = len(input_tokens)\n",
    "    num_decoder_tokens = len(target_tokens)\n",
    "    \n",
    "    #To obtin the maximum number of words inside Arabic and English dataset.\n",
    "    max_encoder_seq_length = np.max([len(l) for l in tokenized_ds_copy.Arabic_input])\n",
    "    max_decoder_seq_length = np.max([len(l) for l in tokenized_ds_copy.English_target])\n",
    "\n",
    "    return input_tokens, target_tokens, num_encoder_tokens, num_decoder_tokens, max_encoder_seq_length, max_decoder_seq_length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens, target_tokens, num_encoder_tokens, num_decoder_tokens, max_encoder_seq_length, max_decoder_seq_length  = data_stats(tokenized_ds_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 12158\n",
      "Number of unique input tokens: 7205\n",
      "Number of unique output tokens: 4298\n",
      "Max sequence length for inputs: 52\n",
      "Max sequence length for outputs: 36\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples:', len(dataset))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "\n",
    "In this step we will build our vocab2int table which will be used to map between words and their indices.'Machine Learning can't work directly with words since computer doen't understand words, so it should be converted into numbers'.\n",
    "\n",
    "Note that the pad and separation should be considered during obtaining the vocab2int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_tok = 'PAD'\n",
    "sep_tok = ' '\n",
    "special_tokens = [pad_tok, sep_tok, st_tok, end_tok] \n",
    "\n",
    "#Increase the number of token by the number of special characters.\n",
    "num_encoder_tokens += len(special_tokens)\n",
    "num_decoder_tokens += len(special_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab(input_tokens, target_tokens):\n",
    "    input_token_index = {}\n",
    "    target_token_index = {}\n",
    "    for i,tok in enumerate(special_tokens):\n",
    "        input_token_index[tok] = i\n",
    "        target_token_index[tok] = i \n",
    "\n",
    "    offset = len(special_tokens)\n",
    "    for i, tok in enumerate(input_tokens):\n",
    "        input_token_index[tok] = i+offset\n",
    "\n",
    "    for i, tok in enumerate(target_tokens):\n",
    "        target_token_index[tok] = i+offset\n",
    "   \n",
    "    # Reverse-lookup token index to decode sequences back to something readable.\n",
    "    reverse_input_tok_index = dict(\n",
    "        (i, tok) for tok, i in input_token_index.items())\n",
    "    reverse_target_tok_index = dict(\n",
    "        (i, tok) for tok, i in target_token_index.items())\n",
    "    return input_token_index, target_token_index, reverse_input_tok_index, reverse_target_tok_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index, target_token_index, reverse_input_tok_index, reverse_target_tok_index = vocab(input_tokens, target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PAD': 0,\n",
       " ' ': 1,\n",
       " 'START_': 2,\n",
       " '_END': 3,\n",
       " 'NOAN': 4,\n",
       " 'آب': 5,\n",
       " 'آباء': 6,\n",
       " 'آبد': 7,\n",
       " 'آبقو': 8,\n",
       " 'آت': 9,\n",
       " 'آتون': 10,\n",
       " 'آتي': 11,\n",
       " 'آثار': 12,\n",
       " 'آخذ': 13,\n",
       " 'آخر': 14,\n",
       " 'آخرة': 15,\n",
       " 'آخرعلي': 16,\n",
       " 'آخرون': 17,\n",
       " 'آخرين': 18,\n",
       " 'آدم': 19,\n",
       " 'آذار': 20,\n",
       " 'آذان': 21,\n",
       " 'آذيتم': 22,\n",
       " 'آراء': 23,\n",
       " 'آرية': 24,\n",
       " 'آسفون': 25,\n",
       " 'آسيا': 26,\n",
       " 'آفاق': 27,\n",
       " 'آكل': 28,\n",
       " 'آلاف': 29,\n",
       " 'آلام': 30,\n",
       " 'آلة': 31,\n",
       " 'آلن': 32,\n",
       " 'آلي': 33,\n",
       " 'آليا': 34,\n",
       " 'آمال': 35,\n",
       " 'آمل': 36,\n",
       " 'آمن': 37,\n",
       " 'آمنة': 38,\n",
       " 'آن': 39,\n",
       " 'آنا': 40,\n",
       " 'آنذاك': 41,\n",
       " 'آنس': 42,\n",
       " 'آني': 43,\n",
       " 'آية': 44,\n",
       " 'أ': 45,\n",
       " 'أأريتها': 46,\n",
       " 'أأشتري': 47,\n",
       " 'أأنت': 48,\n",
       " 'أؤجر': 49,\n",
       " 'أؤذي': 50,\n",
       " 'أؤكد': 51,\n",
       " 'أؤلف': 52,\n",
       " 'أؤمن': 53,\n",
       " 'أإلى': 54,\n",
       " 'أاصيب': 55,\n",
       " 'أب': 56,\n",
       " 'أبإمكانك': 57,\n",
       " 'أبا': 58,\n",
       " 'أبتاع': 59,\n",
       " 'أبتز': 60,\n",
       " 'أبتسم': 61,\n",
       " 'أبتل': 62,\n",
       " 'أبحار': 63,\n",
       " 'أبحث': 64,\n",
       " 'أبد': 65,\n",
       " 'أبدأ': 66,\n",
       " 'أبدا': 67,\n",
       " 'أبدو': 68,\n",
       " 'أبدوا': 69,\n",
       " 'أبدين': 70,\n",
       " 'أبذل': 71,\n",
       " 'أبر': 72,\n",
       " 'أبرد': 73,\n",
       " 'أبريل': 74,\n",
       " 'أبشع': 75,\n",
       " 'أبعد': 76,\n",
       " 'أبق': 77,\n",
       " 'أبقار': 78,\n",
       " 'أبقى': 79,\n",
       " 'أبقيت': 80,\n",
       " 'أبقينا': 81,\n",
       " 'أبكر': 82,\n",
       " 'أبلغ': 83,\n",
       " 'أبلي': 84,\n",
       " 'أبليت': 85,\n",
       " 'أبناء': 86,\n",
       " 'أبو': 87,\n",
       " 'أبواب': 88,\n",
       " 'أبوي': 89,\n",
       " 'أبي': 90,\n",
       " 'أبيض': 91,\n",
       " 'أبيضا': 92,\n",
       " 'أبيع': 93,\n",
       " 'أبيه': 94,\n",
       " 'أتأخر': 95,\n",
       " 'أتأسف': 96,\n",
       " 'أتأكد': 97,\n",
       " 'أتابع': 98,\n",
       " 'أتبع': 99,\n",
       " 'أتت': 100,\n",
       " 'أتتطلع': 101,\n",
       " 'أتحب': 102,\n",
       " 'أتحبان': 103,\n",
       " 'أتحبني': 104,\n",
       " 'أتحبينني': 105,\n",
       " 'أتحدث': 106,\n",
       " 'أتحدثت': 107,\n",
       " 'أتحس': 108,\n",
       " 'أتخذ': 109,\n",
       " 'أتخرج': 110,\n",
       " 'أتخصص': 111,\n",
       " 'أتخطى': 112,\n",
       " 'أتخيل': 113,\n",
       " 'أتدرس': 114,\n",
       " 'أتذكر': 115,\n",
       " 'أترجم': 116,\n",
       " 'أترع': 117,\n",
       " 'أترك': 118,\n",
       " 'أتريد': 119,\n",
       " 'أتزوج': 120,\n",
       " 'أتساءل': 121,\n",
       " 'أتستطيع': 122,\n",
       " 'أتسكن': 123,\n",
       " 'أتسلق': 124,\n",
       " 'أتسلى': 125,\n",
       " 'أتسمح': 126,\n",
       " 'أتسوق': 127,\n",
       " 'أتصفح': 128,\n",
       " 'أتصل': 129,\n",
       " 'أتضح': 130,\n",
       " 'أتضن': 131,\n",
       " 'أتضور': 132,\n",
       " 'أتطلع': 133,\n",
       " 'أتظن': 134,\n",
       " 'أتظنها': 135,\n",
       " 'أتعامل': 136,\n",
       " 'أتعب': 137,\n",
       " 'أتعتقد': 138,\n",
       " 'أتعجب': 139,\n",
       " 'أتعديني': 140,\n",
       " 'أتعرف': 141,\n",
       " 'أتعلم': 142,\n",
       " 'أتعمد': 143,\n",
       " 'أتغدى': 144,\n",
       " 'أتفاجئ': 145,\n",
       " 'أتفرج': 146,\n",
       " 'أتفضل': 147,\n",
       " 'أتفق': 148,\n",
       " 'أتقبلون': 149,\n",
       " 'أتكلم': 150,\n",
       " 'أتل': 151,\n",
       " 'أتلعب': 152,\n",
       " 'أتلقف': 153,\n",
       " 'أتم': 154,\n",
       " 'أتمتلك': 155,\n",
       " 'أتمشى': 156,\n",
       " 'أتملك': 157,\n",
       " 'أتممنا': 158,\n",
       " 'أتمنتى': 159,\n",
       " 'أتمنى': 160,\n",
       " 'أتناول': 161,\n",
       " 'أتنسى': 162,\n",
       " 'أتهم': 163,\n",
       " 'أتوا': 164,\n",
       " 'أتواصل': 165,\n",
       " 'أتوتر': 166,\n",
       " 'أتوجد': 167,\n",
       " 'أتود': 168,\n",
       " 'أتوسل': 169,\n",
       " 'أتوقع': 170,\n",
       " 'أتوقف': 171,\n",
       " 'أتولى': 172,\n",
       " 'أتون': 173,\n",
       " 'أتى': 174,\n",
       " 'أتيت': 175,\n",
       " 'أتينا': 176,\n",
       " 'أتی': 177,\n",
       " 'أثاث': 178,\n",
       " 'أثار': 179,\n",
       " 'أثر': 180,\n",
       " 'أثرت': 181,\n",
       " 'أثق': 182,\n",
       " 'أثقل': 183,\n",
       " 'أثلجت': 184,\n",
       " 'أثناء': 185,\n",
       " 'أجاب': 186,\n",
       " 'أجابت': 187,\n",
       " 'أجانب': 188,\n",
       " 'أجب': 189,\n",
       " 'أجبت': 190,\n",
       " 'أجبرت': 191,\n",
       " 'أجبروا': 192,\n",
       " 'أجبن': 193,\n",
       " 'أجتاح': 194,\n",
       " 'أجد': 195,\n",
       " 'أجداد': 196,\n",
       " 'أجدد': 197,\n",
       " 'أجر': 198,\n",
       " 'أجرب': 199,\n",
       " 'أجرح': 200,\n",
       " 'أجريت': 201,\n",
       " 'أجزاء': 202,\n",
       " 'أجعل': 203,\n",
       " 'أجل': 204,\n",
       " 'أجلا': 205,\n",
       " 'أجلب': 206,\n",
       " 'أجلت': 207,\n",
       " 'أجلس': 208,\n",
       " 'أجمع': 209,\n",
       " 'أجمل': 210,\n",
       " 'أجنبية': 211,\n",
       " 'أجني': 212,\n",
       " 'أجهزة': 213,\n",
       " 'أجهشت': 214,\n",
       " 'أجيب': 215,\n",
       " 'أجيد': 216,\n",
       " 'أح': 217,\n",
       " 'أحافظ': 218,\n",
       " 'أحال': 219,\n",
       " 'أحاول': 220,\n",
       " 'أحب': 221,\n",
       " 'أحببت': 222,\n",
       " 'أحبت': 223,\n",
       " 'أحتاج': 224,\n",
       " 'أحترق': 225,\n",
       " 'أحترم': 226,\n",
       " 'أحتمل': 227,\n",
       " 'أحجز': 228,\n",
       " 'أحد': 229,\n",
       " 'أحدا': 230,\n",
       " 'أحدثت': 231,\n",
       " 'أحذ': 232,\n",
       " 'أحذر': 233,\n",
       " 'أحذية': 234,\n",
       " 'أحر': 235,\n",
       " 'أحراج': 236,\n",
       " 'أحرجت': 237,\n",
       " 'أحرز': 238,\n",
       " 'أحزن': 239,\n",
       " 'أحس': 240,\n",
       " 'أحسب': 241,\n",
       " 'أحسد': 242,\n",
       " 'أحسست': 243,\n",
       " 'أحسن': 244,\n",
       " 'أحسنت': 245,\n",
       " 'أحصل': 246,\n",
       " 'أحضر': 247,\n",
       " 'أحضرت': 248,\n",
       " 'أحفظ': 249,\n",
       " 'أحقا': 250,\n",
       " 'أحكام': 251,\n",
       " 'أحكى': 252,\n",
       " 'أحكي': 253,\n",
       " 'أحل': 254,\n",
       " 'أحلام': 255,\n",
       " 'أحلاما': 256,\n",
       " 'أحلم': 257,\n",
       " 'أحلى': 258,\n",
       " 'أحمر': 259,\n",
       " 'أحمرا': 260,\n",
       " 'أحمق': 261,\n",
       " 'أحمقا': 262,\n",
       " 'أحمل': 263,\n",
       " 'أحمي': 264,\n",
       " 'أحوال': 265,\n",
       " 'أحول': 266,\n",
       " 'أحي': 267,\n",
       " 'أحياء': 268,\n",
       " 'أحيان': 269,\n",
       " 'أحيانا': 270,\n",
       " 'أخ': 271,\n",
       " 'أخا': 272,\n",
       " 'أخاف': 273,\n",
       " 'أخافت': 274,\n",
       " 'أخبار': 275,\n",
       " 'أخبر': 276,\n",
       " 'أخبرت': 277,\n",
       " 'أخبرنا': 278,\n",
       " 'أخبروا': 279,\n",
       " 'أخبريني': 280,\n",
       " 'أخبز': 281,\n",
       " 'أخت': 282,\n",
       " 'أختان': 283,\n",
       " 'أختتم': 284,\n",
       " 'أختر': 285,\n",
       " 'أخترع': 286,\n",
       " 'أختفي': 287,\n",
       " 'أختلف': 288,\n",
       " 'أخجل': 289,\n",
       " 'أخدود': 290,\n",
       " 'أخذ': 291,\n",
       " 'أخذت': 292,\n",
       " 'أخذل': 293,\n",
       " 'أخذنا': 294,\n",
       " 'أخرج': 295,\n",
       " 'أخرجت': 296,\n",
       " 'أخرس': 297,\n",
       " 'أخرق': 298,\n",
       " 'أخرني': 299,\n",
       " 'أخرى': 300,\n",
       " 'أخسر': 301,\n",
       " 'أخشى': 302,\n",
       " 'أخضر': 303,\n",
       " 'أخطأ': 304,\n",
       " 'أخطأت': 305,\n",
       " 'أخطاء': 306,\n",
       " 'أخطط': 307,\n",
       " 'أخطيء': 308,\n",
       " 'أخفض': 309,\n",
       " 'أخفف': 310,\n",
       " 'أخفي': 311,\n",
       " 'أخلد': 312,\n",
       " 'أخلط': 313,\n",
       " 'أخلع': 314,\n",
       " 'أخلف': 315,\n",
       " 'أخمص': 316,\n",
       " 'أخمن': 317,\n",
       " 'أخو': 318,\n",
       " 'أخوات': 319,\n",
       " 'أخوض': 320,\n",
       " 'أخون': 321,\n",
       " 'أخيب': 322,\n",
       " 'أخير': 323,\n",
       " 'أخيرا': 324,\n",
       " 'أخيرة': 325,\n",
       " 'أداء': 326,\n",
       " 'أدب': 327,\n",
       " 'أدبا': 328,\n",
       " 'أدخل': 329,\n",
       " 'أدخلت': 330,\n",
       " 'أدخن': 331,\n",
       " 'أدرس': 332,\n",
       " 'أدرك': 333,\n",
       " 'أدركت': 334,\n",
       " 'أدري': 335,\n",
       " 'أدع': 336,\n",
       " 'أدعم': 337,\n",
       " 'أدعى': 338,\n",
       " 'أدفأ': 339,\n",
       " 'أدفع': 340,\n",
       " 'أدلة': 341,\n",
       " 'أدلى': 342,\n",
       " 'أدنی': 343,\n",
       " 'أدو': 344,\n",
       " 'أدوات': 345,\n",
       " 'أدوار': 346,\n",
       " 'أدوية': 347,\n",
       " 'أدير': 348,\n",
       " 'أدين': 349,\n",
       " 'أذاكر': 350,\n",
       " 'أذاهب': 351,\n",
       " 'أذق': 352,\n",
       " 'أذكر': 353,\n",
       " 'أذكى': 354,\n",
       " 'أذهب': 355,\n",
       " 'أذهبت': 356,\n",
       " 'أذى': 357,\n",
       " 'أر': 358,\n",
       " 'أرأى': 359,\n",
       " 'أراد': 360,\n",
       " 'أرادت': 361,\n",
       " 'أراسل': 362,\n",
       " 'أرافق': 363,\n",
       " 'أراقب': 364,\n",
       " 'أرانب': 365,\n",
       " 'أربط': 366,\n",
       " 'أربع': 367,\n",
       " 'أربعا': 368,\n",
       " 'أربعاء': 369,\n",
       " 'أربعة': 370,\n",
       " 'أربعون': 371,\n",
       " 'أربعين': 372,\n",
       " 'أرتاح': 373,\n",
       " 'أرتقي': 374,\n",
       " 'أرتكب': 375,\n",
       " 'أرجح': 376,\n",
       " 'أرجع': 377,\n",
       " 'أرجو': 378,\n",
       " 'أرحل': 379,\n",
       " 'أرخص': 380,\n",
       " 'أرد': 381,\n",
       " 'أردت': 382,\n",
       " 'أردنا': 383,\n",
       " 'أردي': 384,\n",
       " 'أرز': 385,\n",
       " 'أرسل': 386,\n",
       " 'أرسلت': 387,\n",
       " 'أرسم': 388,\n",
       " 'أرض': 389,\n",
       " 'أرضا': 390,\n",
       " 'أرضي': 391,\n",
       " 'أرضية': 392,\n",
       " 'أرغب': 393,\n",
       " 'أرغم': 394,\n",
       " 'أرغمت': 395,\n",
       " 'أرفع': 396,\n",
       " 'أرقص': 397,\n",
       " 'أركب': 398,\n",
       " 'أركز': 399,\n",
       " 'أركض': 400,\n",
       " 'أرم': 401,\n",
       " 'أرمل': 402,\n",
       " 'أرملة': 403,\n",
       " 'أرنب': 404,\n",
       " 'أرني': 405,\n",
       " 'أرهقت': 406,\n",
       " 'أروع': 407,\n",
       " 'أرى': 408,\n",
       " 'أري': 409,\n",
       " 'أرياف': 410,\n",
       " 'أريتني': 411,\n",
       " 'أريد': 412,\n",
       " 'أريكة': 413,\n",
       " 'أريكتي': 414,\n",
       " 'أريني': 415,\n",
       " 'أزاحت': 416,\n",
       " 'أزال': 417,\n",
       " 'أزالت': 418,\n",
       " 'أزرق': 419,\n",
       " 'أزرقا': 420,\n",
       " 'أزعج': 421,\n",
       " 'أزل': 422,\n",
       " 'أزلت': 423,\n",
       " 'أزمات': 424,\n",
       " 'أزمة': 425,\n",
       " 'أزمت': 426,\n",
       " 'أزهار': 427,\n",
       " 'أزور': 428,\n",
       " 'أزياء': 429,\n",
       " 'أس': 430,\n",
       " 'أسأت': 431,\n",
       " 'أسأل': 432,\n",
       " 'أسئل': 433,\n",
       " 'أسئلة': 434,\n",
       " 'أساء': 435,\n",
       " 'أسابيع': 436,\n",
       " 'أساسي': 437,\n",
       " 'أساعد': 438,\n",
       " 'أسافر': 439,\n",
       " 'أساكوسا': 440,\n",
       " 'أسامحك': 441,\n",
       " 'أسامحه': 442,\n",
       " 'أسباب': 443,\n",
       " 'أسبح': 444,\n",
       " 'أسبوع': 445,\n",
       " 'أسبوعا': 446,\n",
       " 'أسبوعيا': 447,\n",
       " 'أسبوعين': 448,\n",
       " 'أستاذ': 449,\n",
       " 'أستاذة': 450,\n",
       " 'أستحق': 451,\n",
       " 'أستحم': 452,\n",
       " 'أستخدم': 453,\n",
       " 'أستدر': 454,\n",
       " 'أستدعي': 455,\n",
       " 'أستذهب': 456,\n",
       " 'أستراليا': 457,\n",
       " 'أستساعدهم': 458,\n",
       " 'أستطع': 459,\n",
       " 'أستطيع': 460,\n",
       " 'أستعد': 461,\n",
       " 'أستعمل': 462,\n",
       " 'أستعوب': 463,\n",
       " 'أستعير': 464,\n",
       " 'أستغرق': 465,\n",
       " 'أستقيل': 466,\n",
       " 'أستلقي': 467,\n",
       " 'أستلم': 468,\n",
       " 'أستمتع': 469,\n",
       " 'أستمطر': 470,\n",
       " 'أستمع': 471,\n",
       " 'أستيقظ': 472,\n",
       " 'أسحب': 473,\n",
       " 'أسد': 474,\n",
       " 'أسدد': 475,\n",
       " 'أسرة': 476,\n",
       " 'أسرتي': 477,\n",
       " 'أسرع': 478,\n",
       " 'أسرعا': 479,\n",
       " 'أسرق': 480,\n",
       " 'أسطوانة': 481,\n",
       " 'أسطورة': 482,\n",
       " 'أسطوري': 483,\n",
       " 'أسعار': 484,\n",
       " 'أسعد': 485,\n",
       " 'أسعدت': 486,\n",
       " 'أسعدنياجعلني': 487,\n",
       " 'أسف': 488,\n",
       " 'أسفل': 489,\n",
       " 'أسق': 490,\n",
       " 'أسقطت': 491,\n",
       " 'أسكت': 492,\n",
       " 'أسكتنا': 493,\n",
       " 'أسكن': 494,\n",
       " 'أسلب': 495,\n",
       " 'أسلحة': 496,\n",
       " 'أسماء': 497,\n",
       " 'أسماك': 498,\n",
       " 'أسمح': 499,\n",
       " 'أسمع': 500,\n",
       " 'أسمعت': 501,\n",
       " 'أسنان': 502,\n",
       " 'أسهم': 503,\n",
       " 'أسوأ': 504,\n",
       " 'أسود': 505,\n",
       " 'أسودا': 506,\n",
       " 'أسورة': 507,\n",
       " 'أشأ': 508,\n",
       " 'أشار': 509,\n",
       " 'أشارك': 510,\n",
       " 'أشاهد': 511,\n",
       " 'أشباح': 512,\n",
       " 'أشتاق': 513,\n",
       " 'أشتري': 514,\n",
       " 'أشتكي': 515,\n",
       " 'أشتية': 516,\n",
       " 'أشجار': 517,\n",
       " 'أشخاص': 518,\n",
       " 'أشرب': 519,\n",
       " 'أشرت': 520,\n",
       " 'أشرح': 521,\n",
       " 'أشرق': 522,\n",
       " 'أشعار': 523,\n",
       " 'أشعارا': 524,\n",
       " 'أشعة': 525,\n",
       " 'أشعر': 526,\n",
       " 'أشعل': 527,\n",
       " 'أشقاء': 528,\n",
       " 'أشك': 529,\n",
       " 'أشكال': 530,\n",
       " 'أشكر': 531,\n",
       " 'أشلاء': 532,\n",
       " 'أشم': 533,\n",
       " 'أشهر': 534,\n",
       " 'أشياء': 535,\n",
       " 'أصاب': 536,\n",
       " 'أصبت': 537,\n",
       " 'أصبح': 538,\n",
       " 'أصبحا': 539,\n",
       " 'أصبحت': 540,\n",
       " 'أصبحنا': 541,\n",
       " 'أصبحوا': 542,\n",
       " 'أصحاب': 543,\n",
       " 'أصحو': 544,\n",
       " 'أصحيح': 545,\n",
       " 'أصدر': 546,\n",
       " 'أصدق': 547,\n",
       " 'أصدقاء': 548,\n",
       " 'أصر': 549,\n",
       " 'أصطاد': 550,\n",
       " 'أصطحب': 551,\n",
       " 'أصعب': 552,\n",
       " 'أصغر': 553,\n",
       " 'أصغيتم': 554,\n",
       " 'أصفر': 555,\n",
       " 'أصل': 556,\n",
       " 'أصلا': 557,\n",
       " 'أصلح': 558,\n",
       " 'أصلحت': 559,\n",
       " 'أصلع': 560,\n",
       " 'أصليين': 561,\n",
       " 'أصم': 562,\n",
       " 'أصمت': 563,\n",
       " 'أصنع': 564,\n",
       " 'أصوات': 565,\n",
       " 'أصوت': 566,\n",
       " 'أصوليين': 567,\n",
       " 'أصيب': 568,\n",
       " 'أصيبت': 569,\n",
       " 'أصيبوا': 570,\n",
       " 'أصير': 571,\n",
       " 'أضحت': 572,\n",
       " 'أضحك': 573,\n",
       " 'أضرب': 574,\n",
       " 'أضطر': 575,\n",
       " 'أضع': 576,\n",
       " 'أضعاف': 577,\n",
       " 'أضعت': 578,\n",
       " 'أضغط': 579,\n",
       " 'أضف': 580,\n",
       " 'أضفت': 581,\n",
       " 'أضمن': 582,\n",
       " 'أضن': 583,\n",
       " 'أضواء': 584,\n",
       " 'أضيع': 585,\n",
       " 'أضيف': 586,\n",
       " 'أطباء': 587,\n",
       " 'أطباق': 588,\n",
       " 'أطبخ': 589,\n",
       " 'أطرح': 590,\n",
       " 'أطع': 591,\n",
       " 'أطعم': 592,\n",
       " 'أطعمة': 593,\n",
       " 'أطعمت': 594,\n",
       " 'أطفأ': 595,\n",
       " 'أطفأت': 596,\n",
       " 'أطفئ': 597,\n",
       " 'أطفال': 598,\n",
       " 'أطفالا': 599,\n",
       " 'أطلب': 600,\n",
       " 'أطلق': 601,\n",
       " 'أطلقت': 602,\n",
       " 'أطمئن': 603,\n",
       " 'أطهو': 604,\n",
       " 'أطول': 605,\n",
       " 'أطيب': 606,\n",
       " 'أطيق': 607,\n",
       " 'أظل': 608,\n",
       " 'أظن': 609,\n",
       " 'أظنن': 610,\n",
       " 'أظنني': 611,\n",
       " 'أظهر': 612,\n",
       " 'أظهرت': 613,\n",
       " 'أظهروا': 614,\n",
       " 'أعاد': 615,\n",
       " 'أعار': 616,\n",
       " 'أعارض': 617,\n",
       " 'أعاود': 618,\n",
       " 'أعتبر': 619,\n",
       " 'أعتذر': 620,\n",
       " 'أعترف': 621,\n",
       " 'أعتقد': 622,\n",
       " 'أعتمد': 623,\n",
       " 'أعتن': 624,\n",
       " 'أعتني': 625,\n",
       " 'أعثر': 626,\n",
       " 'أعجبت': 627,\n",
       " 'أعجوبة': 628,\n",
       " 'أعد': 629,\n",
       " 'أعداء': 630,\n",
       " 'أعداد': 631,\n",
       " 'أعدت': 632,\n",
       " 'أعددت': 633,\n",
       " 'أعدو': 634,\n",
       " 'أعذر': 635,\n",
       " 'أعر': 636,\n",
       " 'أعرت': 637,\n",
       " 'أعرف': 638,\n",
       " 'أعرني': 639,\n",
       " 'أعز': 640,\n",
       " 'أعزب': 641,\n",
       " 'أعزف': 642,\n",
       " 'أعشاب': 643,\n",
       " 'أعشق': 644,\n",
       " 'أعصاب': 645,\n",
       " 'أعضاء': 646,\n",
       " 'أعط': 647,\n",
       " 'أعطانيه': 648,\n",
       " 'أعطت': 649,\n",
       " 'أعطس': 650,\n",
       " 'أعطني': 651,\n",
       " 'أعطه': 652,\n",
       " 'أعطوا': 653,\n",
       " 'أعطى': 654,\n",
       " 'أعطي': 655,\n",
       " 'أعطيت': 656,\n",
       " 'أعظم': 657,\n",
       " 'أعلام': 658,\n",
       " 'أعلق': 659,\n",
       " 'أعلم': 660,\n",
       " 'أعلميني': 661,\n",
       " 'أعلن': 662,\n",
       " 'أعلنت': 663,\n",
       " 'أعلى': 664,\n",
       " 'أعماق': 665,\n",
       " 'أعمال': 666,\n",
       " 'أعمامي': 667,\n",
       " 'أعمق': 668,\n",
       " 'أعمل': 669,\n",
       " 'أعمى': 670,\n",
       " 'أعندك': 671,\n",
       " 'أعني': 672,\n",
       " 'أعوام': 673,\n",
       " 'أعواما': 674,\n",
       " 'أعود': 675,\n",
       " 'أعور': 676,\n",
       " 'أعي': 677,\n",
       " 'أعياد': 678,\n",
       " 'أعيد': 679,\n",
       " 'أعير': 680,\n",
       " 'أعيش': 681,\n",
       " 'أعين': 682,\n",
       " 'أغادر': 683,\n",
       " 'أغبى': 684,\n",
       " 'أغراض': 685,\n",
       " 'أغرب': 686,\n",
       " 'أغرق': 687,\n",
       " 'أغسل': 688,\n",
       " 'أغضب': 689,\n",
       " 'أغضبت': 690,\n",
       " 'أغلب': 691,\n",
       " 'أغلبي': 692,\n",
       " 'أغلق': 693,\n",
       " 'أغلقت': 694,\n",
       " 'أغلى': 695,\n",
       " 'أغني': 696,\n",
       " 'أغنية': 697,\n",
       " 'أغيب': 698,\n",
       " 'أغير': 699,\n",
       " 'أفاجئ': 700,\n",
       " 'أفادت': 701,\n",
       " 'أفاعي': 702,\n",
       " 'أفتح': 703,\n",
       " 'أفترض': 704,\n",
       " 'أفتقد': 705,\n",
       " 'أفحص': 706,\n",
       " 'أفراد': 707,\n",
       " 'أفرش': 708,\n",
       " 'أفسح': 709,\n",
       " 'أفسد': 710,\n",
       " 'أفضل': 711,\n",
       " 'أفطرت': 712,\n",
       " 'أفعال': 713,\n",
       " 'أفعل': 714,\n",
       " 'أفكار': 715,\n",
       " 'أفكر': 716,\n",
       " 'أفكرت': 717,\n",
       " 'أفلام': 718,\n",
       " 'أفهم': 719,\n",
       " 'أفواه': 720,\n",
       " 'أفوز': 721,\n",
       " 'أفون': 722,\n",
       " 'أقابل': 723,\n",
       " 'أقارب': 724,\n",
       " 'أقاضي': 725,\n",
       " 'أقاطع': 726,\n",
       " 'أقام': 727,\n",
       " 'أقبل': 728,\n",
       " 'أقترب': 729,\n",
       " 'أقترح': 730,\n",
       " 'أقترض': 731,\n",
       " 'أقترف': 732,\n",
       " 'أقتل': 733,\n",
       " 'أقدام': 734,\n",
       " 'أقدر': 735,\n",
       " 'أقدم': 736,\n",
       " 'أقرأ': 737,\n",
       " 'أقرا': 738,\n",
       " 'أقراص': 739,\n",
       " 'أقرب': 740,\n",
       " 'أقربون': 741,\n",
       " 'أقرر': 742,\n",
       " 'أقرض': 743,\n",
       " 'أقرضت': 744,\n",
       " 'أقساط': 745,\n",
       " 'أقسم': 746,\n",
       " 'أقشر': 747,\n",
       " 'أقصد': 748,\n",
       " 'أقصر': 749,\n",
       " 'أقصوصة': 750,\n",
       " 'أقصى': 751,\n",
       " 'أقضي': 752,\n",
       " 'أقطع': 753,\n",
       " 'أقعد': 754,\n",
       " 'أقف': 755,\n",
       " 'أقفز': 756,\n",
       " 'أقفل': 757,\n",
       " 'أقفلت': 758,\n",
       " 'أقل': 759,\n",
       " 'أقلام': 760,\n",
       " 'أقلب': 761,\n",
       " 'أقلعت': 762,\n",
       " 'أقم': 763,\n",
       " 'أقمت': 764,\n",
       " 'أقنع': 765,\n",
       " 'أقنعت': 766,\n",
       " 'أقو': 767,\n",
       " 'أقوال': 768,\n",
       " 'أقود': 769,\n",
       " 'أقول': 770,\n",
       " 'أقوم': 771,\n",
       " 'أقوى': 772,\n",
       " 'أقوياء': 773,\n",
       " 'أقيس': 774,\n",
       " 'أكاد': 775,\n",
       " 'أكبر': 776,\n",
       " 'أكتاف': 777,\n",
       " 'أكتب': 778,\n",
       " 'أكتبت': 779,\n",
       " 'أكتوبر': 780,\n",
       " 'أكثر': 781,\n",
       " 'أكره': 782,\n",
       " 'أكسجين': 783,\n",
       " 'أكل': 784,\n",
       " 'أكلت': 785,\n",
       " 'أكلم': 786,\n",
       " 'أكلنا': 787,\n",
       " 'أكلوا': 788,\n",
       " 'أكمل': 789,\n",
       " 'أكملت': 790,\n",
       " 'أكن': 791,\n",
       " 'أكنت': 792,\n",
       " 'أكو': 793,\n",
       " 'أكون': 794,\n",
       " 'أكيد': 795,\n",
       " 'أكيدة': 796,\n",
       " 'أكيليز': 797,\n",
       " 'أل': 798,\n",
       " 'ألبرت': 799,\n",
       " 'ألبس': 800,\n",
       " 'ألتقط': 801,\n",
       " 'ألتمس': 802,\n",
       " 'ألحق': 803,\n",
       " 'ألحقت': 804,\n",
       " 'ألدي': 805,\n",
       " 'ألزمت': 806,\n",
       " 'ألعاب': 807,\n",
       " 'ألعب': 808,\n",
       " 'ألغ': 809,\n",
       " 'ألغوا': 810,\n",
       " 'ألغيت': 811,\n",
       " 'ألف': 812,\n",
       " 'ألفت': 813,\n",
       " 'ألفي': 814,\n",
       " 'ألفين': 815,\n",
       " 'ألق': 816,\n",
       " 'ألقت': 817,\n",
       " 'ألقوا': 818,\n",
       " 'ألقى': 819,\n",
       " 'ألقي': 820,\n",
       " 'أللقصه': 821,\n",
       " 'ألم': 822,\n",
       " 'ألما': 823,\n",
       " 'ألماس': 824,\n",
       " 'ألمانيا': 825,\n",
       " 'ألمانية': 826,\n",
       " 'ألنا': 827,\n",
       " 'ألوان': 828,\n",
       " 'ألوب': 829,\n",
       " 'ألوت': 830,\n",
       " 'ألوم': 831,\n",
       " 'ألياف': 832,\n",
       " 'أليافا': 833,\n",
       " 'أليست': 834,\n",
       " 'أليفا': 835,\n",
       " 'أليفة': 836,\n",
       " 'أم': 837,\n",
       " 'أما': 838,\n",
       " 'أمازالوا': 839,\n",
       " 'أماكن': 840,\n",
       " 'أمام': 841,\n",
       " 'أمامي': 842,\n",
       " 'أمامية': 843,\n",
       " 'أمان': 844,\n",
       " 'أمانع': 845,\n",
       " 'أماه': 846,\n",
       " 'أمتأكد': 847,\n",
       " 'أمتا': 848,\n",
       " 'أمتار': 849,\n",
       " 'أمتلك': 850,\n",
       " 'أمثال': 851,\n",
       " 'أمدرس': 852,\n",
       " 'أمر': 853,\n",
       " 'أمرا': 854,\n",
       " 'أمرت': 855,\n",
       " 'أمرض': 856,\n",
       " 'أمريكا': 857,\n",
       " 'أمريكي': 858,\n",
       " 'أمريكية': 859,\n",
       " 'أمريكيون': 860,\n",
       " 'أمريكيين': 861,\n",
       " 'أمزح': 862,\n",
       " 'أمس': 863,\n",
       " 'أمسكت': 864,\n",
       " 'أمسية': 865,\n",
       " 'أمش': 866,\n",
       " 'أمشط': 867,\n",
       " 'أمشى': 868,\n",
       " 'أمض': 869,\n",
       " 'أمضت': 870,\n",
       " 'أمضغ': 871,\n",
       " 'أمضوا': 872,\n",
       " 'أمضي': 873,\n",
       " 'أمضيت': 874,\n",
       " 'أمضينا': 875,\n",
       " 'أمطار': 876,\n",
       " 'أمطرت': 877,\n",
       " 'أمكن': 878,\n",
       " 'أمكننا': 879,\n",
       " 'أمكنني': 880,\n",
       " 'أمل': 881,\n",
       " 'أملئ': 882,\n",
       " 'أملا': 883,\n",
       " 'أملت': 884,\n",
       " 'أملك': 885,\n",
       " 'أملى': 886,\n",
       " 'أمهات': 887,\n",
       " 'أمواج': 888,\n",
       " 'أموال': 889,\n",
       " 'أموت': 890,\n",
       " 'أمور': 891,\n",
       " 'أميال': 892,\n",
       " 'أميرة': 893,\n",
       " 'أميرتي': 894,\n",
       " 'أميركي': 895,\n",
       " 'أمين': 896,\n",
       " 'أمينا': 897,\n",
       " 'أمينة': 898,\n",
       " 'أن': 899,\n",
       " 'أنا': 900,\n",
       " 'أنابيب': 901,\n",
       " 'أناس': 902,\n",
       " 'أناقش': 903,\n",
       " 'أنام': 904,\n",
       " 'أناناس': 905,\n",
       " 'أنانيين': 906,\n",
       " 'أنت': 907,\n",
       " 'أنتبه': 908,\n",
       " 'أنتحر': 909,\n",
       " 'أنتظر': 910,\n",
       " 'أنتقل': 911,\n",
       " 'أنتم': 912,\n",
       " 'أنته': 913,\n",
       " 'أنتهي': 914,\n",
       " 'أنجح': 915,\n",
       " 'أنجز': 916,\n",
       " 'أنجزت': 917,\n",
       " 'أنجلترا': 918,\n",
       " 'أنحاء': 919,\n",
       " 'أنحني': 920,\n",
       " 'أنذر': 921,\n",
       " 'أنذهب': 922,\n",
       " 'أنذهل': 923,\n",
       " 'أنزع': 924,\n",
       " 'أنزل': 925,\n",
       " 'أنس': 926,\n",
       " 'أنسجم': 927,\n",
       " 'أنسخ': 928,\n",
       " 'أنسى': 929,\n",
       " 'أنشئت': 930,\n",
       " 'أنصت': 931,\n",
       " 'أنصح': 932,\n",
       " 'أنضم': 933,\n",
       " 'أنظر': 934,\n",
       " 'أنظف': 935,\n",
       " 'أنف': 936,\n",
       " 'أنفاس': 937,\n",
       " 'أنفاق': 938,\n",
       " 'أنفس': 939,\n",
       " 'أنفق': 940,\n",
       " 'أنفي': 941,\n",
       " 'أنقذ': 942,\n",
       " 'أنقر': 943,\n",
       " 'أنقلب': 944,\n",
       " 'أنكر': 945,\n",
       " 'أنكسر': 946,\n",
       " 'أنم': 947,\n",
       " 'أنهار': 948,\n",
       " 'أنهت': 949,\n",
       " 'أنهض': 950,\n",
       " 'أنهى': 951,\n",
       " 'أنهي': 952,\n",
       " 'أنهيت': 953,\n",
       " 'أنهينا': 954,\n",
       " 'أنوار': 955,\n",
       " 'أنواع': 956,\n",
       " 'أنوف': 957,\n",
       " 'أنوي': 958,\n",
       " 'أنيقة': 959,\n",
       " 'أنی': 960,\n",
       " 'أهتم': 961,\n",
       " 'أهدأ': 962,\n",
       " 'أهداف': 963,\n",
       " 'أهدت': 964,\n",
       " 'أهذ': 965,\n",
       " 'أهذا': 966,\n",
       " 'أهرب': 967,\n",
       " 'أهزم': 968,\n",
       " 'أهل': 969,\n",
       " 'أهلا': 970,\n",
       " 'أهم': 971,\n",
       " 'أهمية': 972,\n",
       " 'أهنئ': 973,\n",
       " 'أهنا': 974,\n",
       " 'أهو': 975,\n",
       " 'أهي': 976,\n",
       " 'أو': 977,\n",
       " 'أواجه': 978,\n",
       " 'أوافق': 979,\n",
       " 'أوافي': 980,\n",
       " 'أوامر': 981,\n",
       " 'أوان': 982,\n",
       " 'أوبخ': 983,\n",
       " 'أوبرا': 984,\n",
       " 'أوتان': 985,\n",
       " 'أوتظنني': 986,\n",
       " 'أوتوماتيكي': 987,\n",
       " 'أوج': 988,\n",
       " 'أوجدت': 989,\n",
       " 'أود': 990,\n",
       " 'أوذيك': 991,\n",
       " 'أوراق': 992,\n",
       " 'أوروبا': 993,\n",
       " 'أوروبيين': 994,\n",
       " 'أوساكا': 995,\n",
       " 'أوشكت': 996,\n",
       " 'أوصل': 997,\n",
       " 'أوصلت': 998,\n",
       " 'أوقات': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_token_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will put the max_encoder_seq for both decoder and encoder with 64 since the max number of tokens in Arabic is 52 and in English is 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_encoder_seq_length = 64\n",
    "max_decoder_seq_length = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Arabic_input</th>\n",
       "      <th>English_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>مرحبا</td>\n",
       "      <td>START_ hi _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>اركض</td>\n",
       "      <td>START_ run _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>اخفض رأسك</td>\n",
       "      <td>START_ duck _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>اخفضي رأسك</td>\n",
       "      <td>START_ duck _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>اخفضوا رؤوسكم</td>\n",
       "      <td>START_ duck _END</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Arabic_input    English_target\n",
       "0          مرحبا    START_ hi _END\n",
       "1           اركض   START_ run _END\n",
       "2      اخفض رأسك  START_ duck _END\n",
       "3     اخفضي رأسك  START_ duck _END\n",
       "4  اخفضوا رؤوسكم  START_ duck _END"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_input_target(dataset, max_encoder_seq_length, max_decoder_seq_length, num_decoder_tokens):\n",
    "    # The input setence to the encoder is 64 leghth.\n",
    "    encoder_input_data = np.zeros( (len(dataset.Arabic_input), max_encoder_seq_length),dtype='float32')\n",
    "    # The input setence to the decoder is 64 leghth.\n",
    "    decoder_input_data = np.zeros((len(dataset.English_target), max_decoder_seq_length), dtype='float32')\n",
    "    # The output setence of the decoder is 64 x all_tokens_inside_encoder. Since the decoder will do softmax for each token.\n",
    "    decoder_target_data = np.zeros((len(dataset.English_target), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "    \n",
    "    return encoder_input_data, decoder_input_data, decoder_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data, decoder_input_data, decoder_target_data = init_input_target(dataset, max_encoder_seq_length, max_decoder_seq_length, num_decoder_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(tokenized_ds_copy, max_encoder_seq_length, max_decoder_seq_length, num_decoder_tokens):\n",
    "\n",
    "    for i, (input_text_ls, target_text_ls) in enumerate(zip(tokenized_ds_copy.Arabic_input, tokenized_ds_copy.English_target)):\n",
    "        # preparing the encoder inputs\n",
    "        for t, tok in enumerate(input_text_ls):\n",
    "            #To obtain the ids of encoder sentence's tokens from input_token_index\n",
    "            encoder_input_data[i, t] = input_token_index[tok]\n",
    "            \n",
    "        encoder_input_data[i, t+1:] = input_token_index[pad_tok]\n",
    "        \n",
    "        # This loop is used to prepare the input and output of the decoder\n",
    "        for t, tok in enumerate(target_text_ls):\n",
    "            #1- prepare the decoder input\n",
    "            #To obtain the ids of decoder sentence's tokens from target_token_index\n",
    "            decoder_input_data[i, t] = target_token_index[tok]    \n",
    "            \n",
    "            # To obtain the decoder output\n",
    "            if t > 0:\n",
    "                # decoder_target_data will be ahead by one timestep\n",
    "                # and will not include the start character.\n",
    "                #We put 1 in the place of the expected word\n",
    "                decoder_target_data[i, t - 1, target_token_index[tok]] = 1.\n",
    "        decoder_input_data[i, t+1:] = target_token_index[pad_tok] \n",
    "        decoder_target_data[i, t:, target_token_index[pad_tok]] = 1.          \n",
    "              \n",
    "    return encoder_input_data, decoder_input_data, decoder_target_data              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data, decoder_input_data, decoder_target_data = vectorize(tokenized_ds_copy, max_encoder_seq_length, max_decoder_seq_length, num_decoder_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5490.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.], dtype=float32)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5490"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_token_index['مرحبا']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \n",
    "\n",
    "The 1st sentence in the Arabic_input column is 'مرحبا', and there is no any other word inside the sentence. If we printed the 1st place in encoder_input_data, we will find that there is only one word, this word took the same number of 'مرحبا "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   5., 1802.,    6.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.], dtype=float32)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1802"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_token_index['hi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_token_index['START_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_token_index['_END']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "    The 1st sentence in the decoder input is START_ hi _END, so there are only three tokens, each token has id. If we check the tokens ids inside the target_token_index, then we find that the decoder_input_data has the similar number in the location 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The _END location\n",
    "decoder_target_data[0][1][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The hi location\n",
    "decoder_target_data[0][0][1800:1810]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "what should appear in the output of the decoder in this case is 'hi _END', we will find hi on gate 0 in the 3rd location, and _END on gate  1 n the 6th location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mdeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will implement the encoder separated from decoder to simplify the operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_training_encoder(num_encoder_tokens, emb_sz, lstm_sz, mask_zero):\n",
    "    # 1- Define the input to the encoder\n",
    "    encoder_inputs = Input(shape=(None,))\n",
    "    \n",
    "    # 2- Define the embedding layer. This layer is built on the previous layer the input to the encoder.\n",
    "    # This embedding layer need the following parameters 1- all the expected encoder tokens number. 2- embedding size of each word\n",
    "    # The vector of each word output from embedding layer has this word charcteristics.\n",
    "    en_x=  Embedding(num_encoder_tokens, emb_sz,mask_zero=mask_zero)(encoder_inputs)\n",
    "    \n",
    "    # 3- Define the 1st LSTM layer, \n",
    "    #This layer parametrs are:\n",
    "    # 1- reurn_satate which enable us to output the cell state and the hidden state\n",
    "    # 2- lstm_sz: which is the number of hidden units inside the LSTM layer.\n",
    "    encoder = LSTM(lstm_sz, return_state=True)\n",
    "    \n",
    "    # 4- Put the encoder LSTM on the embedding layer output \n",
    "    #and take the output of this LSTM which will be used to build the context vector\n",
    "    encoder_outputs, state_h, state_c = encoder(en_x)\n",
    "    \n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]\n",
    "  \n",
    "    # Encoder model\n",
    "    encoder_model = Model(encoder_inputs, encoder_states)\n",
    "    print('\\n The encoder model \\n')\n",
    "    encoder_model.summary()\n",
    "  \n",
    "    return encoder_model, encoder_states, encoder_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "    \n",
    "def build_training_decoder(num_decoder_tokens, emb_sz, lstm_sz, encoder_states, encoder_inputs, mask_zero):\n",
    "        \n",
    "    #1- define the input layer to the decoder.\n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "    # 2- define the embeddding layer for the decoder/ In training it will take the expected tokens from the dataset\n",
    "    # The embedding layer parameters are:\n",
    "    # 1- All the expected tokens to the decoder.\n",
    "    # 2- the embedding size\n",
    "    decoder_embedding=  Embedding(num_decoder_tokens, emb_sz,mask_zero=True)\n",
    "\n",
    "    # 3- put the layer of embedding on the layer of decoder inputs. \n",
    "    embedding_output= decoder_embedding(decoder_inputs)\n",
    "\n",
    "    \n",
    "    #4- Define the LSTM which should output the hidden state and cell state and cell output for each cell.\n",
    "    # the hidden state and cell state which can be done enabling return_state\n",
    "    #  The output sequence of all the input sequence tokens can be done by enabling the return_sequences \n",
    "    decoder_lstm = LSTM(lstm_sz, return_sequences=True, return_state=True)\n",
    "\n",
    "    \n",
    "    # 5- Put the LSTM on the top of embedding layer and feeed the context vector to the LSTM 'encoder_states'\n",
    "    decoder_outputs, _, _ = decoder_lstm(embedding_output, initial_state=encoder_states)\n",
    "\n",
    "    #6- Define the fully connected layer which predicts the output token.\n",
    "    # This layer will output a vector. This vector has a probability for each expected token to output. \n",
    "    #Then this layer feeds this to an activation function 'softmax' to decide which word sould output at each timestep.\n",
    "    decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "\n",
    "    # 6-Put the dense/fully connected layer on the top of lstm \n",
    "    #and feed the lstm output vector for all input tokens to the dense.\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    # 7-Here build the combined model which takes the training tokens to the encoder and decoder and output the decoder output. \n",
    "    # This decoder output comes after the softmax of the dense layer\n",
    "    combined_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    \n",
    "    print('\\n The combined model \\n')\n",
    "    combined_model.summary()\n",
    "    \n",
    "    return combined_model, decoder_inputs,embedding_output, decoder_lstm, decoder_dense\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will use the same architecture of decoder in the training pahse, so we will use the followiing\n",
    "# 1- decoder_input.\n",
    "# 2- decoder_embedding_output. \n",
    "# 3- decoder_lstm\n",
    "#4- decoder_dense\n",
    "#why will we use the same architecture? since we will use the same cells but by using the inference methodology which is mentioned above.\n",
    "def build_inference_decoder(num_decoder_tokens, lstm_sz, emb_sz, embedding_output, decoder_inputs, decoder_lstm, decoder_dense):\n",
    "    \n",
    "    # Decoder model: Re-build based on explicit state inputs. Needed for step-by-step inference:\n",
    "    \n",
    "    # define the hidden state of the context vector which will come from the encoder in the prediction\n",
    "    decoder_state_input_h = Input(shape=(lstm_sz,))\n",
    "    # define the cell state of the context vector which will come from the encoder in the prediction\n",
    "    decoder_state_input_c = Input(shape=(lstm_sz,))\n",
    "    \n",
    "    #define the conext vector which will feed into the decoder to initialize it. The values of this will be feed in the prediction.\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    # feed the decoder LSTM with embedding output, and inilize its state to decoder_states_inputs \n",
    "    #which will be fed after that with the encoder context vector.\n",
    "    decoder_outputs2, state_h2, state_c2 = decoder_lstm(embedding_output, initial_state=decoder_states_inputs)\n",
    "    \n",
    "    # Define the input to the dense layer\n",
    "    decoder_states2 = [state_h2, state_c2]\n",
    "    \n",
    "    # Feed the hidden and cell state to the dense layer which will predict the output.\n",
    "    decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "    #define the decoder_model which will take take the decoder inputs and initial state for the decoder.\n",
    "    # this model will output the decoder output token in addition to the hidden and cell states.\n",
    "    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs2] + decoder_states2) \n",
    "    \n",
    "    return decoder_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "emb_sz = 50\n",
    "lstm_sz = 64\n",
    " \n",
    "def model_seq_to_seq(batch_size, epochs, mask_zero):\n",
    "    \n",
    "    encoder_model, encoder_states, encoder_inputs = build_training_encoder(num_encoder_tokens, emb_sz, lstm_sz, mask_zero)\n",
    "    combined_model, decoder_inputs,embedding_output, decoder_lstm, decoder_dense = build_training_decoder(num_decoder_tokens,\n",
    "                                                                                                          emb_sz, lstm_sz, \n",
    "                                                                                                          encoder_states, \n",
    "                                                                                                          encoder_inputs,mask_zero)\n",
    "    \n",
    "    decoder_model = build_inference_decoder(num_decoder_tokens, lstm_sz, emb_sz, embedding_output, decoder_inputs,\n",
    "                                            decoder_lstm, decoder_dense)\n",
    "\n",
    "    \n",
    "    # 8- compile the combined model in training phase\n",
    "    combined_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "    combined_model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=batch_size,\n",
    "                       epochs=epochs, validation_split=0.05)\n",
    "    \n",
    "    return combined_model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The encoder model \n",
      "\n",
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_12 (Embedding)     (None, None, 50)          360450    \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               [(None, 64), (None, 64),  29440     \n",
      "=================================================================\n",
      "Total params: 389,890\n",
      "Trainable params: 389,890\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      " The combined model \n",
      "\n",
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        (None, None, 50)     360450      input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)        (None, None, 50)     215100      input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_12 (LSTM)                  [(None, 64), (None,  29440       embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_13 (LSTM)                  [(None, None, 64), ( 29440       embedding_13[0][0]               \n",
      "                                                                 lstm_12[0][1]                    \n",
      "                                                                 lstm_12[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, None, 4302)   279630      lstm_13[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 914,060\n",
      "Trainable params: 914,060\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/30\n",
      "181/181 [==============================] - 52s 253ms/step - loss: 3.3147 - acc: 0.8719 - val_loss: 1.2910 - val_acc: 0.8069\n",
      "Epoch 2/30\n",
      "181/181 [==============================] - 44s 244ms/step - loss: 0.5464 - acc: 0.9167 - val_loss: 1.2179 - val_acc: 0.8225\n",
      "Epoch 3/30\n",
      "181/181 [==============================] - 46s 256ms/step - loss: 0.5171 - acc: 0.9196 - val_loss: 1.2314 - val_acc: 0.8229\n",
      "Epoch 4/30\n",
      "181/181 [==============================] - 49s 274ms/step - loss: 0.5029 - acc: 0.9208 - val_loss: 1.2085 - val_acc: 0.8242\n",
      "Epoch 5/30\n",
      "181/181 [==============================] - 47s 259ms/step - loss: 0.4881 - acc: 0.9219 - val_loss: 1.1860 - val_acc: 0.8271\n",
      "Epoch 6/30\n",
      "181/181 [==============================] - 47s 259ms/step - loss: 0.4737 - acc: 0.9235 - val_loss: 1.1839 - val_acc: 0.8276\n",
      "Epoch 7/30\n",
      "181/181 [==============================] - 45s 247ms/step - loss: 0.4653 - acc: 0.9239 - val_loss: 1.1713 - val_acc: 0.8300\n",
      "Epoch 8/30\n",
      "181/181 [==============================] - 49s 271ms/step - loss: 0.4523 - acc: 0.9253 - val_loss: 1.1676 - val_acc: 0.8312\n",
      "Epoch 9/30\n",
      "181/181 [==============================] - 48s 266ms/step - loss: 0.4422 - acc: 0.9261 - val_loss: 1.1408 - val_acc: 0.8331\n",
      "Epoch 10/30\n",
      "181/181 [==============================] - 48s 266ms/step - loss: 0.4339 - acc: 0.9267 - val_loss: 1.1228 - val_acc: 0.8344\n",
      "Epoch 11/30\n",
      "181/181 [==============================] - 49s 272ms/step - loss: 0.4240 - acc: 0.9274 - val_loss: 1.1349 - val_acc: 0.8338\n",
      "Epoch 12/30\n",
      "181/181 [==============================] - 48s 265ms/step - loss: 0.4163 - acc: 0.9283 - val_loss: 1.1282 - val_acc: 0.8338\n",
      "Epoch 13/30\n",
      "181/181 [==============================] - 46s 255ms/step - loss: 0.4124 - acc: 0.9283 - val_loss: 1.1267 - val_acc: 0.8346\n",
      "Epoch 14/30\n",
      "181/181 [==============================] - 44s 245ms/step - loss: 0.4062 - acc: 0.9289 - val_loss: 1.1249 - val_acc: 0.8354\n",
      "Epoch 15/30\n",
      "181/181 [==============================] - 45s 247ms/step - loss: 0.4032 - acc: 0.9291 - val_loss: 1.1257 - val_acc: 0.8353\n",
      "Epoch 16/30\n",
      "181/181 [==============================] - 46s 254ms/step - loss: 0.3976 - acc: 0.9295 - val_loss: 1.1120 - val_acc: 0.8362\n",
      "Epoch 17/30\n",
      "181/181 [==============================] - 45s 250ms/step - loss: 0.3927 - acc: 0.9300 - val_loss: 1.1284 - val_acc: 0.8359\n",
      "Epoch 18/30\n",
      "181/181 [==============================] - 44s 244ms/step - loss: 0.3868 - acc: 0.9308 - val_loss: 1.1112 - val_acc: 0.8366\n",
      "Epoch 19/30\n",
      "181/181 [==============================] - 45s 251ms/step - loss: 0.3836 - acc: 0.9313 - val_loss: 1.1226 - val_acc: 0.8367\n",
      "Epoch 20/30\n",
      "181/181 [==============================] - 45s 247ms/step - loss: 0.3743 - acc: 0.9323 - val_loss: 1.0905 - val_acc: 0.8376\n",
      "Epoch 21/30\n",
      "181/181 [==============================] - 45s 246ms/step - loss: 0.3715 - acc: 0.9329 - val_loss: 1.1046 - val_acc: 0.8390\n",
      "Epoch 22/30\n",
      "181/181 [==============================] - 45s 251ms/step - loss: 0.3644 - acc: 0.9339 - val_loss: 1.1174 - val_acc: 0.8386\n",
      "Epoch 23/30\n",
      "181/181 [==============================] - 46s 253ms/step - loss: 0.3631 - acc: 0.9339 - val_loss: 1.1193 - val_acc: 0.8396\n",
      "Epoch 24/30\n",
      "181/181 [==============================] - 47s 260ms/step - loss: 0.3564 - acc: 0.9347 - val_loss: 1.1090 - val_acc: 0.8401\n",
      "Epoch 25/30\n",
      "181/181 [==============================] - 46s 252ms/step - loss: 0.3536 - acc: 0.9355 - val_loss: 1.1253 - val_acc: 0.8397\n",
      "Epoch 26/30\n",
      "181/181 [==============================] - 46s 255ms/step - loss: 0.3468 - acc: 0.9364 - val_loss: 1.1039 - val_acc: 0.8403\n",
      "Epoch 27/30\n",
      "181/181 [==============================] - 48s 263ms/step - loss: 0.3427 - acc: 0.9372 - val_loss: 1.1034 - val_acc: 0.8409\n",
      "Epoch 28/30\n",
      "181/181 [==============================] - 47s 259ms/step - loss: 0.3390 - acc: 0.9376 - val_loss: 1.1111 - val_acc: 0.8407\n",
      "Epoch 29/30\n",
      "181/181 [==============================] - 46s 257ms/step - loss: 0.3352 - acc: 0.9383 - val_loss: 1.1176 - val_acc: 0.8418\n",
      "Epoch 30/30\n",
      "181/181 [==============================] - 47s 258ms/step - loss: 0.3292 - acc: 0.9393 - val_loss: 1.1053 - val_acc: 0.8414\n"
     ]
    }
   ],
   "source": [
    "combined_model, encoder_model, decoder_model = model_seq_to_seq(batch_size=64, epochs=30, mask_zero=False)\n",
    "combined_model.save_weights(\"translatemodel.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, sep = ' '):\n",
    "    # to obtain the encoder model\n",
    "    # 1- Encode the input to obtain the context vector from the encoder.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # Generate empty target sequence of length 1 like this [[0]]\n",
    "    target_seq = np.zeros((1,1))\n",
    "    \n",
    "    # Populate the first character of target sequence with the start character to be like [[5]]\n",
    "    target_seq[0, 0] = target_token_index[st_tok]\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    # if you don't find end token\n",
    "    while not stop_condition:\n",
    "        #feed the predict() with the input to the decoder model which is target_seq and\n",
    "        #the context vector from the encode which is states_value\n",
    "        # output_tokens will hold the last output of the LSTM\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        \n",
    "\n",
    "        # Sample a token \n",
    "        # It returns the index of the maximum item in the 1st array last row\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        # return the word as letters from its index\n",
    "        sampled_tok = reverse_target_tok_index[sampled_token_index]\n",
    "        # form the sentence which consists of words and separatoe\n",
    "        decoded_sentence += sep + sampled_tok\n",
    "\n",
    "        # Exit condition: either hit max length which is 64\n",
    "        # or find stop character.\n",
    "        if (sampled_tok == end_tok or len(decoded_sentence) > 64):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1) with the index of the output token.\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states to be feed to the prediction\n",
    "        states_value = [h, c]\n",
    " \n",
    "    return decoded_sentence\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: 0    مرحبا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of him _END\n",
      "-\n",
      "Input sentence: 1    اركض\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot _END\n",
      "-\n",
      "Input sentence: 2    اخفض رأسك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of the table _END\n",
      "-\n",
      "Input sentence: 3    اخفضي رأسك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  this is a beautiful country _END\n",
      "-\n",
      "Input sentence: 4    اخفضوا رؤوسكم\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  this is a good car _END\n",
      "-\n",
      "Input sentence: 5    النجده\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot _END\n",
      "-\n",
      "Input sentence: 6    اقفز\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of the table _END\n",
      "-\n",
      "Input sentence: 7    قف\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of him _END\n",
      "-\n",
      "Input sentence: 8    توقف \n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot _END\n",
      "-\n",
      "Input sentence: 9    إنتظر\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of the table _END\n",
      "-\n",
      "Input sentence: 10    داوم\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  this house is beautiful _END\n",
      "-\n",
      "Input sentence: 11    استمر\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  this house is beautiful _END\n",
      "-\n",
      "Input sentence: 12    مرحبا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of him _END\n",
      "-\n",
      "Input sentence: 13    تعجل\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of the table _END\n",
      "-\n",
      "Input sentence: 14    استعجل\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of the table _END\n",
      "-\n",
      "Input sentence: 15    انا اري\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im not a doctor _END\n",
      "-\n",
      "Input sentence: 16    أنا فزت\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i am a lot _END\n",
      "-\n",
      "Input sentence: 17    استرح\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of the table _END\n",
      "-\n",
      "Input sentence: 18    ابتسم\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  this is a car _END\n",
      "-\n",
      "Input sentence: 19    في صحتك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  what do you have _END\n",
      "-\n",
      "Input sentence: 20    هل فهمت\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  do you have a doctor _END\n",
      "-\n",
      "Input sentence: 21    ركض\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot _END\n",
      "-\n",
      "Input sentence: 22    أعرف\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i know _END\n",
      "-\n",
      "Input sentence: 23    أعلم ذلك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i know _END\n",
      "-\n",
      "Input sentence: 24    أنا أعلم\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i know _END\n",
      "-\n",
      "Input sentence: 25    أنا في \n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im a good student _END\n",
      "-\n",
      "Input sentence: 26    أنا بخير\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im a good student _END\n",
      "-\n",
      "Input sentence: 27    استمع\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of the table _END\n",
      "-\n",
      "Input sentence: 28    غير معقول\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  what are you _END\n",
      "-\n",
      "Input sentence: 29    حقا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im not a good _END\n",
      "-\n",
      "Input sentence: 30    شكرا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of him _END\n",
      "-\n",
      "Input sentence: 31    شكرا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of him _END\n",
      "-\n",
      "Input sentence: 32    لماذا أنا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i dont have _END\n",
      "-\n",
      "Input sentence: 33    رائع\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a good _END\n",
      "-\n",
      "Input sentence: 34    خذ راحتك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  my mother is a lot of tom _END\n",
      "-\n",
      "Input sentence: 35    أغرب عن وجهي\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is very fast _END\n",
      "-\n",
      "Input sentence: 36    هاتفني\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of the table _END\n",
      "-\n",
      "Input sentence: 37    اتصل بي\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  lets get out _END\n",
      "-\n",
      "Input sentence: 38    تفضل بالدخول\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  please come _END\n",
      "-\n",
      "Input sentence: 39    تعال إلى الداخل\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  my father is a lot of tom _END\n",
      "-\n",
      "Input sentence: 40    بالله عليك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is not a lot _END\n",
      "-\n",
      "Input sentence: 41    هيا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of him _END\n",
      "-\n",
      "Input sentence: 42    هيا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of him _END\n",
      "-\n",
      "Input sentence: 43    اخرج من هنا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is not a lot of the table _END\n",
      "-\n",
      "Input sentence: 44    أخرج\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of the table _END\n",
      "-\n",
      "Input sentence: 45    اخرج\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of the table _END\n",
      "-\n",
      "Input sentence: 46    اتركني و شأني\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  lets go _END\n",
      "-\n",
      "Input sentence: 47    اذهب بعيدا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of the table _END\n",
      "-\n",
      "Input sentence: 48    ارحل\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  my brother is a good country _END\n",
      "-\n",
      "Input sentence: 49    مع السلامه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of the table _END\n",
      "-\n",
      "Input sentence: 50    إلى اللقاء\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i have a lot of water _END\n",
      "-\n",
      "Input sentence: 51    إنتظر\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of the table _END\n",
      "-\n",
      "Input sentence: 52    لقد أتى\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i am a lot _END\n",
      "-\n",
      "Input sentence: 53    هو يجري\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of him _END\n",
      "-\n",
      "Input sentence: 54    ساعدني\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of the table _END\n",
      "-\n",
      "Input sentence: 55    النجده ساعدني\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  my father is a lot of tom _END\n",
      "-\n",
      "Input sentence: 56    ساعدوني\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  my father is a lot of tom _END\n",
      "-\n",
      "Input sentence: 57    انتظر\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of the table _END\n",
      "-\n",
      "Input sentence: 58    أنا موافق\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i am a good car _END\n",
      "-\n",
      "Input sentence: 59    أنا حزين\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im a good student _END\n",
      "-\n",
      "Input sentence: 60    أنا أيضا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i am a good idea _END\n",
      "-\n",
      "Input sentence: 61    اخرس\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of the table _END\n",
      "-\n",
      "Input sentence: 62    اصمت\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  this house is very _END\n",
      "-\n",
      "Input sentence: 63    اسكت\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of the table _END\n",
      "-\n",
      "Input sentence: 64    أغلق فمك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of the table _END\n",
      "-\n",
      "Input sentence: 65    أوقفه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of him _END\n",
      "-\n",
      "Input sentence: 66    خذه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of him _END\n",
      "-\n",
      "Input sentence: 67    أخبرني\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  dont be a doctor _END\n",
      "-\n",
      "Input sentence: 68    توم فاز\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  tom is a good student _END\n",
      "-\n",
      "Input sentence: 69    لقد ربح توم\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  tom is a lot of tom _END\n",
      "-\n",
      "Input sentence: 70    استيقظ\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of the table _END\n",
      "-\n",
      "Input sentence: 71    أهلا و سهلا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  this is a good country _END\n",
      "-\n",
      "Input sentence: 72    مرحبا بك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  where are the book _END\n",
      "-\n",
      "Input sentence: 73    اهلا وسهلا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  this is a good country _END\n",
      "-\n",
      "Input sentence: 74    مرحبا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of him _END\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: 75    من فاز\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of him _END\n",
      "-\n",
      "Input sentence: 76    من الذي ربح\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he is a lot of him _END\n",
      "-\n",
      "Input sentence: 77    لم لا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  why do you want _END\n",
      "-\n",
      "Input sentence: 78    لما لا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i dont have _END\n",
      "-\n",
      "Input sentence: 79    لا فكره لدي\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i dont have a doctor _END\n",
      "-\n",
      "Input sentence: 80    استمتع بوقتك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i am going to the hospital _END\n",
      "-\n",
      "Input sentence: 81    أسرعا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  this house is very _END\n",
      "-\n",
      "Input sentence: 82    لقد نسيت\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i am not going _END\n",
      "-\n",
      "Input sentence: 83    فهمته\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i am very hungry _END\n",
      "-\n",
      "Input sentence: 84    فهمتها\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i am a lot _END\n",
      "-\n",
      "Input sentence: 85    فهمت ذلك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im not a doctor _END\n",
      "-\n",
      "Input sentence: 86    أستخدمه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i was a lot of the table _END\n",
      "-\n",
      "Input sentence: 87    سأدفع أنا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i am not going _END\n",
      "-\n",
      "Input sentence: 88    أنا مشغول\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im not a doctor _END\n",
      "-\n",
      "Input sentence: 89    إنني مشغول\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im not a doctor _END\n",
      "-\n",
      "Input sentence: 90    أشعر بالبرد\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i am a good idea _END\n",
      "-\n",
      "Input sentence: 91    أنا حر\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im a good student _END\n",
      "-\n",
      "Input sentence: 92    أنا هنا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im not a good _END\n",
      "-\n",
      "Input sentence: 93    لقد عدت إلى البيت\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i have a lot of water _END\n",
      "-\n",
      "Input sentence: 94    أنا فقير\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im a good student _END\n",
      "-\n",
      "Input sentence: 95    أنا ثري\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im not a doctor _END\n",
      "-\n",
      "Input sentence: 96    هذا مؤلم\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i am a lot _END\n",
      "-\n",
      "Input sentence: 97    انها جافه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i have a lot of water _END\n",
      "-\n",
      "Input sentence: 98    الجو حار\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i have a lot _END\n",
      "-\n",
      "Input sentence: 99    إنه جديد\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  its a good _END\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(100): #[14077,20122,40035,40064, 40056, 40068, 40090, 40095, 40100, 40119, 40131, 40136, 40150, 40153]:\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', dataset.Arabic_input[seq_index: seq_index + 1])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### changing the epochs and notice its effect on the decoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_sz = 50\n",
    "lstm_sz = 256\n",
    " \n",
    "def model_seq_to_seq(batch_size, epochs,mask_zero):\n",
    "    \n",
    "    encoder_model, encoder_states, encoder_inputs = build_training_encoder(num_encoder_tokens, emb_sz, lstm_sz,mask_zero)\n",
    "    combined_model, decoder_inputs,embedding_output, decoder_lstm, decoder_dense = build_training_decoder(num_decoder_tokens,\n",
    "                                                                                                          emb_sz, lstm_sz, \n",
    "                                                                                                          encoder_states, \n",
    "                                                                                                          encoder_inputs, \n",
    "                                                                                                          mask_zero)\n",
    "    \n",
    "    decoder_model = build_inference_decoder(num_decoder_tokens, lstm_sz, emb_sz, embedding_output, decoder_inputs,\n",
    "                                            decoder_lstm, decoder_dense)\n",
    "\n",
    "    \n",
    "    # 8- compile the combined model in training phase\n",
    "    combined_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "    combined_model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "    \n",
    "    return combined_model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The encoder model \n",
      "\n",
      "Model: \"model_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_33 (InputLayer)        [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_22 (Embedding)     (None, None, 50)          360450    \n",
      "_________________________________________________________________\n",
      "lstm_22 (LSTM)               [(None, 256), (None, 256) 314368    \n",
      "=================================================================\n",
      "Total params: 674,818\n",
      "Trainable params: 674,818\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      " The combined model \n",
      "\n",
      "Model: \"model_28\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_33 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_34 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_22 (Embedding)        (None, None, 50)     360450      input_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_23 (Embedding)        (None, None, 50)     215100      input_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_22 (LSTM)                  [(None, 256), (None, 314368      embedding_22[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_23 (LSTM)                  [(None, None, 256),  314368      embedding_23[0][0]               \n",
      "                                                                 lstm_22[0][1]                    \n",
      "                                                                 lstm_22[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, None, 4302)   1105614     lstm_23[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,309,900\n",
      "Trainable params: 2,309,900\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "152/152 [==============================] - 190s 1s/step - loss: 0.5909 - acc: 0.2060 - val_loss: 0.9355 - val_acc: 0.2140\n",
      "Epoch 2/100\n",
      "152/152 [==============================] - 195s 1s/step - loss: 0.4565 - acc: 0.3362 - val_loss: 0.9233 - val_acc: 0.2261\n",
      "Epoch 3/100\n",
      "152/152 [==============================] - 201s 1s/step - loss: 0.4295 - acc: 0.3524 - val_loss: 0.9043 - val_acc: 0.2397\n",
      "Epoch 4/100\n",
      "152/152 [==============================] - 205s 1s/step - loss: 0.4026 - acc: 0.3683 - val_loss: 0.8800 - val_acc: 0.2493\n",
      "Epoch 5/100\n",
      "152/152 [==============================] - 204s 1s/step - loss: 0.3852 - acc: 0.3812 - val_loss: 0.8801 - val_acc: 0.2554\n",
      "Epoch 6/100\n",
      "152/152 [==============================] - 205s 1s/step - loss: 0.3704 - acc: 0.3952 - val_loss: 0.8724 - val_acc: 0.2622\n",
      "Epoch 7/100\n",
      "152/152 [==============================] - 203s 1s/step - loss: 0.3538 - acc: 0.4164 - val_loss: 0.8699 - val_acc: 0.2688\n",
      "Epoch 8/100\n",
      "152/152 [==============================] - 203s 1s/step - loss: 0.3367 - acc: 0.4382 - val_loss: 0.8564 - val_acc: 0.2785\n",
      "Epoch 9/100\n",
      "152/152 [==============================] - 205s 1s/step - loss: 0.3203 - acc: 0.4565 - val_loss: 0.8479 - val_acc: 0.2862\n",
      "Epoch 10/100\n",
      "152/152 [==============================] - 204s 1s/step - loss: 0.3069 - acc: 0.4739 - val_loss: 0.8488 - val_acc: 0.2926\n",
      "Epoch 11/100\n",
      "152/152 [==============================] - 205s 1s/step - loss: 0.2952 - acc: 0.4885 - val_loss: 0.8494 - val_acc: 0.2965\n",
      "Epoch 12/100\n",
      "152/152 [==============================] - 204s 1s/step - loss: 0.2834 - acc: 0.5030 - val_loss: 0.8520 - val_acc: 0.2982\n",
      "Epoch 13/100\n",
      "152/152 [==============================] - 203s 1s/step - loss: 0.2713 - acc: 0.5190 - val_loss: 0.8665 - val_acc: 0.3006\n",
      "Epoch 14/100\n",
      "152/152 [==============================] - 201s 1s/step - loss: 0.2601 - acc: 0.5343 - val_loss: 0.8539 - val_acc: 0.3017\n",
      "Epoch 15/100\n",
      "152/152 [==============================] - 204s 1s/step - loss: 0.2513 - acc: 0.5492 - val_loss: 0.8605 - val_acc: 0.3053\n",
      "Epoch 16/100\n",
      "152/152 [==============================] - 209s 1s/step - loss: 0.2416 - acc: 0.5603 - val_loss: 0.8510 - val_acc: 0.3088\n",
      "Epoch 17/100\n",
      "152/152 [==============================] - 218s 1s/step - loss: 0.2320 - acc: 0.5761 - val_loss: 0.8556 - val_acc: 0.3096\n",
      "Epoch 18/100\n",
      "152/152 [==============================] - 148s 973ms/step - loss: 0.2217 - acc: 0.5917 - val_loss: 0.8545 - val_acc: 0.3097\n",
      "Epoch 19/100\n",
      "152/152 [==============================] - 133s 874ms/step - loss: 0.2148 - acc: 0.6042 - val_loss: 0.8694 - val_acc: 0.3107\n",
      "Epoch 20/100\n",
      "152/152 [==============================] - 128s 842ms/step - loss: 0.2070 - acc: 0.6134 - val_loss: 0.8875 - val_acc: 0.3091\n",
      "Epoch 21/100\n",
      "152/152 [==============================] - 129s 847ms/step - loss: 0.1963 - acc: 0.6296 - val_loss: 0.8766 - val_acc: 0.3094\n",
      "Epoch 22/100\n",
      "152/152 [==============================] - 136s 894ms/step - loss: 0.1912 - acc: 0.6414 - val_loss: 0.9090 - val_acc: 0.3098\n",
      "Epoch 23/100\n",
      "152/152 [==============================] - 143s 941ms/step - loss: 0.1836 - acc: 0.6542 - val_loss: 0.9000 - val_acc: 0.3095\n",
      "Epoch 24/100\n",
      "152/152 [==============================] - 143s 938ms/step - loss: 0.1745 - acc: 0.6718 - val_loss: 0.9094 - val_acc: 0.3110\n",
      "Epoch 25/100\n",
      "152/152 [==============================] - 139s 915ms/step - loss: 0.1684 - acc: 0.6840 - val_loss: 0.8961 - val_acc: 0.3119\n",
      "Epoch 26/100\n",
      "152/152 [==============================] - 137s 904ms/step - loss: 0.1612 - acc: 0.6944 - val_loss: 0.8949 - val_acc: 0.3148\n",
      "Epoch 27/100\n",
      "152/152 [==============================] - 138s 910ms/step - loss: 0.1543 - acc: 0.7107 - val_loss: 0.9095 - val_acc: 0.3122\n",
      "Epoch 28/100\n",
      "152/152 [==============================] - 135s 885ms/step - loss: 0.1494 - acc: 0.7219 - val_loss: 0.9316 - val_acc: 0.3109\n",
      "Epoch 29/100\n",
      "152/152 [==============================] - 136s 894ms/step - loss: 0.1437 - acc: 0.7321 - val_loss: 0.9421 - val_acc: 0.3110\n",
      "Epoch 30/100\n",
      "152/152 [==============================] - 134s 880ms/step - loss: 0.1376 - acc: 0.7430 - val_loss: 0.9374 - val_acc: 0.3104\n",
      "Epoch 31/100\n",
      "152/152 [==============================] - 132s 870ms/step - loss: 0.1314 - acc: 0.7559 - val_loss: 0.9509 - val_acc: 0.3131\n",
      "Epoch 32/100\n",
      "152/152 [==============================] - 130s 857ms/step - loss: 0.1257 - acc: 0.7691 - val_loss: 0.9623 - val_acc: 0.3107\n",
      "Epoch 33/100\n",
      "152/152 [==============================] - 134s 884ms/step - loss: 0.1208 - acc: 0.7795 - val_loss: 0.9649 - val_acc: 0.3134\n",
      "Epoch 34/100\n",
      "152/152 [==============================] - 131s 863ms/step - loss: 0.1165 - acc: 0.7871 - val_loss: 0.9592 - val_acc: 0.3122\n",
      "Epoch 35/100\n",
      "152/152 [==============================] - 174s 1s/step - loss: 0.1107 - acc: 0.8013 - val_loss: 0.9679 - val_acc: 0.3141\n",
      "Epoch 36/100\n",
      "152/152 [==============================] - 137s 897ms/step - loss: 0.1061 - acc: 0.8094 - val_loss: 0.9726 - val_acc: 0.3123\n",
      "Epoch 37/100\n",
      "152/152 [==============================] - 122s 801ms/step - loss: 0.1004 - acc: 0.8237 - val_loss: 0.9851 - val_acc: 0.3112\n",
      "Epoch 38/100\n",
      "152/152 [==============================] - 125s 825ms/step - loss: 0.0958 - acc: 0.8315 - val_loss: 0.9957 - val_acc: 0.3090\n",
      "Epoch 39/100\n",
      "152/152 [==============================] - 123s 809ms/step - loss: 0.0921 - acc: 0.8391 - val_loss: 0.9912 - val_acc: 0.3101\n",
      "Epoch 40/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 124s 818ms/step - loss: 0.0869 - acc: 0.8515 - val_loss: 1.0116 - val_acc: 0.3098\n",
      "Epoch 41/100\n",
      "152/152 [==============================] - 124s 817ms/step - loss: 0.0826 - acc: 0.8584 - val_loss: 1.0192 - val_acc: 0.3120\n",
      "Epoch 42/100\n",
      "152/152 [==============================] - 126s 832ms/step - loss: 0.0786 - acc: 0.8676 - val_loss: 1.0313 - val_acc: 0.3097\n",
      "Epoch 43/100\n",
      "152/152 [==============================] - 125s 825ms/step - loss: 0.0756 - acc: 0.8746 - val_loss: 1.0340 - val_acc: 0.3094\n",
      "Epoch 44/100\n",
      "152/152 [==============================] - 122s 803ms/step - loss: 0.0710 - acc: 0.8824 - val_loss: 1.0380 - val_acc: 0.3119\n",
      "Epoch 45/100\n",
      "152/152 [==============================] - 128s 842ms/step - loss: 0.0694 - acc: 0.8866 - val_loss: 1.0310 - val_acc: 0.3098\n",
      "Epoch 46/100\n",
      "152/152 [==============================] - 129s 846ms/step - loss: 0.0654 - acc: 0.8947 - val_loss: 1.0505 - val_acc: 0.3092\n",
      "Epoch 47/100\n",
      "152/152 [==============================] - 128s 842ms/step - loss: 0.0617 - acc: 0.9030 - val_loss: 1.0645 - val_acc: 0.3069\n",
      "Epoch 48/100\n",
      "152/152 [==============================] - 127s 834ms/step - loss: 0.0592 - acc: 0.9072 - val_loss: 1.0632 - val_acc: 0.3112\n",
      "Epoch 49/100\n",
      "152/152 [==============================] - 125s 824ms/step - loss: 0.0553 - acc: 0.9145 - val_loss: 1.0679 - val_acc: 0.3062\n",
      "Epoch 50/100\n",
      "152/152 [==============================] - 127s 839ms/step - loss: 0.0538 - acc: 0.9178 - val_loss: 1.0740 - val_acc: 0.3075\n",
      "Epoch 51/100\n",
      "152/152 [==============================] - 123s 810ms/step - loss: 0.0514 - acc: 0.9227 - val_loss: 1.0840 - val_acc: 0.3079\n",
      "Epoch 52/100\n",
      "152/152 [==============================] - 125s 825ms/step - loss: 0.0491 - acc: 0.9265 - val_loss: 1.0951 - val_acc: 0.3075\n",
      "Epoch 53/100\n",
      "152/152 [==============================] - 126s 830ms/step - loss: 0.0463 - acc: 0.9299 - val_loss: 1.0923 - val_acc: 0.3081\n",
      "Epoch 54/100\n",
      "152/152 [==============================] - 126s 827ms/step - loss: 0.0442 - acc: 0.9340 - val_loss: 1.0946 - val_acc: 0.3059\n",
      "Epoch 55/100\n",
      "152/152 [==============================] - 124s 813ms/step - loss: 0.0418 - acc: 0.9397 - val_loss: 1.1044 - val_acc: 0.3064\n",
      "Epoch 56/100\n",
      "152/152 [==============================] - 125s 820ms/step - loss: 0.0394 - acc: 0.9431 - val_loss: 1.1146 - val_acc: 0.3063\n",
      "Epoch 57/100\n",
      "152/152 [==============================] - 126s 832ms/step - loss: 0.0380 - acc: 0.9459 - val_loss: 1.1085 - val_acc: 0.3056\n",
      "Epoch 58/100\n",
      "152/152 [==============================] - 125s 822ms/step - loss: 0.0361 - acc: 0.9489 - val_loss: 1.1154 - val_acc: 0.3069\n",
      "Epoch 59/100\n",
      "152/152 [==============================] - 128s 845ms/step - loss: 0.0347 - acc: 0.9512 - val_loss: 1.1240 - val_acc: 0.3069\n",
      "Epoch 60/100\n",
      "152/152 [==============================] - 127s 838ms/step - loss: 0.0328 - acc: 0.9537 - val_loss: 1.1298 - val_acc: 0.3065\n",
      "Epoch 61/100\n",
      "152/152 [==============================] - 126s 829ms/step - loss: 0.0316 - acc: 0.9569 - val_loss: 1.1309 - val_acc: 0.3054\n",
      "Epoch 62/100\n",
      "152/152 [==============================] - 127s 834ms/step - loss: 0.0300 - acc: 0.9601 - val_loss: 1.1426 - val_acc: 0.3040\n",
      "Epoch 63/100\n",
      "152/152 [==============================] - 127s 839ms/step - loss: 0.0283 - acc: 0.9623 - val_loss: 1.1633 - val_acc: 0.3055\n",
      "Epoch 64/100\n",
      "152/152 [==============================] - 127s 836ms/step - loss: 0.0272 - acc: 0.9642 - val_loss: 1.1514 - val_acc: 0.3056\n",
      "Epoch 65/100\n",
      "152/152 [==============================] - 126s 827ms/step - loss: 0.0258 - acc: 0.9661 - val_loss: 1.1585 - val_acc: 0.3060\n",
      "Epoch 66/100\n",
      "152/152 [==============================] - 124s 820ms/step - loss: 0.0247 - acc: 0.9685 - val_loss: 1.1613 - val_acc: 0.3075\n",
      "Epoch 67/100\n",
      "152/152 [==============================] - 128s 841ms/step - loss: 0.0236 - acc: 0.9705 - val_loss: 1.1677 - val_acc: 0.3062\n",
      "Epoch 68/100\n",
      "152/152 [==============================] - 127s 839ms/step - loss: 0.0228 - acc: 0.9711 - val_loss: 1.1644 - val_acc: 0.3078\n",
      "Epoch 69/100\n",
      "152/152 [==============================] - 125s 822ms/step - loss: 0.0221 - acc: 0.9715 - val_loss: 1.1633 - val_acc: 0.3047\n",
      "Epoch 70/100\n",
      "152/152 [==============================] - 126s 831ms/step - loss: 0.0205 - acc: 0.9751 - val_loss: 1.1837 - val_acc: 0.3059\n",
      "Epoch 71/100\n",
      "152/152 [==============================] - 125s 826ms/step - loss: 0.0196 - acc: 0.9758 - val_loss: 1.1850 - val_acc: 0.3045\n",
      "Epoch 72/100\n",
      "152/152 [==============================] - 127s 833ms/step - loss: 0.0187 - acc: 0.9769 - val_loss: 1.1931 - val_acc: 0.3062\n",
      "Epoch 73/100\n",
      "152/152 [==============================] - 124s 816ms/step - loss: 0.0177 - acc: 0.9787 - val_loss: 1.1873 - val_acc: 0.3031\n",
      "Epoch 74/100\n",
      "152/152 [==============================] - 126s 832ms/step - loss: 0.0171 - acc: 0.9795 - val_loss: 1.1968 - val_acc: 0.3072\n",
      "Epoch 75/100\n",
      "152/152 [==============================] - 125s 823ms/step - loss: 0.0161 - acc: 0.9803 - val_loss: 1.2035 - val_acc: 0.3040\n",
      "Epoch 76/100\n",
      "152/152 [==============================] - 125s 821ms/step - loss: 0.0152 - acc: 0.9810 - val_loss: 1.2029 - val_acc: 0.3044\n",
      "Epoch 77/100\n",
      "152/152 [==============================] - 124s 817ms/step - loss: 0.0146 - acc: 0.9828 - val_loss: 1.2043 - val_acc: 0.3051\n",
      "Epoch 78/100\n",
      "152/152 [==============================] - 126s 829ms/step - loss: 0.0139 - acc: 0.9836 - val_loss: 1.2035 - val_acc: 0.3057\n",
      "Epoch 79/100\n",
      "152/152 [==============================] - 126s 828ms/step - loss: 0.0133 - acc: 0.9839 - val_loss: 1.2239 - val_acc: 0.3050\n",
      "Epoch 80/100\n",
      "152/152 [==============================] - 125s 822ms/step - loss: 0.0125 - acc: 0.9843 - val_loss: 1.2117 - val_acc: 0.3051\n",
      "Epoch 81/100\n",
      "152/152 [==============================] - 125s 823ms/step - loss: 0.0124 - acc: 0.9840 - val_loss: 1.2030 - val_acc: 0.3061\n",
      "Epoch 82/100\n",
      "152/152 [==============================] - 125s 825ms/step - loss: 0.0118 - acc: 0.9861 - val_loss: 1.2227 - val_acc: 0.3056\n",
      "Epoch 83/100\n",
      "152/152 [==============================] - 125s 825ms/step - loss: 0.0113 - acc: 0.9862 - val_loss: 1.2405 - val_acc: 0.3061\n",
      "Epoch 84/100\n",
      "152/152 [==============================] - 125s 822ms/step - loss: 0.0105 - acc: 0.9878 - val_loss: 1.2222 - val_acc: 0.3054\n",
      "Epoch 85/100\n",
      "152/152 [==============================] - 128s 842ms/step - loss: 0.0105 - acc: 0.9871 - val_loss: 1.2405 - val_acc: 0.3064\n",
      "Epoch 86/100\n",
      "152/152 [==============================] - 127s 838ms/step - loss: 0.0098 - acc: 0.9879 - val_loss: 1.2325 - val_acc: 0.3046\n",
      "Epoch 87/100\n",
      "152/152 [==============================] - 125s 820ms/step - loss: 0.0093 - acc: 0.9881 - val_loss: 1.2334 - val_acc: 0.3023\n",
      "Epoch 88/100\n",
      "152/152 [==============================] - 130s 859ms/step - loss: 0.0092 - acc: 0.9881 - val_loss: 1.2477 - val_acc: 0.3050\n",
      "Epoch 89/100\n",
      "152/152 [==============================] - 127s 839ms/step - loss: 0.0087 - acc: 0.9883 - val_loss: 1.2435 - val_acc: 0.3032\n",
      "Epoch 90/100\n",
      "152/152 [==============================] - 128s 841ms/step - loss: 0.0084 - acc: 0.9895 - val_loss: 1.2670 - val_acc: 0.3030\n",
      "Epoch 91/100\n",
      "152/152 [==============================] - 128s 843ms/step - loss: 0.0081 - acc: 0.9893 - val_loss: 1.2539 - val_acc: 0.3043\n",
      "Epoch 92/100\n",
      "152/152 [==============================] - 128s 843ms/step - loss: 0.0077 - acc: 0.9892 - val_loss: 1.2599 - val_acc: 0.3022\n",
      "Epoch 93/100\n",
      "152/152 [==============================] - 128s 842ms/step - loss: 0.0073 - acc: 0.9902 - val_loss: 1.2711 - val_acc: 0.3033\n",
      "Epoch 94/100\n",
      "152/152 [==============================] - 124s 819ms/step - loss: 0.0069 - acc: 0.9905 - val_loss: 1.2621 - val_acc: 0.3063\n",
      "Epoch 95/100\n",
      "152/152 [==============================] - 125s 825ms/step - loss: 0.0068 - acc: 0.9901 - val_loss: 1.2605 - val_acc: 0.3032\n",
      "Epoch 96/100\n",
      "152/152 [==============================] - 126s 828ms/step - loss: 0.0065 - acc: 0.9903 - val_loss: 1.2817 - val_acc: 0.3047\n",
      "Epoch 97/100\n",
      "152/152 [==============================] - 126s 831ms/step - loss: 0.0063 - acc: 0.9910 - val_loss: 1.2559 - val_acc: 0.3035\n",
      "Epoch 98/100\n",
      "152/152 [==============================] - 126s 833ms/step - loss: 0.0060 - acc: 0.9909 - val_loss: 1.2821 - val_acc: 0.3050\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 131s 860ms/step - loss: 0.0059 - acc: 0.9899 - val_loss: 1.3017 - val_acc: 0.3062\n",
      "Epoch 100/100\n",
      "152/152 [==============================] - 135s 886ms/step - loss: 0.0056 - acc: 0.9914 - val_loss: 1.2865 - val_acc: 0.3043\n"
     ]
    }
   ],
   "source": [
    "combined_model, encoder_model, decoder_model = model_seq_to_seq(batch_size=64, epochs=100, mask_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: 0    مرحبا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  welcome _END\n",
      "-\n",
      "Input sentence: 1    اركض\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  run _END\n",
      "-\n",
      "Input sentence: 2    اخفض رأسك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  duck _END\n",
      "-\n",
      "Input sentence: 3    اخفضي رأسك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  duck _END\n",
      "-\n",
      "Input sentence: 4    اخفضوا رؤوسكم\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  duck _END\n",
      "-\n",
      "Input sentence: 5    النجده\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  help _END\n",
      "-\n",
      "Input sentence: 6    اقفز\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  jump _END\n",
      "-\n",
      "Input sentence: 7    قف\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  stand up _END\n",
      "-\n",
      "Input sentence: 8    توقف \n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  stop _END\n",
      "-\n",
      "Input sentence: 9    إنتظر\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  wait _END\n",
      "-\n",
      "Input sentence: 10    داوم\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  go on _END\n",
      "-\n",
      "Input sentence: 11    استمر\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  go on _END\n",
      "-\n",
      "Input sentence: 12    مرحبا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  welcome _END\n",
      "-\n",
      "Input sentence: 13    تعجل\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  hurry _END\n",
      "-\n",
      "Input sentence: 14    استعجل\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  hurry _END\n",
      "-\n",
      "Input sentence: 15    انا اري\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i see _END\n",
      "-\n",
      "Input sentence: 16    أنا فزت\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i won _END\n",
      "-\n",
      "Input sentence: 17    استرح\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  take a rest _END\n",
      "-\n",
      "Input sentence: 18    ابتسم\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  smile _END\n",
      "-\n",
      "Input sentence: 19    في صحتك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  cheers _END\n",
      "-\n",
      "Input sentence: 20    هل فهمت\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  got it _END\n",
      "-\n",
      "Input sentence: 21    ركض\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he ran _END\n",
      "-\n",
      "Input sentence: 22    أعرف\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i know _END\n",
      "-\n",
      "Input sentence: 23    أعلم ذلك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i know _END\n",
      "-\n",
      "Input sentence: 24    أنا أعلم\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i know _END\n",
      "-\n",
      "Input sentence: 25    أنا في \n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im  _END\n",
      "-\n",
      "Input sentence: 26    أنا بخير\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im ok _END\n",
      "-\n",
      "Input sentence: 27    استمع\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  listen _END\n",
      "-\n",
      "Input sentence: 28    غير معقول\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  no way _END\n",
      "-\n",
      "Input sentence: 29    حقا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  really _END\n",
      "-\n",
      "Input sentence: 30    شكرا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  thanks _END\n",
      "-\n",
      "Input sentence: 31    شكرا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  thanks _END\n",
      "-\n",
      "Input sentence: 32    لماذا أنا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  why me _END\n",
      "-\n",
      "Input sentence: 33    رائع\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  terrific _END\n",
      "-\n",
      "Input sentence: 34    خذ راحتك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  be cool _END\n",
      "-\n",
      "Input sentence: 35    أغرب عن وجهي\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  beat it _END\n",
      "-\n",
      "Input sentence: 36    هاتفني\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  call me _END\n",
      "-\n",
      "Input sentence: 37    اتصل بي\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  call me _END\n",
      "-\n",
      "Input sentence: 38    تفضل بالدخول\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  come inside _END\n",
      "-\n",
      "Input sentence: 39    تعال إلى الداخل\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  come in _END\n",
      "-\n",
      "Input sentence: 40    بالله عليك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  come on _END\n",
      "-\n",
      "Input sentence: 41    هيا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  come on _END\n",
      "-\n",
      "Input sentence: 42    هيا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  come on _END\n",
      "-\n",
      "Input sentence: 43    اخرج من هنا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  get out of here _END\n",
      "-\n",
      "Input sentence: 44    أخرج\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  get out _END\n",
      "-\n",
      "Input sentence: 45    اخرج\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  get out _END\n",
      "-\n",
      "Input sentence: 46    اتركني و شأني\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  leave me alone _END\n",
      "-\n",
      "Input sentence: 47    اذهب بعيدا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  go away _END\n",
      "-\n",
      "Input sentence: 48    ارحل\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  go away _END\n",
      "-\n",
      "Input sentence: 49    مع السلامه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  take care _END\n",
      "-\n",
      "Input sentence: 50    إلى اللقاء\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  see you again _END\n",
      "-\n",
      "Input sentence: 51    إنتظر\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  wait _END\n",
      "-\n",
      "Input sentence: 52    لقد أتى\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he came _END\n",
      "-\n",
      "Input sentence: 53    هو يجري\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he runs _END\n",
      "-\n",
      "Input sentence: 54    ساعدني\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  help me _END\n",
      "-\n",
      "Input sentence: 55    النجده ساعدني\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  help me _END\n",
      "-\n",
      "Input sentence: 56    ساعدوني\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  help me _END\n",
      "-\n",
      "Input sentence: 57    انتظر\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  wait _END\n",
      "-\n",
      "Input sentence: 58    أنا موافق\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i agree _END\n",
      "-\n",
      "Input sentence: 59    أنا حزين\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im sad _END\n",
      "-\n",
      "Input sentence: 60    أنا أيضا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  me too _END\n",
      "-\n",
      "Input sentence: 61    اخرس\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  shut up _END\n",
      "-\n",
      "Input sentence: 62    اصمت\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  shut up _END\n",
      "-\n",
      "Input sentence: 63    اسكت\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  shut up _END\n",
      "-\n",
      "Input sentence: 64    أغلق فمك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  shut your mouth _END\n",
      "-\n",
      "Input sentence: 65    أوقفه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  stop it _END\n",
      "-\n",
      "Input sentence: 66    خذه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  take it _END\n",
      "-\n",
      "Input sentence: 67    أخبرني\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  tell me _END\n",
      "-\n",
      "Input sentence: 68    توم فاز\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  tom won _END\n",
      "-\n",
      "Input sentence: 69    لقد ربح توم\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  tom won _END\n",
      "-\n",
      "Input sentence: 70    استيقظ\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  wake up _END\n",
      "-\n",
      "Input sentence: 71    أهلا و سهلا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  welcome _END\n",
      "-\n",
      "Input sentence: 72    مرحبا بك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  welcome _END\n",
      "-\n",
      "Input sentence: 73    اهلا وسهلا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  welcome _END\n",
      "-\n",
      "Input sentence: 74    مرحبا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  welcome _END\n",
      "-\n",
      "Input sentence: 75    من فاز\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  who won _END\n",
      "-\n",
      "Input sentence: 76    من الذي ربح\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  who won _END\n",
      "-\n",
      "Input sentence: 77    لم لا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  why not _END\n",
      "-\n",
      "Input sentence: 78    لما لا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  why not _END\n",
      "-\n",
      "Input sentence: 79    لا فكره لدي\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  beats me _END\n",
      "-\n",
      "Input sentence: 80    استمتع بوقتك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  have fun _END\n",
      "-\n",
      "Input sentence: 81    أسرعا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  hurry up _END\n",
      "-\n",
      "Input sentence: 82    لقد نسيت\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i forgot _END\n",
      "-\n",
      "Input sentence: 83    فهمته\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i got it _END\n",
      "-\n",
      "Input sentence: 84    فهمتها\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i got it _END\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: 85    فهمت ذلك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i got it _END\n",
      "-\n",
      "Input sentence: 86    أستخدمه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im using it _END\n",
      "-\n",
      "Input sentence: 87    سأدفع أنا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  ill pay _END\n",
      "-\n",
      "Input sentence: 88    أنا مشغول\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im not free _END\n",
      "-\n",
      "Input sentence: 89    إنني مشغول\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im busy _END\n",
      "-\n",
      "Input sentence: 90    أشعر بالبرد\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i am cold _END\n",
      "-\n",
      "Input sentence: 91    أنا حر\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im free _END\n",
      "-\n",
      "Input sentence: 92    أنا هنا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im here _END\n",
      "-\n",
      "Input sentence: 93    لقد عدت إلى البيت\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im home _END\n",
      "-\n",
      "Input sentence: 94    أنا فقير\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im poor _END\n",
      "-\n",
      "Input sentence: 95    أنا ثري\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im rich _END\n",
      "-\n",
      "Input sentence: 96    هذا مؤلم\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  it hurts _END\n",
      "-\n",
      "Input sentence: 97    انها جافه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  its dry _END\n",
      "-\n",
      "Input sentence: 98    الجو حار\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  its hot _END\n",
      "-\n",
      "Input sentence: 99    إنه جديد\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  its new _END\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(100): #[14077,20122,40035,40064, 40056, 40068, 40090, 40095, 40100, 40119, 40131, 40136, 40150, 40153]:\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', dataset.Arabic_input[seq_index: seq_index + 1])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Here although the accuracy is a small value. But if we notice the translation is correct. The accuracy in problems of sequence to sequence can't be cosidered as a metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "- https://github.com/motazsaad/process-arabic-text/blob/master/clean_arabic_text.py\n",
    "- https://thescipub.com/pdf/jcssp.2020.117.125.pdf\n",
    "- Hands -On Python Natural Language Processing Book for Aman Kedia and Mayank Rasu\n",
    "- https://colab.research.google.com/drive/1dhlc3Nt_LvZcxY5tUd-XLU1fvGt4pPuh?usp=sharing\n",
    "- https://github.com/CAMeL-Lab/camel_tools#installing-data\n",
    "- https://colab.research.google.com/drive/1Y3qCbD6Gw1KEw-lixQx1rI6WlyWnrnDS?usp=sharing#scrollTo=9knGLLGg7cnm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
