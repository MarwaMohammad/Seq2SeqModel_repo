{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq with Attention\n",
    "\n",
    "## Introduction\n",
    "\n",
    "\n",
    "In the last tutorial, we used the Seq2Seq model which contains an encoder and decoder. The encoder is used to memorize the input sentence and feed it to the decoder. I used the Seq2Seq model to translate from Arabic to English https://github.com/MarwaMohammad/Seq2SeqModel_repo/blob/main/Seq2SeqModel.ipynb\n",
    " \n",
    "The Seq2Seq was considered a big step towards generating a new sequence based on the input sequence.  it has some drawbacks, like, in case the input to the encoder is a little long sentence. the decoder can't memorize all the input sentence words. For example, in translation example, if the encoder has a long Arabic sentence the decoder can't memorize the whole sentence words embeddings inside the context vector which will be fed to the decoder after that.\n",
    "\n",
    "\n",
    "Mainly, the human translation also doesn't like that. If we asked the translator to translate the long sentence, the translator divides the sentence into smaller parts and translates each part 'means a few tokens' together beacuse it will be difficult to memorize the whole sentence.\n",
    "\n",
    "For this reason, the translation task performance using Seq2Seq has a good performance in short sentences, but its performance comes down in long sentences.\n",
    "\n",
    "So, the solution for this is to divide the input sentence into tokens. Then, compute the attention vector weights for all encoder timsteps 'or for all your input tokens', and provide this attention vector to the intended decoder cell. So that this will help this decoder cell to give more attention to the intended token like the human translator exactly. Let's understand how this can be done mathematically using deep learning.\n",
    "\n",
    "## Attention Mechanism:\n",
    "\n",
    "We will devide it into steps to more easier,\n",
    "\n",
    "### 1st step: obtainning the Encoder the Hidden States \n",
    "\n",
    "Feed the input into encoder and obtain the encoder the hidden states for each input token\n",
    "\n",
    "![title](img/encode_attention.png)\n",
    "\n",
    "\n",
    "### 2nd step: Calculating the Attention scores\n",
    "\n",
    "Pass the encoder hidden state (h) at all timesteps and decoder previous hidden state into a scoring function such as dot ptroduct to compute the similarity. The dot product will the return the similarity between the each encoder hidden state and the decoder hidden state.\n",
    "\n",
    "\n",
    "![title](img/step2_attention.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above image all the encoder output hidden states of all LSTMs [h1,h2,h3,h4,h5] are passed to the score function 'dot product here' with the input hidden state to the 1st decoder cell as follows:\n",
    "\n",
    "$ b_{01} = s_{0} • h_{1} = 11$\n",
    "\n",
    "$ b_{02} = s_{0} • h_{2} = 4$\n",
    "\n",
    "$ b_{03} = s_{0} • h_{3} = 4$\n",
    "\n",
    "$ b_{04} = s_{0} • h_{4} = 2$\n",
    "\n",
    "$ b_{05} = s_{0} • h_{5} = 0$\n",
    "\n",
    "This operation will output a vector looks like this [$ b_{01},  b_{02}, b_{03}, b_{04}, b_{05}$] = [11, 4, 4, 2, 0]. These numbers indicate the importance of each hidden state in the encoder part to the output token at cell1 in the decoder part.\n",
    "\n",
    "we notice that the $ b_{01}$ is the highest output value of the decoder output which indicates that the input token 'كيف' has a great effect on the output token 'how' in the decoder part.\n",
    "\n",
    "Note:\n",
    "\n",
    "The above operation will be repeated but with the next decoder hidden state so the dot product this time between encoder output hidden states of all LSTMs [h1,h2,h3,h4,h5] and $s_{1}$.\n",
    "\n",
    "This operation will keep repeating until the end token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3rd Step: Normalizing the attention scores\n",
    "\n",
    "The attention scores are normalized using softmax function to have values from 0 to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ a_{ij} = \\frac{exp(b_{ij})}{\\sum \\limits _{k=1} ^{T_{x}} exp(b_{ik}) }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "$ b_{ij} $ is the influence of input token j in the prediction of output i which comes from the 2nd step.\n",
    "\n",
    "$ T_{x} $ is the length of input tokens\n",
    "\n",
    "$ a_{ij}$ is the probability of the output token $ y_{i}$ being alligned to the input $ x_{j}$\n",
    "\n",
    "This operation of calculating the probability of the output token $ y_{1}$ being alligned to the input will be repeated for each time step or token $ x_{1}$, $ x_{2}$, $ x_{3}$, $ x_{4}$, $ x_{5}$ \n",
    "\n",
    "We will have $ a_{01}$, $ a_{02}$, $ a_{03}$, $ a_{04}$, $ a_{05}$ for each input token to \n",
    "\n",
    "The output of this step is as follows [$ a_{01}$, $ a_{02}$, $ a_{03}$, $ a_{04}$, $ a_{05}$] = [0.998, 0.00091, 0.00091, 0.00012,  0]\n",
    "\n",
    "\n",
    "### 4th Step : Computing the context vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute a context vector for the current decoder state by taking a weighted average over all the encoder hidden states.\n",
    "\n",
    "$C_{i} = {\\sum \\limits _{j=1} ^{T_{x}} } a_{ij} x h_{j}$\n",
    "\n",
    "The above context vector $C_{i}$ carries the weights of each input token to determine the output token at timestep i.\n",
    "\n",
    "These steps from 1 to 4 will be repeated to obtain the context vector for each decoder timestep.\n",
    "\n",
    "The attention mechanism enhances the neural machine translation.\n",
    "\n",
    "We finally have a context vector that considers the information from the entire encoder states that is dynamically updated to reflect the needs of the decoder at each step of decoding. This context vector will be fed into the decoder cell as input as shown in the following diagram.\n",
    "\n",
    "\n",
    "![title](img/context_vector_of_attention.png)\n",
    "\n",
    "What I have illustrated above is the Bahdanau Attention Algorithm, but the Bahdanau attention used the BiLSTM. I used LSTM just for simplicity.\n",
    "\n",
    "In Bahdanau Attention Algorithm, the decoder LSTM cell has the following inputs:\n",
    "\n",
    "- The Previous outputs.\n",
    "- The Previous hidden state.\n",
    "- The context vector obtained from the $C_{i} = {\\sum \\limits _{j=1} ^{T_{x}} } a_{ij} x h_{j}$\n",
    "\n",
    "\n",
    "To know more about Bahdanau Attention Algorithm please look at [The Bahdanau Attention Algorithm](https://machinelearningmastery.com/the-bahdanau-attention-mechanism/), [NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE](https://arxiv.org/pdf/1409.0473.pdf)\n",
    " and [This book, page 224, Figure10.10](https://web.stanford.edu/~jurafsky/slp3/ed3book_jan122022.pdf)\n",
    "\n",
    "There are some enhancement happened to Bahdanau Attention Algorithm which is called the Luong mechanism. Lets know the Luong mechanism in the next section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Luong mechanism\n",
    "\n",
    "It improved the Bahdanau mechanism. It looks like the Bahdanau mechanism but with some differences, let's see similarities and differences:\n",
    "\n",
    "- Step 1 is similar to Bahdanau.\n",
    "\n",
    "\n",
    "- Step 2 which is calculating the Attention Scores, use the current hidden state instead of using the previous hidden state.\n",
    "\n",
    "\n",
    "- Step 3 is similar to Bahdanau.\n",
    "\n",
    "\n",
    "- Step 4 is similar to Bahdanau.\n",
    "\n",
    "\n",
    "- Step 5: Calculating an attention hidden state which considered is a concatenation between the current decoder hidden state and the context vector.\n",
    "\n",
    "\n",
    "Then feed this to the softmax function to obtain the output token. To read more please open [this, The Luong Attention Algorithm part](https://machinelearningmastery.com/the-luong-attention-mechanism/)\n",
    "\n",
    "Note:\n",
    "The steps from 1 to 5 will repeated until obtaining the end of the sequence.\n",
    "\n",
    "Look at the following figure it will show all steps together:\n",
    "\n",
    "![title](img/context_vector_of_attention_Loungh.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Translation using Seq2Seq with Attention Modeling:\n",
    "I will use a simple version of Luong attention which will use the score function as dot product.\n",
    "\n",
    "Lets import all our preprocessing code from the previous [Seq2Seq Notebook](https://github.com/MarwaMohammad/Seq2SeqModel_repo/blob/main/Seq2SeqModel.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense,LSTM,Embedding,Input, Bidirectional, Concatenate, Dot, Activation\n",
    "import io\n",
    "from string import digits\n",
    "import string\n",
    "import tkseem as tk\n",
    "# instantiate the Maximum Likelihood Disambiguator\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "from camel_tools.disambig.mle import MLEDisambiguator\n",
    "from camel_tools.tokenizers.morphological import MorphologicalTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining the dataset for Arabic and English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file):\n",
    "    file_data = []\n",
    "    data = []\n",
    "    Arabic_data = []\n",
    "    english_data = []\n",
    "    # Read the file lines\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        file_data = f.readlines()\n",
    "    # separate the lines using '\\t'\n",
    "    for line in (file_data):\n",
    "        english_sent, arabic_sent, _ = line.split('\\t')\n",
    "        Arabic_data.append(arabic_sent)\n",
    "        english_data.append(english_sent)\n",
    "        \n",
    "    return english_data, Arabic_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'datasets/ara-eng/ara.txt'\n",
    "english_data,  Arabic_data= read_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi.',\n",
       " 'Run!',\n",
       " 'Duck!',\n",
       " 'Duck!',\n",
       " 'Duck!',\n",
       " 'Help!',\n",
       " 'Jump!',\n",
       " 'Stop!',\n",
       " 'Stop!',\n",
       " 'Wait!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['إلى اللقاء',\n",
       " 'إنتظر',\n",
       " 'لقد أتى.',\n",
       " 'هو يجري',\n",
       " 'ساعدني!',\n",
       " 'النجدة! ساعدني!',\n",
       " 'ساعدوني',\n",
       " 'انتظر.',\n",
       " 'أنا موافق',\n",
       " 'أنا حزين.',\n",
       " 'أنا أيضاً.',\n",
       " 'اخرس!',\n",
       " 'اصمت!',\n",
       " 'اسكت!',\n",
       " 'أغلق فمك!',\n",
       " 'أوقفه',\n",
       " 'خذه',\n",
       " 'أخبرني',\n",
       " 'توم فاز.',\n",
       " 'لقد ربح توم.',\n",
       " 'استيقظ!',\n",
       " 'أهلاً و سهلاً!',\n",
       " 'مرحباً بك!',\n",
       " 'اهلا وسهلا',\n",
       " 'مرحبا!',\n",
       " 'من فاز؟',\n",
       " 'من الذي ربح؟',\n",
       " 'لم لا؟',\n",
       " 'لما لا؟',\n",
       " 'لا فكرة لدي',\n",
       " 'استمتع بوقتك.',\n",
       " 'أسرعا.',\n",
       " 'لقد نسيت.',\n",
       " 'فهمتُهُ.',\n",
       " 'فهمتُها.',\n",
       " 'فَهمتُ ذلك.',\n",
       " 'أستخدمه.',\n",
       " 'سأدفع أنا.',\n",
       " 'أنا مشغول.',\n",
       " 'إنني مشغول.',\n",
       " 'أشعر بالبرد.',\n",
       " 'أنا حُرّ.',\n",
       " 'أنا هنا',\n",
       " 'لقد عدت إلى البيت',\n",
       " 'أنا فقير.',\n",
       " 'أنا ثري.',\n",
       " 'هذا مؤلم',\n",
       " 'انها جافه',\n",
       " 'الجو حار',\n",
       " 'إنه جديد']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Arabic_data[50:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12158"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Arabic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12158"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 12158 line in both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame({'Arabic_input':Arabic_data, 'English_target':english_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Arabic_input</th>\n",
       "      <th>English_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>مرحبًا.</td>\n",
       "      <td>Hi.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>اركض!</td>\n",
       "      <td>Run!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>اخفض رأسك!</td>\n",
       "      <td>Duck!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>اخفضي رأسك!</td>\n",
       "      <td>Duck!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>اخفضوا رؤوسكم!</td>\n",
       "      <td>Duck!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Arabic_input English_target\n",
       "0         مرحبًا.            Hi.\n",
       "1           اركض!           Run!\n",
       "2      اخفض رأسك!          Duck!\n",
       "3     اخفضي رأسك!          Duck!\n",
       "4  اخفضوا رؤوسكم!          Duck!"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arabic Normalization\n",
    "The Arabic has a special preprocessing, why?\n",
    "\n",
    "Arabic has difirrent characteristics like:\n",
    "- The word in Arabic can mean a complete sentence in other languages. So, it requires a special segmentation step which considers the Arabic language rules. \n",
    "- Arabic has diactrics which should be normalized. \n",
    "- Some Aabic letters has more than one shape so, it should be unified.\n",
    "\n",
    "If you would like to know more about Arabic charasteristics, please read this part  \"Arabic Challenges in the Context of NER\" in the the following paper:\n",
    "\n",
    "https://thescipub.com/pdf/jcssp.2020.117.125.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the following repo: https://github.com/motazsaad/process-arabic-text/blob/master/clean_arabic_text.py\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = arabic_punctuations + english_punctuations\n",
    "\n",
    "arabic_diacritics = re.compile(\"\"\"\n",
    "                             ّ    | # Tashdid\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "\n",
    "\n",
    "def normalize_arabic(text):\n",
    "    #text = re.sub(\"[إأآا]\", \"ا\", text)\n",
    "\n",
    "    #text = re.sub(\"ى\", \"ي\", text)\n",
    "    #text = re.sub(\"ؤ\", \"ء\", text)\n",
    "    #text = re.sub(\"ئ\", \"ء\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    text = re.sub(\"گ\", \"ك\", text)\n",
    "    return text\n",
    "\n",
    "def remove_digits(text):\n",
    "    text = re.sub(r\"[1234567890١٢٣٤٥٦٧٨٩٠]+\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def remove_english_characters(text):\n",
    "    text = re.sub(r'[a-zA-Z]+','',text)\n",
    "    return text\n",
    "    \n",
    "\n",
    "def remove_diacritics(text):\n",
    "    text = re.sub(arabic_diacritics, '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Arabic_normalization(Arabic_sentence_list):\n",
    "    Arabic_data_list = []\n",
    "    for item in Arabic_sentence_list:\n",
    "        text = remove_english_characters(item)\n",
    "        text = remove_digits(text)\n",
    "        text = normalize_arabic(text)\n",
    "        text = remove_diacritics(text)\n",
    "        text = remove_punctuations(text)\n",
    "        Arabic_data_list.append(text)\n",
    "        \n",
    "    return Arabic_data_list    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Arabic_input'] = Arabic_normalization(dataset.Arabic_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            مرحبا\n",
       "1             اركض\n",
       "2        اخفض رأسك\n",
       "3       اخفضي رأسك\n",
       "4    اخفضوا رؤوسكم\n",
       "5           النجده\n",
       "6             اقفز\n",
       "7               قف\n",
       "8            توقف \n",
       "9            إنتظر\n",
       "Name: Arabic_input, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Arabic_input'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English Normalization:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def English_normalization(English_target_ls):\n",
    "    English_data_list = []\n",
    "    # Since we work on word level, if we normalize the text to lower case, this will reduce the vocabulary. \n",
    "    #It's easy to recover the case later. \n",
    "    English_data_list = English_target_ls.apply(lambda x: x.lower())\n",
    "\n",
    "    # Clean up punctuations and digits. Such special chars are common to both domains, and can just be copied with no error.\n",
    "    exclude = set(string.punctuation)\n",
    "    English_data_list = English_data_list.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
    "\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    English_data_list = English_data_list.apply(lambda x: x.translate(remove_digits))\n",
    "    \n",
    "    return English_data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['English_target'] = English_normalization(dataset.English_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Arabic_input</th>\n",
       "      <th>English_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>مرحبا</td>\n",
       "      <td>hi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>اركض</td>\n",
       "      <td>run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>اخفض رأسك</td>\n",
       "      <td>duck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>اخفضي رأسك</td>\n",
       "      <td>duck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>اخفضوا رؤوسكم</td>\n",
       "      <td>duck</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Arabic_input English_target\n",
       "0          مرحبا             hi\n",
       "1           اركض            run\n",
       "2      اخفض رأسك           duck\n",
       "3     اخفضي رأسك           duck\n",
       "4  اخفضوا رؤوسكم           duck"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation:\n",
    "In this step we start to convert the data for training. \n",
    "- Adding the start and end token to the target language.Since the english is our target, so we will add the start and tokens to it. lets see how can we do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_tok = 'START_'\n",
    "end_tok = '_END'\n",
    "def data_prep():\n",
    "    dataset.English_target = dataset.English_target.apply(lambda x : st_tok + ' ' + x + ' ' + end_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Arabic_input</th>\n",
       "      <th>English_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>مرحبا</td>\n",
       "      <td>START_ hi _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>اركض</td>\n",
       "      <td>START_ run _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>اخفض رأسك</td>\n",
       "      <td>START_ duck _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>اخفضي رأسك</td>\n",
       "      <td>START_ duck _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>اخفضوا رؤوسكم</td>\n",
       "      <td>START_ duck _END</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Arabic_input    English_target\n",
       "0          مرحبا    START_ hi _END\n",
       "1           اركض   START_ run _END\n",
       "2      اخفض رأسك  START_ duck _END\n",
       "3     اخفضي رأسك  START_ duck _END\n",
       "4  اخفضوا رؤوسكم  START_ duck _END"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenization\n",
    "Arabic tokenization is completely diffrent from English tokenization. English tokenization depends on spaces, but in Arabic this is not valid. Since the token in Arabic can be used to mean a complete sentence in another languages.\n",
    "\n",
    "Note, I used her the camel_tools which is not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arabic Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "\n",
    "def tokenize_Arabic():\n",
    "    # The tokenizer expects pre-tokenized text\n",
    "    Arabic_input_ls = dataset.Arabic_input.apply(simple_word_tokenize)\n",
    "\n",
    "    # Load a pretrained disambiguator to use with a tokenizer\n",
    "    mle = MLEDisambiguator.pretrained('calima-msa-r13')\n",
    "\n",
    "    # By specifying `split=True`, the morphological tokens are output as seperate\n",
    "    # strings.\n",
    "    tokenizer = MorphologicalTokenizer(mle,scheme='d3tok', split=True)\n",
    "    Arabic_input_ls = Arabic_input_ls.apply(tokenizer.tokenize)\n",
    "    \n",
    "    return Arabic_input_ls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_plus(tokens):\n",
    "    sentence_ls = []\n",
    "    for token in tokens:\n",
    "        if '+' in token:\n",
    "            token_without_plus = token.replace('+','')\n",
    "            sentence_ls.append(token_without_plus) \n",
    "        else:\n",
    "            sentence_ls.append(token) \n",
    "\n",
    "            \n",
    "    return sentence_ls\n",
    "#Arabic_input_ls = Arabic_input_ls.apply(remove_plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Arabic_input</th>\n",
       "      <th>English_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>مرحبا</td>\n",
       "      <td>START_ hi _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>اركض</td>\n",
       "      <td>START_ run _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>اخفض رأسك</td>\n",
       "      <td>START_ duck _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>اخفضي رأسك</td>\n",
       "      <td>START_ duck _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>اخفضوا رؤوسكم</td>\n",
       "      <td>START_ duck _END</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Arabic_input    English_target\n",
       "0          مرحبا    START_ hi _END\n",
       "1           اركض   START_ run _END\n",
       "2      اخفض رأسك  START_ duck _END\n",
       "3     اخفضي رأسك  START_ duck _END\n",
       "4  اخفضوا رؤوسكم  START_ duck _END"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds_copy = dataset.copy()\n",
    "tokenized_ds_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The camel tool returned the hamza letter again but in a unified way for all the words. I mean the same word can't exist in two difrrent spellings.\n",
    "#### English tokenization\n",
    "English is tokenized according to spaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_split_word2word(data):\n",
    "    return data.split(' ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_stats(tokenized_ds_copy):\n",
    "    #Obtain the tokenized words in Arabic\n",
    "    tokenized_ds_copy['Arabic_input'] = tokenize_Arabic()\n",
    "    # The tokenization output has + in the separated token, which should be removed\n",
    "    tokenized_ds_copy['Arabic_input'] = tokenized_ds_copy.Arabic_input.apply(remove_plus)\n",
    "    \n",
    "    #create a set to hold all Arabic words uniquely.\n",
    "    input_tokens=set()\n",
    "    for item in tokenized_ds_copy.Arabic_input:\n",
    "        for tok in item:\n",
    "            input_tokens.add(tok)\n",
    "    \n",
    "    #Obtain the tokenized words in English dataset\n",
    "    tokenized_ds_copy['English_target'] = tokenized_ds_copy.English_target.apply(tok_split_word2word)\n",
    "    \n",
    "    #create a set to hold all English words uniquely.\n",
    "    target_tokens=set()\n",
    "    for item in tokenized_ds_copy.English_target:\n",
    "        for tok in item:\n",
    "            target_tokens.add(tok)\n",
    "        \n",
    "    input_tokens = sorted(list(input_tokens))\n",
    "    target_tokens = sorted(list(target_tokens))\n",
    "\n",
    "\n",
    "    \n",
    "    num_encoder_tokens = len(input_tokens)\n",
    "    num_decoder_tokens = len(target_tokens)\n",
    "    \n",
    "    #To obtin the maximum number of words inside Arabic and English dataset.\n",
    "    max_encoder_seq_length = np.max([len(l) for l in tokenized_ds_copy.Arabic_input])\n",
    "    max_decoder_seq_length = np.max([len(l) for l in tokenized_ds_copy.English_target])\n",
    "\n",
    "    return input_tokens, target_tokens, num_encoder_tokens, num_decoder_tokens, max_encoder_seq_length, max_decoder_seq_length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens, target_tokens, num_encoder_tokens, num_decoder_tokens, max_encoder_seq_length, max_decoder_seq_length  = data_stats(tokenized_ds_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 12158\n",
      "Number of unique input tokens: 7205\n",
      "Number of unique output tokens: 4298\n",
      "Max sequence length for inputs: 52\n",
      "Max sequence length for outputs: 36\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples:', len(dataset))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization\n",
    "\n",
    "In this step we will build our vocab2int table which will be used to map between words and their indices.'Machine Learning can't work directly with words since computer doen't understand words, so it should be converted into numbers'.\n",
    "\n",
    "Note that the pad and separation should be considered during obtaining the vocab2int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_tok = 'PAD'\n",
    "sep_tok = ' '\n",
    "special_tokens = [pad_tok, sep_tok, st_tok, end_tok] \n",
    "\n",
    "#Increase the number of token by the number of special characters.\n",
    "num_encoder_tokens += len(special_tokens)\n",
    "num_decoder_tokens += len(special_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab(input_tokens, target_tokens):\n",
    "    input_token_index = {}\n",
    "    target_token_index = {}\n",
    "    for i,tok in enumerate(special_tokens):\n",
    "        input_token_index[tok] = i\n",
    "        target_token_index[tok] = i \n",
    "\n",
    "    offset = len(special_tokens)\n",
    "    for i, tok in enumerate(input_tokens):\n",
    "        input_token_index[tok] = i+offset\n",
    "\n",
    "    for i, tok in enumerate(target_tokens):\n",
    "        target_token_index[tok] = i+offset\n",
    "   \n",
    "    # Reverse-lookup token index to decode sequences back to something readable.\n",
    "    reverse_input_tok_index = dict(\n",
    "        (i, tok) for tok, i in input_token_index.items())\n",
    "    reverse_target_tok_index = dict(\n",
    "        (i, tok) for tok, i in target_token_index.items())\n",
    "    return input_token_index, target_token_index, reverse_input_tok_index, reverse_target_tok_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index, target_token_index, reverse_input_tok_index, reverse_target_tok_index = vocab(input_tokens, target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PAD': 0,\n",
       " ' ': 1,\n",
       " 'START_': 2,\n",
       " '_END': 3,\n",
       " 'NOAN': 4,\n",
       " 'آب': 5,\n",
       " 'آباء': 6,\n",
       " 'آبد': 7,\n",
       " 'آبقو': 8,\n",
       " 'آت': 9,\n",
       " 'آتون': 10,\n",
       " 'آتي': 11,\n",
       " 'آثار': 12,\n",
       " 'آخذ': 13,\n",
       " 'آخر': 14,\n",
       " 'آخرة': 15,\n",
       " 'آخرعلي': 16,\n",
       " 'آخرون': 17,\n",
       " 'آخرين': 18,\n",
       " 'آدم': 19,\n",
       " 'آذار': 20,\n",
       " 'آذان': 21,\n",
       " 'آذيتم': 22,\n",
       " 'آراء': 23,\n",
       " 'آرية': 24,\n",
       " 'آسفون': 25,\n",
       " 'آسيا': 26,\n",
       " 'آفاق': 27,\n",
       " 'آكل': 28,\n",
       " 'آلاف': 29,\n",
       " 'آلام': 30,\n",
       " 'آلة': 31,\n",
       " 'آلن': 32,\n",
       " 'آلي': 33,\n",
       " 'آليا': 34,\n",
       " 'آمال': 35,\n",
       " 'آمل': 36,\n",
       " 'آمن': 37,\n",
       " 'آمنة': 38,\n",
       " 'آن': 39,\n",
       " 'آنا': 40,\n",
       " 'آنذاك': 41,\n",
       " 'آنس': 42,\n",
       " 'آني': 43,\n",
       " 'آية': 44,\n",
       " 'أ': 45,\n",
       " 'أأريتها': 46,\n",
       " 'أأشتري': 47,\n",
       " 'أأنت': 48,\n",
       " 'أؤجر': 49,\n",
       " 'أؤذي': 50,\n",
       " 'أؤكد': 51,\n",
       " 'أؤلف': 52,\n",
       " 'أؤمن': 53,\n",
       " 'أإلى': 54,\n",
       " 'أاصيب': 55,\n",
       " 'أب': 56,\n",
       " 'أبإمكانك': 57,\n",
       " 'أبا': 58,\n",
       " 'أبتاع': 59,\n",
       " 'أبتز': 60,\n",
       " 'أبتسم': 61,\n",
       " 'أبتل': 62,\n",
       " 'أبحار': 63,\n",
       " 'أبحث': 64,\n",
       " 'أبد': 65,\n",
       " 'أبدأ': 66,\n",
       " 'أبدا': 67,\n",
       " 'أبدو': 68,\n",
       " 'أبدوا': 69,\n",
       " 'أبدين': 70,\n",
       " 'أبذل': 71,\n",
       " 'أبر': 72,\n",
       " 'أبرد': 73,\n",
       " 'أبريل': 74,\n",
       " 'أبشع': 75,\n",
       " 'أبعد': 76,\n",
       " 'أبق': 77,\n",
       " 'أبقار': 78,\n",
       " 'أبقى': 79,\n",
       " 'أبقيت': 80,\n",
       " 'أبقينا': 81,\n",
       " 'أبكر': 82,\n",
       " 'أبلغ': 83,\n",
       " 'أبلي': 84,\n",
       " 'أبليت': 85,\n",
       " 'أبناء': 86,\n",
       " 'أبو': 87,\n",
       " 'أبواب': 88,\n",
       " 'أبوي': 89,\n",
       " 'أبي': 90,\n",
       " 'أبيض': 91,\n",
       " 'أبيضا': 92,\n",
       " 'أبيع': 93,\n",
       " 'أبيه': 94,\n",
       " 'أتأخر': 95,\n",
       " 'أتأسف': 96,\n",
       " 'أتأكد': 97,\n",
       " 'أتابع': 98,\n",
       " 'أتبع': 99,\n",
       " 'أتت': 100,\n",
       " 'أتتطلع': 101,\n",
       " 'أتحب': 102,\n",
       " 'أتحبان': 103,\n",
       " 'أتحبني': 104,\n",
       " 'أتحبينني': 105,\n",
       " 'أتحدث': 106,\n",
       " 'أتحدثت': 107,\n",
       " 'أتحس': 108,\n",
       " 'أتخذ': 109,\n",
       " 'أتخرج': 110,\n",
       " 'أتخصص': 111,\n",
       " 'أتخطى': 112,\n",
       " 'أتخيل': 113,\n",
       " 'أتدرس': 114,\n",
       " 'أتذكر': 115,\n",
       " 'أترجم': 116,\n",
       " 'أترع': 117,\n",
       " 'أترك': 118,\n",
       " 'أتريد': 119,\n",
       " 'أتزوج': 120,\n",
       " 'أتساءل': 121,\n",
       " 'أتستطيع': 122,\n",
       " 'أتسكن': 123,\n",
       " 'أتسلق': 124,\n",
       " 'أتسلى': 125,\n",
       " 'أتسمح': 126,\n",
       " 'أتسوق': 127,\n",
       " 'أتصفح': 128,\n",
       " 'أتصل': 129,\n",
       " 'أتضح': 130,\n",
       " 'أتضن': 131,\n",
       " 'أتضور': 132,\n",
       " 'أتطلع': 133,\n",
       " 'أتظن': 134,\n",
       " 'أتظنها': 135,\n",
       " 'أتعامل': 136,\n",
       " 'أتعب': 137,\n",
       " 'أتعتقد': 138,\n",
       " 'أتعجب': 139,\n",
       " 'أتعديني': 140,\n",
       " 'أتعرف': 141,\n",
       " 'أتعلم': 142,\n",
       " 'أتعمد': 143,\n",
       " 'أتغدى': 144,\n",
       " 'أتفاجئ': 145,\n",
       " 'أتفرج': 146,\n",
       " 'أتفضل': 147,\n",
       " 'أتفق': 148,\n",
       " 'أتقبلون': 149,\n",
       " 'أتكلم': 150,\n",
       " 'أتل': 151,\n",
       " 'أتلعب': 152,\n",
       " 'أتلقف': 153,\n",
       " 'أتم': 154,\n",
       " 'أتمتلك': 155,\n",
       " 'أتمشى': 156,\n",
       " 'أتملك': 157,\n",
       " 'أتممنا': 158,\n",
       " 'أتمنتى': 159,\n",
       " 'أتمنى': 160,\n",
       " 'أتناول': 161,\n",
       " 'أتنسى': 162,\n",
       " 'أتهم': 163,\n",
       " 'أتوا': 164,\n",
       " 'أتواصل': 165,\n",
       " 'أتوتر': 166,\n",
       " 'أتوجد': 167,\n",
       " 'أتود': 168,\n",
       " 'أتوسل': 169,\n",
       " 'أتوقع': 170,\n",
       " 'أتوقف': 171,\n",
       " 'أتولى': 172,\n",
       " 'أتون': 173,\n",
       " 'أتى': 174,\n",
       " 'أتيت': 175,\n",
       " 'أتينا': 176,\n",
       " 'أتی': 177,\n",
       " 'أثاث': 178,\n",
       " 'أثار': 179,\n",
       " 'أثر': 180,\n",
       " 'أثرت': 181,\n",
       " 'أثق': 182,\n",
       " 'أثقل': 183,\n",
       " 'أثلجت': 184,\n",
       " 'أثناء': 185,\n",
       " 'أجاب': 186,\n",
       " 'أجابت': 187,\n",
       " 'أجانب': 188,\n",
       " 'أجب': 189,\n",
       " 'أجبت': 190,\n",
       " 'أجبرت': 191,\n",
       " 'أجبروا': 192,\n",
       " 'أجبن': 193,\n",
       " 'أجتاح': 194,\n",
       " 'أجد': 195,\n",
       " 'أجداد': 196,\n",
       " 'أجدد': 197,\n",
       " 'أجر': 198,\n",
       " 'أجرب': 199,\n",
       " 'أجرح': 200,\n",
       " 'أجريت': 201,\n",
       " 'أجزاء': 202,\n",
       " 'أجعل': 203,\n",
       " 'أجل': 204,\n",
       " 'أجلا': 205,\n",
       " 'أجلب': 206,\n",
       " 'أجلت': 207,\n",
       " 'أجلس': 208,\n",
       " 'أجمع': 209,\n",
       " 'أجمل': 210,\n",
       " 'أجنبية': 211,\n",
       " 'أجني': 212,\n",
       " 'أجهزة': 213,\n",
       " 'أجهشت': 214,\n",
       " 'أجيب': 215,\n",
       " 'أجيد': 216,\n",
       " 'أح': 217,\n",
       " 'أحافظ': 218,\n",
       " 'أحال': 219,\n",
       " 'أحاول': 220,\n",
       " 'أحب': 221,\n",
       " 'أحببت': 222,\n",
       " 'أحبت': 223,\n",
       " 'أحتاج': 224,\n",
       " 'أحترق': 225,\n",
       " 'أحترم': 226,\n",
       " 'أحتمل': 227,\n",
       " 'أحجز': 228,\n",
       " 'أحد': 229,\n",
       " 'أحدا': 230,\n",
       " 'أحدثت': 231,\n",
       " 'أحذ': 232,\n",
       " 'أحذر': 233,\n",
       " 'أحذية': 234,\n",
       " 'أحر': 235,\n",
       " 'أحراج': 236,\n",
       " 'أحرجت': 237,\n",
       " 'أحرز': 238,\n",
       " 'أحزن': 239,\n",
       " 'أحس': 240,\n",
       " 'أحسب': 241,\n",
       " 'أحسد': 242,\n",
       " 'أحسست': 243,\n",
       " 'أحسن': 244,\n",
       " 'أحسنت': 245,\n",
       " 'أحصل': 246,\n",
       " 'أحضر': 247,\n",
       " 'أحضرت': 248,\n",
       " 'أحفظ': 249,\n",
       " 'أحقا': 250,\n",
       " 'أحكام': 251,\n",
       " 'أحكى': 252,\n",
       " 'أحكي': 253,\n",
       " 'أحل': 254,\n",
       " 'أحلام': 255,\n",
       " 'أحلاما': 256,\n",
       " 'أحلم': 257,\n",
       " 'أحلى': 258,\n",
       " 'أحمر': 259,\n",
       " 'أحمرا': 260,\n",
       " 'أحمق': 261,\n",
       " 'أحمقا': 262,\n",
       " 'أحمل': 263,\n",
       " 'أحمي': 264,\n",
       " 'أحوال': 265,\n",
       " 'أحول': 266,\n",
       " 'أحي': 267,\n",
       " 'أحياء': 268,\n",
       " 'أحيان': 269,\n",
       " 'أحيانا': 270,\n",
       " 'أخ': 271,\n",
       " 'أخا': 272,\n",
       " 'أخاف': 273,\n",
       " 'أخافت': 274,\n",
       " 'أخبار': 275,\n",
       " 'أخبر': 276,\n",
       " 'أخبرت': 277,\n",
       " 'أخبرنا': 278,\n",
       " 'أخبروا': 279,\n",
       " 'أخبريني': 280,\n",
       " 'أخبز': 281,\n",
       " 'أخت': 282,\n",
       " 'أختان': 283,\n",
       " 'أختتم': 284,\n",
       " 'أختر': 285,\n",
       " 'أخترع': 286,\n",
       " 'أختفي': 287,\n",
       " 'أختلف': 288,\n",
       " 'أخجل': 289,\n",
       " 'أخدود': 290,\n",
       " 'أخذ': 291,\n",
       " 'أخذت': 292,\n",
       " 'أخذل': 293,\n",
       " 'أخذنا': 294,\n",
       " 'أخرج': 295,\n",
       " 'أخرجت': 296,\n",
       " 'أخرس': 297,\n",
       " 'أخرق': 298,\n",
       " 'أخرني': 299,\n",
       " 'أخرى': 300,\n",
       " 'أخسر': 301,\n",
       " 'أخشى': 302,\n",
       " 'أخضر': 303,\n",
       " 'أخطأ': 304,\n",
       " 'أخطأت': 305,\n",
       " 'أخطاء': 306,\n",
       " 'أخطط': 307,\n",
       " 'أخطيء': 308,\n",
       " 'أخفض': 309,\n",
       " 'أخفف': 310,\n",
       " 'أخفي': 311,\n",
       " 'أخلد': 312,\n",
       " 'أخلط': 313,\n",
       " 'أخلع': 314,\n",
       " 'أخلف': 315,\n",
       " 'أخمص': 316,\n",
       " 'أخمن': 317,\n",
       " 'أخو': 318,\n",
       " 'أخوات': 319,\n",
       " 'أخوض': 320,\n",
       " 'أخون': 321,\n",
       " 'أخيب': 322,\n",
       " 'أخير': 323,\n",
       " 'أخيرا': 324,\n",
       " 'أخيرة': 325,\n",
       " 'أداء': 326,\n",
       " 'أدب': 327,\n",
       " 'أدبا': 328,\n",
       " 'أدخل': 329,\n",
       " 'أدخلت': 330,\n",
       " 'أدخن': 331,\n",
       " 'أدرس': 332,\n",
       " 'أدرك': 333,\n",
       " 'أدركت': 334,\n",
       " 'أدري': 335,\n",
       " 'أدع': 336,\n",
       " 'أدعم': 337,\n",
       " 'أدعى': 338,\n",
       " 'أدفأ': 339,\n",
       " 'أدفع': 340,\n",
       " 'أدلة': 341,\n",
       " 'أدلى': 342,\n",
       " 'أدنی': 343,\n",
       " 'أدو': 344,\n",
       " 'أدوات': 345,\n",
       " 'أدوار': 346,\n",
       " 'أدوية': 347,\n",
       " 'أدير': 348,\n",
       " 'أدين': 349,\n",
       " 'أذاكر': 350,\n",
       " 'أذاهب': 351,\n",
       " 'أذق': 352,\n",
       " 'أذكر': 353,\n",
       " 'أذكى': 354,\n",
       " 'أذهب': 355,\n",
       " 'أذهبت': 356,\n",
       " 'أذى': 357,\n",
       " 'أر': 358,\n",
       " 'أرأى': 359,\n",
       " 'أراد': 360,\n",
       " 'أرادت': 361,\n",
       " 'أراسل': 362,\n",
       " 'أرافق': 363,\n",
       " 'أراقب': 364,\n",
       " 'أرانب': 365,\n",
       " 'أربط': 366,\n",
       " 'أربع': 367,\n",
       " 'أربعا': 368,\n",
       " 'أربعاء': 369,\n",
       " 'أربعة': 370,\n",
       " 'أربعون': 371,\n",
       " 'أربعين': 372,\n",
       " 'أرتاح': 373,\n",
       " 'أرتقي': 374,\n",
       " 'أرتكب': 375,\n",
       " 'أرجح': 376,\n",
       " 'أرجع': 377,\n",
       " 'أرجو': 378,\n",
       " 'أرحل': 379,\n",
       " 'أرخص': 380,\n",
       " 'أرد': 381,\n",
       " 'أردت': 382,\n",
       " 'أردنا': 383,\n",
       " 'أردي': 384,\n",
       " 'أرز': 385,\n",
       " 'أرسل': 386,\n",
       " 'أرسلت': 387,\n",
       " 'أرسم': 388,\n",
       " 'أرض': 389,\n",
       " 'أرضا': 390,\n",
       " 'أرضي': 391,\n",
       " 'أرضية': 392,\n",
       " 'أرغب': 393,\n",
       " 'أرغم': 394,\n",
       " 'أرغمت': 395,\n",
       " 'أرفع': 396,\n",
       " 'أرقص': 397,\n",
       " 'أركب': 398,\n",
       " 'أركز': 399,\n",
       " 'أركض': 400,\n",
       " 'أرم': 401,\n",
       " 'أرمل': 402,\n",
       " 'أرملة': 403,\n",
       " 'أرنب': 404,\n",
       " 'أرني': 405,\n",
       " 'أرهقت': 406,\n",
       " 'أروع': 407,\n",
       " 'أرى': 408,\n",
       " 'أري': 409,\n",
       " 'أرياف': 410,\n",
       " 'أريتني': 411,\n",
       " 'أريد': 412,\n",
       " 'أريكة': 413,\n",
       " 'أريكتي': 414,\n",
       " 'أريني': 415,\n",
       " 'أزاحت': 416,\n",
       " 'أزال': 417,\n",
       " 'أزالت': 418,\n",
       " 'أزرق': 419,\n",
       " 'أزرقا': 420,\n",
       " 'أزعج': 421,\n",
       " 'أزل': 422,\n",
       " 'أزلت': 423,\n",
       " 'أزمات': 424,\n",
       " 'أزمة': 425,\n",
       " 'أزمت': 426,\n",
       " 'أزهار': 427,\n",
       " 'أزور': 428,\n",
       " 'أزياء': 429,\n",
       " 'أس': 430,\n",
       " 'أسأت': 431,\n",
       " 'أسأل': 432,\n",
       " 'أسئل': 433,\n",
       " 'أسئلة': 434,\n",
       " 'أساء': 435,\n",
       " 'أسابيع': 436,\n",
       " 'أساسي': 437,\n",
       " 'أساعد': 438,\n",
       " 'أسافر': 439,\n",
       " 'أساكوسا': 440,\n",
       " 'أسامحك': 441,\n",
       " 'أسامحه': 442,\n",
       " 'أسباب': 443,\n",
       " 'أسبح': 444,\n",
       " 'أسبوع': 445,\n",
       " 'أسبوعا': 446,\n",
       " 'أسبوعيا': 447,\n",
       " 'أسبوعين': 448,\n",
       " 'أستاذ': 449,\n",
       " 'أستاذة': 450,\n",
       " 'أستحق': 451,\n",
       " 'أستحم': 452,\n",
       " 'أستخدم': 453,\n",
       " 'أستدر': 454,\n",
       " 'أستدعي': 455,\n",
       " 'أستذهب': 456,\n",
       " 'أستراليا': 457,\n",
       " 'أستساعدهم': 458,\n",
       " 'أستطع': 459,\n",
       " 'أستطيع': 460,\n",
       " 'أستعد': 461,\n",
       " 'أستعمل': 462,\n",
       " 'أستعوب': 463,\n",
       " 'أستعير': 464,\n",
       " 'أستغرق': 465,\n",
       " 'أستقيل': 466,\n",
       " 'أستلقي': 467,\n",
       " 'أستلم': 468,\n",
       " 'أستمتع': 469,\n",
       " 'أستمطر': 470,\n",
       " 'أستمع': 471,\n",
       " 'أستيقظ': 472,\n",
       " 'أسحب': 473,\n",
       " 'أسد': 474,\n",
       " 'أسدد': 475,\n",
       " 'أسرة': 476,\n",
       " 'أسرتي': 477,\n",
       " 'أسرع': 478,\n",
       " 'أسرعا': 479,\n",
       " 'أسرق': 480,\n",
       " 'أسطوانة': 481,\n",
       " 'أسطورة': 482,\n",
       " 'أسطوري': 483,\n",
       " 'أسعار': 484,\n",
       " 'أسعد': 485,\n",
       " 'أسعدت': 486,\n",
       " 'أسعدنياجعلني': 487,\n",
       " 'أسف': 488,\n",
       " 'أسفل': 489,\n",
       " 'أسق': 490,\n",
       " 'أسقطت': 491,\n",
       " 'أسكت': 492,\n",
       " 'أسكتنا': 493,\n",
       " 'أسكن': 494,\n",
       " 'أسلب': 495,\n",
       " 'أسلحة': 496,\n",
       " 'أسماء': 497,\n",
       " 'أسماك': 498,\n",
       " 'أسمح': 499,\n",
       " 'أسمع': 500,\n",
       " 'أسمعت': 501,\n",
       " 'أسنان': 502,\n",
       " 'أسهم': 503,\n",
       " 'أسوأ': 504,\n",
       " 'أسود': 505,\n",
       " 'أسودا': 506,\n",
       " 'أسورة': 507,\n",
       " 'أشأ': 508,\n",
       " 'أشار': 509,\n",
       " 'أشارك': 510,\n",
       " 'أشاهد': 511,\n",
       " 'أشباح': 512,\n",
       " 'أشتاق': 513,\n",
       " 'أشتري': 514,\n",
       " 'أشتكي': 515,\n",
       " 'أشتية': 516,\n",
       " 'أشجار': 517,\n",
       " 'أشخاص': 518,\n",
       " 'أشرب': 519,\n",
       " 'أشرت': 520,\n",
       " 'أشرح': 521,\n",
       " 'أشرق': 522,\n",
       " 'أشعار': 523,\n",
       " 'أشعارا': 524,\n",
       " 'أشعة': 525,\n",
       " 'أشعر': 526,\n",
       " 'أشعل': 527,\n",
       " 'أشقاء': 528,\n",
       " 'أشك': 529,\n",
       " 'أشكال': 530,\n",
       " 'أشكر': 531,\n",
       " 'أشلاء': 532,\n",
       " 'أشم': 533,\n",
       " 'أشهر': 534,\n",
       " 'أشياء': 535,\n",
       " 'أصاب': 536,\n",
       " 'أصبت': 537,\n",
       " 'أصبح': 538,\n",
       " 'أصبحا': 539,\n",
       " 'أصبحت': 540,\n",
       " 'أصبحنا': 541,\n",
       " 'أصبحوا': 542,\n",
       " 'أصحاب': 543,\n",
       " 'أصحو': 544,\n",
       " 'أصحيح': 545,\n",
       " 'أصدر': 546,\n",
       " 'أصدق': 547,\n",
       " 'أصدقاء': 548,\n",
       " 'أصر': 549,\n",
       " 'أصطاد': 550,\n",
       " 'أصطحب': 551,\n",
       " 'أصعب': 552,\n",
       " 'أصغر': 553,\n",
       " 'أصغيتم': 554,\n",
       " 'أصفر': 555,\n",
       " 'أصل': 556,\n",
       " 'أصلا': 557,\n",
       " 'أصلح': 558,\n",
       " 'أصلحت': 559,\n",
       " 'أصلع': 560,\n",
       " 'أصليين': 561,\n",
       " 'أصم': 562,\n",
       " 'أصمت': 563,\n",
       " 'أصنع': 564,\n",
       " 'أصوات': 565,\n",
       " 'أصوت': 566,\n",
       " 'أصوليين': 567,\n",
       " 'أصيب': 568,\n",
       " 'أصيبت': 569,\n",
       " 'أصيبوا': 570,\n",
       " 'أصير': 571,\n",
       " 'أضحت': 572,\n",
       " 'أضحك': 573,\n",
       " 'أضرب': 574,\n",
       " 'أضطر': 575,\n",
       " 'أضع': 576,\n",
       " 'أضعاف': 577,\n",
       " 'أضعت': 578,\n",
       " 'أضغط': 579,\n",
       " 'أضف': 580,\n",
       " 'أضفت': 581,\n",
       " 'أضمن': 582,\n",
       " 'أضن': 583,\n",
       " 'أضواء': 584,\n",
       " 'أضيع': 585,\n",
       " 'أضيف': 586,\n",
       " 'أطباء': 587,\n",
       " 'أطباق': 588,\n",
       " 'أطبخ': 589,\n",
       " 'أطرح': 590,\n",
       " 'أطع': 591,\n",
       " 'أطعم': 592,\n",
       " 'أطعمة': 593,\n",
       " 'أطعمت': 594,\n",
       " 'أطفأ': 595,\n",
       " 'أطفأت': 596,\n",
       " 'أطفئ': 597,\n",
       " 'أطفال': 598,\n",
       " 'أطفالا': 599,\n",
       " 'أطلب': 600,\n",
       " 'أطلق': 601,\n",
       " 'أطلقت': 602,\n",
       " 'أطمئن': 603,\n",
       " 'أطهو': 604,\n",
       " 'أطول': 605,\n",
       " 'أطيب': 606,\n",
       " 'أطيق': 607,\n",
       " 'أظل': 608,\n",
       " 'أظن': 609,\n",
       " 'أظنن': 610,\n",
       " 'أظنني': 611,\n",
       " 'أظهر': 612,\n",
       " 'أظهرت': 613,\n",
       " 'أظهروا': 614,\n",
       " 'أعاد': 615,\n",
       " 'أعار': 616,\n",
       " 'أعارض': 617,\n",
       " 'أعاود': 618,\n",
       " 'أعتبر': 619,\n",
       " 'أعتذر': 620,\n",
       " 'أعترف': 621,\n",
       " 'أعتقد': 622,\n",
       " 'أعتمد': 623,\n",
       " 'أعتن': 624,\n",
       " 'أعتني': 625,\n",
       " 'أعثر': 626,\n",
       " 'أعجبت': 627,\n",
       " 'أعجوبة': 628,\n",
       " 'أعد': 629,\n",
       " 'أعداء': 630,\n",
       " 'أعداد': 631,\n",
       " 'أعدت': 632,\n",
       " 'أعددت': 633,\n",
       " 'أعدو': 634,\n",
       " 'أعذر': 635,\n",
       " 'أعر': 636,\n",
       " 'أعرت': 637,\n",
       " 'أعرف': 638,\n",
       " 'أعرني': 639,\n",
       " 'أعز': 640,\n",
       " 'أعزب': 641,\n",
       " 'أعزف': 642,\n",
       " 'أعشاب': 643,\n",
       " 'أعشق': 644,\n",
       " 'أعصاب': 645,\n",
       " 'أعضاء': 646,\n",
       " 'أعط': 647,\n",
       " 'أعطانيه': 648,\n",
       " 'أعطت': 649,\n",
       " 'أعطس': 650,\n",
       " 'أعطني': 651,\n",
       " 'أعطه': 652,\n",
       " 'أعطوا': 653,\n",
       " 'أعطى': 654,\n",
       " 'أعطي': 655,\n",
       " 'أعطيت': 656,\n",
       " 'أعظم': 657,\n",
       " 'أعلام': 658,\n",
       " 'أعلق': 659,\n",
       " 'أعلم': 660,\n",
       " 'أعلميني': 661,\n",
       " 'أعلن': 662,\n",
       " 'أعلنت': 663,\n",
       " 'أعلى': 664,\n",
       " 'أعماق': 665,\n",
       " 'أعمال': 666,\n",
       " 'أعمامي': 667,\n",
       " 'أعمق': 668,\n",
       " 'أعمل': 669,\n",
       " 'أعمى': 670,\n",
       " 'أعندك': 671,\n",
       " 'أعني': 672,\n",
       " 'أعوام': 673,\n",
       " 'أعواما': 674,\n",
       " 'أعود': 675,\n",
       " 'أعور': 676,\n",
       " 'أعي': 677,\n",
       " 'أعياد': 678,\n",
       " 'أعيد': 679,\n",
       " 'أعير': 680,\n",
       " 'أعيش': 681,\n",
       " 'أعين': 682,\n",
       " 'أغادر': 683,\n",
       " 'أغبى': 684,\n",
       " 'أغراض': 685,\n",
       " 'أغرب': 686,\n",
       " 'أغرق': 687,\n",
       " 'أغسل': 688,\n",
       " 'أغضب': 689,\n",
       " 'أغضبت': 690,\n",
       " 'أغلب': 691,\n",
       " 'أغلبي': 692,\n",
       " 'أغلق': 693,\n",
       " 'أغلقت': 694,\n",
       " 'أغلى': 695,\n",
       " 'أغني': 696,\n",
       " 'أغنية': 697,\n",
       " 'أغيب': 698,\n",
       " 'أغير': 699,\n",
       " 'أفاجئ': 700,\n",
       " 'أفادت': 701,\n",
       " 'أفاعي': 702,\n",
       " 'أفتح': 703,\n",
       " 'أفترض': 704,\n",
       " 'أفتقد': 705,\n",
       " 'أفحص': 706,\n",
       " 'أفراد': 707,\n",
       " 'أفرش': 708,\n",
       " 'أفسح': 709,\n",
       " 'أفسد': 710,\n",
       " 'أفضل': 711,\n",
       " 'أفطرت': 712,\n",
       " 'أفعال': 713,\n",
       " 'أفعل': 714,\n",
       " 'أفكار': 715,\n",
       " 'أفكر': 716,\n",
       " 'أفكرت': 717,\n",
       " 'أفلام': 718,\n",
       " 'أفهم': 719,\n",
       " 'أفواه': 720,\n",
       " 'أفوز': 721,\n",
       " 'أفون': 722,\n",
       " 'أقابل': 723,\n",
       " 'أقارب': 724,\n",
       " 'أقاضي': 725,\n",
       " 'أقاطع': 726,\n",
       " 'أقام': 727,\n",
       " 'أقبل': 728,\n",
       " 'أقترب': 729,\n",
       " 'أقترح': 730,\n",
       " 'أقترض': 731,\n",
       " 'أقترف': 732,\n",
       " 'أقتل': 733,\n",
       " 'أقدام': 734,\n",
       " 'أقدر': 735,\n",
       " 'أقدم': 736,\n",
       " 'أقرأ': 737,\n",
       " 'أقرا': 738,\n",
       " 'أقراص': 739,\n",
       " 'أقرب': 740,\n",
       " 'أقربون': 741,\n",
       " 'أقرر': 742,\n",
       " 'أقرض': 743,\n",
       " 'أقرضت': 744,\n",
       " 'أقساط': 745,\n",
       " 'أقسم': 746,\n",
       " 'أقشر': 747,\n",
       " 'أقصد': 748,\n",
       " 'أقصر': 749,\n",
       " 'أقصوصة': 750,\n",
       " 'أقصى': 751,\n",
       " 'أقضي': 752,\n",
       " 'أقطع': 753,\n",
       " 'أقعد': 754,\n",
       " 'أقف': 755,\n",
       " 'أقفز': 756,\n",
       " 'أقفل': 757,\n",
       " 'أقفلت': 758,\n",
       " 'أقل': 759,\n",
       " 'أقلام': 760,\n",
       " 'أقلب': 761,\n",
       " 'أقلعت': 762,\n",
       " 'أقم': 763,\n",
       " 'أقمت': 764,\n",
       " 'أقنع': 765,\n",
       " 'أقنعت': 766,\n",
       " 'أقو': 767,\n",
       " 'أقوال': 768,\n",
       " 'أقود': 769,\n",
       " 'أقول': 770,\n",
       " 'أقوم': 771,\n",
       " 'أقوى': 772,\n",
       " 'أقوياء': 773,\n",
       " 'أقيس': 774,\n",
       " 'أكاد': 775,\n",
       " 'أكبر': 776,\n",
       " 'أكتاف': 777,\n",
       " 'أكتب': 778,\n",
       " 'أكتبت': 779,\n",
       " 'أكتوبر': 780,\n",
       " 'أكثر': 781,\n",
       " 'أكره': 782,\n",
       " 'أكسجين': 783,\n",
       " 'أكل': 784,\n",
       " 'أكلت': 785,\n",
       " 'أكلم': 786,\n",
       " 'أكلنا': 787,\n",
       " 'أكلوا': 788,\n",
       " 'أكمل': 789,\n",
       " 'أكملت': 790,\n",
       " 'أكن': 791,\n",
       " 'أكنت': 792,\n",
       " 'أكو': 793,\n",
       " 'أكون': 794,\n",
       " 'أكيد': 795,\n",
       " 'أكيدة': 796,\n",
       " 'أكيليز': 797,\n",
       " 'أل': 798,\n",
       " 'ألبرت': 799,\n",
       " 'ألبس': 800,\n",
       " 'ألتقط': 801,\n",
       " 'ألتمس': 802,\n",
       " 'ألحق': 803,\n",
       " 'ألحقت': 804,\n",
       " 'ألدي': 805,\n",
       " 'ألزمت': 806,\n",
       " 'ألعاب': 807,\n",
       " 'ألعب': 808,\n",
       " 'ألغ': 809,\n",
       " 'ألغوا': 810,\n",
       " 'ألغيت': 811,\n",
       " 'ألف': 812,\n",
       " 'ألفت': 813,\n",
       " 'ألفي': 814,\n",
       " 'ألفين': 815,\n",
       " 'ألق': 816,\n",
       " 'ألقت': 817,\n",
       " 'ألقوا': 818,\n",
       " 'ألقى': 819,\n",
       " 'ألقي': 820,\n",
       " 'أللقصه': 821,\n",
       " 'ألم': 822,\n",
       " 'ألما': 823,\n",
       " 'ألماس': 824,\n",
       " 'ألمانيا': 825,\n",
       " 'ألمانية': 826,\n",
       " 'ألنا': 827,\n",
       " 'ألوان': 828,\n",
       " 'ألوب': 829,\n",
       " 'ألوت': 830,\n",
       " 'ألوم': 831,\n",
       " 'ألياف': 832,\n",
       " 'أليافا': 833,\n",
       " 'أليست': 834,\n",
       " 'أليفا': 835,\n",
       " 'أليفة': 836,\n",
       " 'أم': 837,\n",
       " 'أما': 838,\n",
       " 'أمازالوا': 839,\n",
       " 'أماكن': 840,\n",
       " 'أمام': 841,\n",
       " 'أمامي': 842,\n",
       " 'أمامية': 843,\n",
       " 'أمان': 844,\n",
       " 'أمانع': 845,\n",
       " 'أماه': 846,\n",
       " 'أمتأكد': 847,\n",
       " 'أمتا': 848,\n",
       " 'أمتار': 849,\n",
       " 'أمتلك': 850,\n",
       " 'أمثال': 851,\n",
       " 'أمدرس': 852,\n",
       " 'أمر': 853,\n",
       " 'أمرا': 854,\n",
       " 'أمرت': 855,\n",
       " 'أمرض': 856,\n",
       " 'أمريكا': 857,\n",
       " 'أمريكي': 858,\n",
       " 'أمريكية': 859,\n",
       " 'أمريكيون': 860,\n",
       " 'أمريكيين': 861,\n",
       " 'أمزح': 862,\n",
       " 'أمس': 863,\n",
       " 'أمسكت': 864,\n",
       " 'أمسية': 865,\n",
       " 'أمش': 866,\n",
       " 'أمشط': 867,\n",
       " 'أمشى': 868,\n",
       " 'أمض': 869,\n",
       " 'أمضت': 870,\n",
       " 'أمضغ': 871,\n",
       " 'أمضوا': 872,\n",
       " 'أمضي': 873,\n",
       " 'أمضيت': 874,\n",
       " 'أمضينا': 875,\n",
       " 'أمطار': 876,\n",
       " 'أمطرت': 877,\n",
       " 'أمكن': 878,\n",
       " 'أمكننا': 879,\n",
       " 'أمكنني': 880,\n",
       " 'أمل': 881,\n",
       " 'أملئ': 882,\n",
       " 'أملا': 883,\n",
       " 'أملت': 884,\n",
       " 'أملك': 885,\n",
       " 'أملى': 886,\n",
       " 'أمهات': 887,\n",
       " 'أمواج': 888,\n",
       " 'أموال': 889,\n",
       " 'أموت': 890,\n",
       " 'أمور': 891,\n",
       " 'أميال': 892,\n",
       " 'أميرة': 893,\n",
       " 'أميرتي': 894,\n",
       " 'أميركي': 895,\n",
       " 'أمين': 896,\n",
       " 'أمينا': 897,\n",
       " 'أمينة': 898,\n",
       " 'أن': 899,\n",
       " 'أنا': 900,\n",
       " 'أنابيب': 901,\n",
       " 'أناس': 902,\n",
       " 'أناقش': 903,\n",
       " 'أنام': 904,\n",
       " 'أناناس': 905,\n",
       " 'أنانيين': 906,\n",
       " 'أنت': 907,\n",
       " 'أنتبه': 908,\n",
       " 'أنتحر': 909,\n",
       " 'أنتظر': 910,\n",
       " 'أنتقل': 911,\n",
       " 'أنتم': 912,\n",
       " 'أنته': 913,\n",
       " 'أنتهي': 914,\n",
       " 'أنجح': 915,\n",
       " 'أنجز': 916,\n",
       " 'أنجزت': 917,\n",
       " 'أنجلترا': 918,\n",
       " 'أنحاء': 919,\n",
       " 'أنحني': 920,\n",
       " 'أنذر': 921,\n",
       " 'أنذهب': 922,\n",
       " 'أنذهل': 923,\n",
       " 'أنزع': 924,\n",
       " 'أنزل': 925,\n",
       " 'أنس': 926,\n",
       " 'أنسجم': 927,\n",
       " 'أنسخ': 928,\n",
       " 'أنسى': 929,\n",
       " 'أنشئت': 930,\n",
       " 'أنصت': 931,\n",
       " 'أنصح': 932,\n",
       " 'أنضم': 933,\n",
       " 'أنظر': 934,\n",
       " 'أنظف': 935,\n",
       " 'أنف': 936,\n",
       " 'أنفاس': 937,\n",
       " 'أنفاق': 938,\n",
       " 'أنفس': 939,\n",
       " 'أنفق': 940,\n",
       " 'أنفي': 941,\n",
       " 'أنقذ': 942,\n",
       " 'أنقر': 943,\n",
       " 'أنقلب': 944,\n",
       " 'أنكر': 945,\n",
       " 'أنكسر': 946,\n",
       " 'أنم': 947,\n",
       " 'أنهار': 948,\n",
       " 'أنهت': 949,\n",
       " 'أنهض': 950,\n",
       " 'أنهى': 951,\n",
       " 'أنهي': 952,\n",
       " 'أنهيت': 953,\n",
       " 'أنهينا': 954,\n",
       " 'أنوار': 955,\n",
       " 'أنواع': 956,\n",
       " 'أنوف': 957,\n",
       " 'أنوي': 958,\n",
       " 'أنيقة': 959,\n",
       " 'أنی': 960,\n",
       " 'أهتم': 961,\n",
       " 'أهدأ': 962,\n",
       " 'أهداف': 963,\n",
       " 'أهدت': 964,\n",
       " 'أهذ': 965,\n",
       " 'أهذا': 966,\n",
       " 'أهرب': 967,\n",
       " 'أهزم': 968,\n",
       " 'أهل': 969,\n",
       " 'أهلا': 970,\n",
       " 'أهم': 971,\n",
       " 'أهمية': 972,\n",
       " 'أهنئ': 973,\n",
       " 'أهنا': 974,\n",
       " 'أهو': 975,\n",
       " 'أهي': 976,\n",
       " 'أو': 977,\n",
       " 'أواجه': 978,\n",
       " 'أوافق': 979,\n",
       " 'أوافي': 980,\n",
       " 'أوامر': 981,\n",
       " 'أوان': 982,\n",
       " 'أوبخ': 983,\n",
       " 'أوبرا': 984,\n",
       " 'أوتان': 985,\n",
       " 'أوتظنني': 986,\n",
       " 'أوتوماتيكي': 987,\n",
       " 'أوج': 988,\n",
       " 'أوجدت': 989,\n",
       " 'أود': 990,\n",
       " 'أوذيك': 991,\n",
       " 'أوراق': 992,\n",
       " 'أوروبا': 993,\n",
       " 'أوروبيين': 994,\n",
       " 'أوساكا': 995,\n",
       " 'أوشكت': 996,\n",
       " 'أوصل': 997,\n",
       " 'أوصلت': 998,\n",
       " 'أوقات': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_token_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will put the max_encoder_seq for both decoder and encoder with 64 since the max number of tokens in Arabic is 52 and in English is 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_encoder_seq_length = 64\n",
    "max_decoder_seq_length = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Arabic_input</th>\n",
       "      <th>English_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>مرحبا</td>\n",
       "      <td>START_ hi _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>اركض</td>\n",
       "      <td>START_ run _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>اخفض رأسك</td>\n",
       "      <td>START_ duck _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>اخفضي رأسك</td>\n",
       "      <td>START_ duck _END</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>اخفضوا رؤوسكم</td>\n",
       "      <td>START_ duck _END</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Arabic_input    English_target\n",
       "0          مرحبا    START_ hi _END\n",
       "1           اركض   START_ run _END\n",
       "2      اخفض رأسك  START_ duck _END\n",
       "3     اخفضي رأسك  START_ duck _END\n",
       "4  اخفضوا رؤوسكم  START_ duck _END"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_input_target(dataset, max_encoder_seq_length, max_decoder_seq_length, num_decoder_tokens):\n",
    "    # The input setence to the encoder is 64 leghth.\n",
    "    encoder_input_data = np.zeros( (len(dataset.Arabic_input), max_encoder_seq_length),dtype='float32')\n",
    "    # The input setence to the decoder is 64 leghth.\n",
    "    decoder_input_data = np.zeros((len(dataset.English_target), max_decoder_seq_length), dtype='float32')\n",
    "    # The output setence of the decoder is 64 x all_tokens_inside_encoder. Since the decoder will do softmax for each token.\n",
    "    decoder_target_data = np.zeros((len(dataset.English_target), max_decoder_seq_length, num_decoder_tokens), dtype='float32')\n",
    "    \n",
    "    return encoder_input_data, decoder_input_data, decoder_target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data, decoder_input_data, decoder_target_data = init_input_target(dataset, max_encoder_seq_length, max_decoder_seq_length, num_decoder_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(tokenized_ds_copy, max_encoder_seq_length, max_decoder_seq_length, num_decoder_tokens):\n",
    "\n",
    "    for i, (input_text_ls, target_text_ls) in enumerate(zip(tokenized_ds_copy.Arabic_input, tokenized_ds_copy.English_target)):\n",
    "        # preparing the encoder inputs\n",
    "        for t, tok in enumerate(input_text_ls):\n",
    "            #To obtain the ids of encoder sentence's tokens from input_token_index\n",
    "            encoder_input_data[i, t] = input_token_index[tok]\n",
    "            \n",
    "        encoder_input_data[i, t+1:] = input_token_index[pad_tok]\n",
    "        \n",
    "        # This loop is used to prepare the input and output of the decoder\n",
    "        for t, tok in enumerate(target_text_ls):\n",
    "            #1- prepare the decoder input\n",
    "            #To obtain the ids of decoder sentence's tokens from target_token_index\n",
    "            decoder_input_data[i, t] = target_token_index[tok]    \n",
    "            \n",
    "            # To obtain the decoder output\n",
    "            if t > 0:\n",
    "                # decoder_target_data will be ahead by one timestep\n",
    "                # and will not include the start character.\n",
    "                #We put 1 in the place of the expected word\n",
    "                decoder_target_data[i, t - 1, target_token_index[tok]] = 1.\n",
    "        decoder_input_data[i, t+1:] = target_token_index[pad_tok] \n",
    "        decoder_target_data[i, t:, target_token_index[pad_tok]] = 1.          \n",
    "              \n",
    "    return encoder_input_data, decoder_input_data, decoder_target_data              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data, decoder_input_data, decoder_target_data = vectorize(tokenized_ds_copy, max_encoder_seq_length, max_decoder_seq_length, num_decoder_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5490.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5490"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_token_index['مرحبا']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \n",
    "\n",
    "The 1st sentence in the Arabic_input column is 'مرحبا', and there is no any other word inside the sentence. If we printed the 1st place in encoder_input_data, we will find that there is only one word, this word took the same number of 'مرحبا "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   5., 1802.,    6.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "          0.], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1802"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_token_index['hi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_token_index['START_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_token_index['_END']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "    The 1st sentence in the decoder input is START_ hi _END, so there are only three tokens, each token has id. If we check the tokens ids inside the target_token_index, then we find that the decoder_input_data has the similar number in the location 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The _END location\n",
    "decoder_target_data[0][1][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The hi location\n",
    "decoder_target_data[0][0][1800:1810]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "what should appear in the output of the decoder in this case is 'hi _END', we will find hi on gate 0 in the 3rd location, and _END on gate  1 n the 6th location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mdeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will implement the encoder separated from decoder to simplify the operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_training_encoder(num_encoder_tokens, emb_sz, lstm_sz, mask_zero):\n",
    "    # 1- Define the input to the encoder\n",
    "    encoder_inputs = Input(shape=(None,), dtype='float32')\n",
    "    \n",
    "    # 2- Define the embedding layer. This layer is built on the previous layer the input to the encoder.\n",
    "    # This embedding layer need the following parameters 1- all the expected encoder tokens number. 2- embedding size of each word\n",
    "    # The vector of each word output from embedding layer has this word charcteristics.\n",
    "    en_x=  Embedding(num_encoder_tokens, emb_sz,mask_zero=mask_zero)(encoder_inputs)\n",
    "    \n",
    "    # 3- Define the 1st LSTM layer, \n",
    "    #This layer parametrs are:\n",
    "    # 1- reurn_satate which enable us to output the cell state and the hidden state\n",
    "    # 2- lstm_sz: which is the number of hidden units inside the LSTM layer.\n",
    "    encoder = Bidirectional(LSTM(lstm_sz, return_state=True, return_sequences=True))\n",
    "    \n",
    "    # 4- Put the encoder LSTM on the embedding layer output \n",
    "    #and take the output and the hidden state of this Bi-LSTM which will be used to build the context vector\n",
    "    encoder_outputs, state_f_h, state_f_c, state_b_h, state_b_c = encoder(en_x)\n",
    "    \n",
    "    # 5- Concatenate the output state from the forward and the backward together\n",
    "    state_h = Concatenate()([state_f_h, state_b_h])# Bi LSTM\n",
    "    # Concatenate the hidden state from the forward and the backward together\n",
    "    state_c = Concatenate()([state_f_c, state_b_c])# Bi LSTM\n",
    "\n",
    "    \n",
    "    # We discard `encoder_outputs` and only keep the states.\n",
    "    encoder_states = [state_h, state_c]\n",
    "  \n",
    "    # Inputs to the encoder model are tokens, and the output of the ecoder are the the output of the encoder and encoder states. \n",
    "    # In attention model we use encoder output and the encoder hidden states not only the states\n",
    "    encoder_model = Model(encoder_inputs, [encoder_outputs] + encoder_states)\n",
    "    print('\\n The encoder model \\n')\n",
    "    encoder_model.summary()\n",
    "          \n",
    "    return encoder_model, encoder_states, encoder_inputs, encoder_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "    \n",
    "def build_training_decoder(num_decoder_tokens, emb_sz, lstm_sz, encoder_states, encoder_inputs, mask_zero, encoder_outputs):\n",
    "        \n",
    "    #1- define the input layer to the decoder.\n",
    "    decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "    # 2- define the embeddding layer for the decoder/ In training it will take the expected tokens from the dataset\n",
    "    # The embedding layer parameters are:\n",
    "    # 1- All the expected tokens to the decoder.\n",
    "    # 2- The embedding size\n",
    "    decoder_embedding=  Embedding(num_decoder_tokens, emb_sz,mask_zero=mask_zero)\n",
    "\n",
    "    # 3- Put the layer of embedding on the layer of decoder inputs. \n",
    "    embedding_output= decoder_embedding(decoder_inputs)\n",
    "\n",
    "    \n",
    "    # 4- Define the LSTM which should output the hidden state and cell state and cell output for each cell.\n",
    "    # the hidden state and cell state which can be done enabling return_state\n",
    "    # The output sequence of all the input sequence tokens can be done by enabling the return_sequences \n",
    "    decoder_lstm = LSTM(lstm_sz*2, return_sequences=True, return_state=True)\n",
    "\n",
    "    \n",
    "    # 5- Put the LSTM on the top of embedding layer and feed the context vector to the 'encoder_states'\n",
    "    decoder_outputs, _, _ = decoder_lstm(embedding_output, initial_state=encoder_states)\n",
    "    \n",
    "    # 6- Define the dot product operation\n",
    "    att_dot = Dot(axes=[2, 2])\n",
    "    \n",
    "    # 7- Obtaining the attention score obtained from dot product of encoder_outputs and decoder_outputs, \n",
    "    # Note this is a Loung mechanism which uses the current decoder output hidden state\n",
    "    attention = att_dot([decoder_outputs, encoder_outputs])\n",
    "    \n",
    "    # 8- Define the Activation function\n",
    "    att_activation = Activation('softmax', name='attention')\n",
    "    \n",
    "    # 9- Step 3: Normalizing the attention scores\n",
    "    attention = att_activation(attention)\n",
    "    print('attention', attention)\n",
    "    \n",
    "    # 10- Defin the dot product for Computing the context vector\n",
    "    context_dot = Dot(axes=[2,1])\n",
    "    \n",
    "    # 11- Step 4: Computing the context vector by multiplying the normalization output and encoder outputs.\n",
    "    context = context_dot([attention, encoder_outputs])\n",
    "    \n",
    "    # 12- Define the Concatenattion layer for the step 5\n",
    "    att_context_concat = Concatenate()\n",
    "    \n",
    "    # 13- Step 5: concatenating the context and the decoder_outputs at the current state.\n",
    "    decoder_combined_context = att_context_concat([context, decoder_outputs])\n",
    "\n",
    "    # 14- Define the fully connected layer which predicts the output token.\n",
    "    # This layer will output a vector. This vector has a probability for each expected token to output. \n",
    "    #Then this layer feeds this to an activation function 'softmax' to decide which word sould output at each timestep.\n",
    "    decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "\n",
    "    # 15- Put the dense/fully connected layer on the top of lstm \n",
    "    #and feed the lstm output vector for all input tokens to the dense.\n",
    "    decoder_outputs = decoder_dense(decoder_combined_context)\n",
    "    \n",
    "    # 16- Here build the combined model which takes the training tokens to the encoder and decoder and output the decoder output. \n",
    "    # This decoder output comes after the softmax of the dense layer\n",
    "    combined_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    \n",
    "    print('\\n The combined model \\n')\n",
    "    combined_model.summary()\n",
    "    \n",
    "    return combined_model, decoder_inputs, embedding_output, decoder_lstm, decoder_dense, att_dot, att_activation, context_dot, att_context_concat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will use the same architecture of decoder in the training pahse, so we will use the followiing\n",
    "# 1- decoder_input.\n",
    "# 2- decoder_embedding_output. \n",
    "# 3- decoder_lstm\n",
    "#4- decoder_dense\n",
    "#why will we use the same architecture? since we will use the same cells but by using the inference methodology which is mentioned above.\n",
    "def build_inference_decoder(num_decoder_tokens, lstm_sz, emb_sz, embedding_output, decoder_inputs, decoder_lstm,\n",
    "                            decoder_dense, att_dot, att_activation, context_dot, att_context_concat):\n",
    "    \n",
    "    # Decoder model: Re-build based on explicit state inputs. Needed for step-by-step inference:\n",
    "    # The encoder outputs which will be considered in attention calculation\n",
    "    encoder_outputs = Input(shape=(None, lstm_sz*2,))\n",
    "    \n",
    "    # define the hidden state of the context vector which will come from the encoder in the prediction\n",
    "    decoder_state_input_h = Input(shape=(lstm_sz*2,))# Bi LSTM\n",
    "    # define the cell state of the context vector which will come from the encoder in the prediction\n",
    "    decoder_state_input_c = Input(shape=(lstm_sz*2,))# Bi LSTM\n",
    "\n",
    "    #define the hidden states which will feed into the decoder to initialize it. The values of this will be feed in the prediction.\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    # feed the decoder LSTM with embedding output, and inilize its state to decoder_states_inputs \n",
    "    #which will be fed after that with the encoder context vector.\n",
    "    decoder_outputs2, state_h2, state_c2 = decoder_lstm(embedding_output, initial_state=decoder_states_inputs)\n",
    "    \n",
    "    # Define the input to the dense layer\n",
    "    decoder_states2 = [state_h2, state_c2]\n",
    "    \n",
    "    # step2 : Attention score function with 'dot' score from Section 3.1 in the paper.\n",
    "    attention = att_dot([decoder_outputs2, encoder_outputs])\n",
    "    \n",
    "    # Step3: Normalizing the attention scores. \n",
    "    # Note that we reuse Softmax-activation layer instead of writing tensor calculation\n",
    "    attention = att_activation(attention)\n",
    "    \n",
    "    # step4: Calculating the context vector \n",
    "    context = context_dot([attention, encoder_outputs])\n",
    "    \n",
    "    # step5: Calculating an attention hidden state\n",
    "    decoder_combined_context = att_context_concat([context, decoder_outputs2])\n",
    "\n",
    "    # Feed the hidden and cell state to the dense layer which will predict the output.\n",
    "    decoder_outputs2 = decoder_dense(decoder_combined_context)\n",
    "    ##define the decoder_model which will take take the decoder inputs and ecoder outputs to make attention calculation\n",
    "    # and initial state for the decoder.\n",
    "    # this model will output the decoder output token with attention in addition to the hidden and cell states.\n",
    "    decoder_model = Model([decoder_inputs, encoder_outputs] + decoder_states_inputs,\n",
    "                          [decoder_outputs2, attention] + decoder_states2) \n",
    "    \n",
    "    return decoder_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_sz = 50\n",
    "lstm_sz = 256\n",
    " \n",
    "def model_seq_to_seq(batch_size, epochs,mask_zero):\n",
    "    \n",
    "    encoder_model, encoder_states, encoder_inputs, encoder_outputs = build_training_encoder(num_encoder_tokens, emb_sz, lstm_sz,mask_zero)\n",
    "    combined_model, decoder_inputs, embedding_output, decoder_lstm, decoder_dense, att_dot, att_activation, context_dot, att_context_concat = build_training_decoder(num_decoder_tokens, emb_sz,\n",
    "                                                                                                                                                                     lstm_sz, encoder_states, encoder_inputs,\n",
    "                                                                                                                                                                     mask_zero, encoder_outputs)\n",
    "    \n",
    "    decoder_model = build_inference_decoder(num_decoder_tokens, lstm_sz, emb_sz, embedding_output, decoder_inputs,\n",
    "                                            decoder_lstm, decoder_dense, att_dot, att_activation, context_dot, att_context_concat)\n",
    "\n",
    "    \n",
    "    # 8- compile the combined model in training phase\n",
    "    combined_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "    combined_model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "    \n",
    "    return combined_model, encoder_model, decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The encoder model \n",
      "\n",
      "Model: \"model_22\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_47 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_25 (Embedding)        (None, None, 50)     360450      input_47[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_14 (Bidirectional [(None, None, 512),  628736      embedding_25[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_36 (Concatenate)    (None, 512)          0           bidirectional_14[0][1]           \n",
      "                                                                 bidirectional_14[0][3]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_37 (Concatenate)    (None, 512)          0           bidirectional_14[0][2]           \n",
      "                                                                 bidirectional_14[0][4]           \n",
      "==================================================================================================\n",
      "Total params: 989,186\n",
      "Trainable params: 989,186\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "attention KerasTensor(type_spec=TensorSpec(shape=(None, None, None), dtype=tf.float32, name=None), name='attention/Softmax:0', description=\"created by layer 'attention'\")\n",
      "\n",
      " The combined model \n",
      "\n",
      "Model: \"model_23\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_47 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_25 (Embedding)        (None, None, 50)     360450      input_47[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_48 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_14 (Bidirectional [(None, None, 512),  628736      embedding_25[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "embedding_26 (Embedding)        (None, None, 50)     215100      input_48[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_36 (Concatenate)    (None, 512)          0           bidirectional_14[0][1]           \n",
      "                                                                 bidirectional_14[0][3]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_37 (Concatenate)    (None, 512)          0           bidirectional_14[0][2]           \n",
      "                                                                 bidirectional_14[0][4]           \n",
      "__________________________________________________________________________________________________\n",
      "lstm_24 (LSTM)                  [(None, None, 512),  1153024     embedding_26[0][0]               \n",
      "                                                                 concatenate_36[0][0]             \n",
      "                                                                 concatenate_37[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dot_17 (Dot)                    (None, None, None)   0           lstm_24[0][0]                    \n",
      "                                                                 bidirectional_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "attention (Activation)          (None, None, None)   0           dot_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dot_18 (Dot)                    (None, None, 512)    0           attention[0][0]                  \n",
      "                                                                 bidirectional_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_38 (Concatenate)    (None, None, 1024)   0           dot_18[0][0]                     \n",
      "                                                                 lstm_24[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, None, 4302)   4409550     concatenate_38[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 6,766,860\n",
      "Trainable params: 6,766,860\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "152/152 [==============================] - 552s 3s/step - loss: 0.5773 - acc: 0.2267 - val_loss: 1.0710 - val_acc: 0.2047\n",
      "Epoch 2/100\n",
      "152/152 [==============================] - 509s 3s/step - loss: 0.4406 - acc: 0.3463 - val_loss: 0.8844 - val_acc: 0.2345\n",
      "Epoch 3/100\n",
      "152/152 [==============================] - 497s 3s/step - loss: 0.4030 - acc: 0.3687 - val_loss: 0.8904 - val_acc: 0.2419\n",
      "Epoch 4/100\n",
      "152/152 [==============================] - 504s 3s/step - loss: 0.3729 - acc: 0.3963 - val_loss: 0.8708 - val_acc: 0.2579\n",
      "Epoch 5/100\n",
      "152/152 [==============================] - 504s 3s/step - loss: 0.3446 - acc: 0.4311 - val_loss: 0.8581 - val_acc: 0.2692\n",
      "Epoch 6/100\n",
      "152/152 [==============================] - 485s 3s/step - loss: 0.3201 - acc: 0.4572 - val_loss: 0.8647 - val_acc: 0.2776\n",
      "Epoch 7/100\n",
      "152/152 [==============================] - 481s 3s/step - loss: 0.2949 - acc: 0.4878 - val_loss: 0.8616 - val_acc: 0.2806\n",
      "Epoch 8/100\n",
      "152/152 [==============================] - 489s 3s/step - loss: 0.2717 - acc: 0.5180 - val_loss: 0.8733 - val_acc: 0.2841\n",
      "Epoch 9/100\n",
      "152/152 [==============================] - 497s 3s/step - loss: 0.2532 - acc: 0.5393 - val_loss: 0.8929 - val_acc: 0.2894\n",
      "Epoch 10/100\n",
      "152/152 [==============================] - 503s 3s/step - loss: 0.2341 - acc: 0.5664 - val_loss: 0.8891 - val_acc: 0.2932\n",
      "Epoch 11/100\n",
      "152/152 [==============================] - 505s 3s/step - loss: 0.2165 - acc: 0.5941 - val_loss: 0.8910 - val_acc: 0.2963\n",
      "Epoch 12/100\n",
      "152/152 [==============================] - 506s 3s/step - loss: 0.1946 - acc: 0.6261 - val_loss: 0.9128 - val_acc: 0.2929\n",
      "Epoch 13/100\n",
      "152/152 [==============================] - 507s 3s/step - loss: 0.1803 - acc: 0.6504 - val_loss: 0.9196 - val_acc: 0.2912\n",
      "Epoch 14/100\n",
      "152/152 [==============================] - 510s 3s/step - loss: 0.1658 - acc: 0.6786 - val_loss: 0.9359 - val_acc: 0.2951\n",
      "Epoch 15/100\n",
      "152/152 [==============================] - 369s 2s/step - loss: 0.1495 - acc: 0.7092 - val_loss: 0.9340 - val_acc: 0.2984\n",
      "Epoch 16/100\n",
      "152/152 [==============================] - 270s 2s/step - loss: 0.1352 - acc: 0.7338 - val_loss: 0.9639 - val_acc: 0.2983\n",
      "Epoch 17/100\n",
      "152/152 [==============================] - 264s 2s/step - loss: 0.1223 - acc: 0.7629 - val_loss: 0.9798 - val_acc: 0.2995\n",
      "Epoch 18/100\n",
      "152/152 [==============================] - 264s 2s/step - loss: 0.1084 - acc: 0.7933 - val_loss: 0.9909 - val_acc: 0.2979\n",
      "Epoch 19/100\n",
      "152/152 [==============================] - 265s 2s/step - loss: 0.0971 - acc: 0.8166 - val_loss: 1.0185 - val_acc: 0.2945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100\n",
      "152/152 [==============================] - 264s 2s/step - loss: 0.0860 - acc: 0.8404 - val_loss: 1.0075 - val_acc: 0.2978\n",
      "Epoch 21/100\n",
      "152/152 [==============================] - 265s 2s/step - loss: 0.0759 - acc: 0.8645 - val_loss: 1.0228 - val_acc: 0.2958\n",
      "Epoch 22/100\n",
      "152/152 [==============================] - 267s 2s/step - loss: 0.0675 - acc: 0.8822 - val_loss: 1.0336 - val_acc: 0.2985\n",
      "Epoch 23/100\n",
      "152/152 [==============================] - 272s 2s/step - loss: 0.0588 - acc: 0.9020 - val_loss: 1.0515 - val_acc: 0.2978\n",
      "Epoch 24/100\n",
      "152/152 [==============================] - 271s 2s/step - loss: 0.0517 - acc: 0.9158 - val_loss: 1.0826 - val_acc: 0.2941\n",
      "Epoch 25/100\n",
      "152/152 [==============================] - 268s 2s/step - loss: 0.0446 - acc: 0.9279 - val_loss: 1.0970 - val_acc: 0.2962\n",
      "Epoch 26/100\n",
      "152/152 [==============================] - 273s 2s/step - loss: 0.0388 - acc: 0.9397 - val_loss: 1.1090 - val_acc: 0.2929\n",
      "Epoch 27/100\n",
      "152/152 [==============================] - 266s 2s/step - loss: 0.0337 - acc: 0.9503 - val_loss: 1.1257 - val_acc: 0.2934\n",
      "Epoch 28/100\n",
      "152/152 [==============================] - 266s 2s/step - loss: 0.0294 - acc: 0.9585 - val_loss: 1.1331 - val_acc: 0.2948\n",
      "Epoch 29/100\n",
      "152/152 [==============================] - 267s 2s/step - loss: 0.0257 - acc: 0.9645 - val_loss: 1.1357 - val_acc: 0.2972\n",
      "Epoch 30/100\n",
      "152/152 [==============================] - 267s 2s/step - loss: 0.0223 - acc: 0.9702 - val_loss: 1.1711 - val_acc: 0.2940\n",
      "Epoch 31/100\n",
      "152/152 [==============================] - 268s 2s/step - loss: 0.0197 - acc: 0.9737 - val_loss: 1.1563 - val_acc: 0.2959\n",
      "Epoch 32/100\n",
      "152/152 [==============================] - 267s 2s/step - loss: 0.0169 - acc: 0.9785 - val_loss: 1.1789 - val_acc: 0.2954\n",
      "Epoch 33/100\n",
      "152/152 [==============================] - 269s 2s/step - loss: 0.0152 - acc: 0.9796 - val_loss: 1.1772 - val_acc: 0.2938\n",
      "Epoch 34/100\n",
      "152/152 [==============================] - 269s 2s/step - loss: 0.0132 - acc: 0.9822 - val_loss: 1.1959 - val_acc: 0.2959\n",
      "Epoch 35/100\n",
      "152/152 [==============================] - 268s 2s/step - loss: 0.0114 - acc: 0.9845 - val_loss: 1.2002 - val_acc: 0.2953\n",
      "Epoch 36/100\n",
      "152/152 [==============================] - 274s 2s/step - loss: 0.0103 - acc: 0.9852 - val_loss: 1.2288 - val_acc: 0.2962\n",
      "Epoch 37/100\n",
      "152/152 [==============================] - 267s 2s/step - loss: 0.0093 - acc: 0.9866 - val_loss: 1.2211 - val_acc: 0.2982\n",
      "Epoch 38/100\n",
      "152/152 [==============================] - 269s 2s/step - loss: 0.0084 - acc: 0.9877 - val_loss: 1.2343 - val_acc: 0.2962\n",
      "Epoch 39/100\n",
      "152/152 [==============================] - 270s 2s/step - loss: 0.0079 - acc: 0.9875 - val_loss: 1.2573 - val_acc: 0.2956\n",
      "Epoch 40/100\n",
      "152/152 [==============================] - 272s 2s/step - loss: 0.0069 - acc: 0.9889 - val_loss: 1.2605 - val_acc: 0.2984\n",
      "Epoch 41/100\n",
      "152/152 [==============================] - 271s 2s/step - loss: 0.0065 - acc: 0.9897 - val_loss: 1.2380 - val_acc: 0.2990\n",
      "Epoch 42/100\n",
      "152/152 [==============================] - 268s 2s/step - loss: 0.0060 - acc: 0.9897 - val_loss: 1.2743 - val_acc: 0.2984\n",
      "Epoch 43/100\n",
      "152/152 [==============================] - 269s 2s/step - loss: 0.0054 - acc: 0.9908 - val_loss: 1.2819 - val_acc: 0.2993\n",
      "Epoch 44/100\n",
      "152/152 [==============================] - 266s 2s/step - loss: 0.0052 - acc: 0.9908 - val_loss: 1.3036 - val_acc: 0.2990\n",
      "Epoch 45/100\n",
      "152/152 [==============================] - 266s 2s/step - loss: 0.0052 - acc: 0.9900 - val_loss: 1.2993 - val_acc: 0.2985\n",
      "Epoch 46/100\n",
      "152/152 [==============================] - 266s 2s/step - loss: 0.0050 - acc: 0.9901 - val_loss: 1.3113 - val_acc: 0.2981\n",
      "Epoch 47/100\n",
      "152/152 [==============================] - 270s 2s/step - loss: 0.0046 - acc: 0.9904 - val_loss: 1.3127 - val_acc: 0.2979\n",
      "Epoch 48/100\n",
      "152/152 [==============================] - 268s 2s/step - loss: 0.0044 - acc: 0.9913 - val_loss: 1.3164 - val_acc: 0.2977\n",
      "Epoch 49/100\n",
      "152/152 [==============================] - 267s 2s/step - loss: 0.0043 - acc: 0.9909 - val_loss: 1.3428 - val_acc: 0.2985\n",
      "Epoch 50/100\n",
      "152/152 [==============================] - 267s 2s/step - loss: 0.0039 - acc: 0.9918 - val_loss: 1.3372 - val_acc: 0.2988\n",
      "Epoch 51/100\n",
      "152/152 [==============================] - 272s 2s/step - loss: 0.0042 - acc: 0.9906 - val_loss: 1.3562 - val_acc: 0.2967\n",
      "Epoch 52/100\n",
      "152/152 [==============================] - 269s 2s/step - loss: 0.0040 - acc: 0.9908 - val_loss: 1.3570 - val_acc: 0.2982\n",
      "Epoch 53/100\n",
      "152/152 [==============================] - 272s 2s/step - loss: 0.0039 - acc: 0.9908 - val_loss: 1.3655 - val_acc: 0.2989\n",
      "Epoch 54/100\n",
      "152/152 [==============================] - 269s 2s/step - loss: 0.0039 - acc: 0.9907 - val_loss: 1.3677 - val_acc: 0.2988\n",
      "Epoch 55/100\n",
      "152/152 [==============================] - 267s 2s/step - loss: 0.0039 - acc: 0.9907 - val_loss: 1.3637 - val_acc: 0.3010\n",
      "Epoch 56/100\n",
      "152/152 [==============================] - 268s 2s/step - loss: 0.0036 - acc: 0.9909 - val_loss: 1.3832 - val_acc: 0.3022\n",
      "Epoch 57/100\n",
      "152/152 [==============================] - 271s 2s/step - loss: 0.0036 - acc: 0.9913 - val_loss: 1.3845 - val_acc: 0.3015\n",
      "Epoch 58/100\n",
      "152/152 [==============================] - 268s 2s/step - loss: 0.0035 - acc: 0.9913 - val_loss: 1.3662 - val_acc: 0.3017\n",
      "Epoch 59/100\n",
      "152/152 [==============================] - 267s 2s/step - loss: 0.0033 - acc: 0.9917 - val_loss: 1.3766 - val_acc: 0.3006\n",
      "Epoch 60/100\n",
      "152/152 [==============================] - 271s 2s/step - loss: 0.0033 - acc: 0.9916 - val_loss: 1.3970 - val_acc: 0.3001\n",
      "Epoch 61/100\n",
      "152/152 [==============================] - 268s 2s/step - loss: 0.0034 - acc: 0.9915 - val_loss: 1.4209 - val_acc: 0.2996\n",
      "Epoch 62/100\n",
      "152/152 [==============================] - 266s 2s/step - loss: 0.0034 - acc: 0.9911 - val_loss: 1.3947 - val_acc: 0.3025\n",
      "Epoch 63/100\n",
      "152/152 [==============================] - 269s 2s/step - loss: 0.0031 - acc: 0.9917 - val_loss: 1.4189 - val_acc: 0.2994\n",
      "Epoch 64/100\n",
      "152/152 [==============================] - 270s 2s/step - loss: 0.0029 - acc: 0.9925 - val_loss: 1.4292 - val_acc: 0.3000\n",
      "Epoch 65/100\n",
      "152/152 [==============================] - 269s 2s/step - loss: 0.0032 - acc: 0.9916 - val_loss: 1.4257 - val_acc: 0.3030\n",
      "Epoch 66/100\n",
      "152/152 [==============================] - 267s 2s/step - loss: 0.0033 - acc: 0.9912 - val_loss: 1.4036 - val_acc: 0.3027\n",
      "Epoch 67/100\n",
      "152/152 [==============================] - 273s 2s/step - loss: 0.0030 - acc: 0.9919 - val_loss: 1.4034 - val_acc: 0.3040\n",
      "Epoch 68/100\n",
      "152/152 [==============================] - 273s 2s/step - loss: 0.0031 - acc: 0.9913 - val_loss: 1.4234 - val_acc: 0.3003\n",
      "Epoch 69/100\n",
      "152/152 [==============================] - 270s 2s/step - loss: 0.0030 - acc: 0.9919 - val_loss: 1.4366 - val_acc: 0.3018\n",
      "Epoch 70/100\n",
      "152/152 [==============================] - 272s 2s/step - loss: 0.0032 - acc: 0.9911 - val_loss: 1.4380 - val_acc: 0.3011\n",
      "Epoch 71/100\n",
      "152/152 [==============================] - 276s 2s/step - loss: 0.0030 - acc: 0.9918 - val_loss: 1.4306 - val_acc: 0.3010\n",
      "Epoch 72/100\n",
      "152/152 [==============================] - 268s 2s/step - loss: 0.0029 - acc: 0.9924 - val_loss: 1.4631 - val_acc: 0.2995\n",
      "Epoch 73/100\n",
      "152/152 [==============================] - 267s 2s/step - loss: 0.0029 - acc: 0.9926 - val_loss: 1.4646 - val_acc: 0.3008\n",
      "Epoch 74/100\n",
      "152/152 [==============================] - 271s 2s/step - loss: 0.0031 - acc: 0.9908 - val_loss: 1.4802 - val_acc: 0.2996\n",
      "Epoch 75/100\n",
      "152/152 [==============================] - 270s 2s/step - loss: 0.0027 - acc: 0.9923 - val_loss: 1.4620 - val_acc: 0.3004\n",
      "Epoch 76/100\n",
      "152/152 [==============================] - 269s 2s/step - loss: 0.0028 - acc: 0.9917 - val_loss: 1.4489 - val_acc: 0.3005\n",
      "Epoch 77/100\n",
      "152/152 [==============================] - 271s 2s/step - loss: 0.0029 - acc: 0.9915 - val_loss: 1.4690 - val_acc: 0.2993\n",
      "Epoch 78/100\n",
      "152/152 [==============================] - 270s 2s/step - loss: 0.0027 - acc: 0.9922 - val_loss: 1.4557 - val_acc: 0.3020\n",
      "Epoch 79/100\n",
      "152/152 [==============================] - 270s 2s/step - loss: 0.0027 - acc: 0.9921 - val_loss: 1.4761 - val_acc: 0.3022\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152/152 [==============================] - 274s 2s/step - loss: 0.0030 - acc: 0.9913 - val_loss: 1.4902 - val_acc: 0.3018\n",
      "Epoch 81/100\n",
      "152/152 [==============================] - 270s 2s/step - loss: 0.0028 - acc: 0.9915 - val_loss: 1.4648 - val_acc: 0.3014\n",
      "Epoch 82/100\n",
      "152/152 [==============================] - 271s 2s/step - loss: 0.0027 - acc: 0.9916 - val_loss: 1.4807 - val_acc: 0.3011\n",
      "Epoch 83/100\n",
      "152/152 [==============================] - 272s 2s/step - loss: 0.0028 - acc: 0.9918 - val_loss: 1.4834 - val_acc: 0.3023\n",
      "Epoch 84/100\n",
      "152/152 [==============================] - 274s 2s/step - loss: 0.0027 - acc: 0.9920 - val_loss: 1.5056 - val_acc: 0.3014\n",
      "Epoch 85/100\n",
      "152/152 [==============================] - 272s 2s/step - loss: 0.0027 - acc: 0.9915 - val_loss: 1.5084 - val_acc: 0.3032\n",
      "Epoch 86/100\n",
      "152/152 [==============================] - 272s 2s/step - loss: 0.0027 - acc: 0.9915 - val_loss: 1.5257 - val_acc: 0.3020\n",
      "Epoch 87/100\n",
      "152/152 [==============================] - 272s 2s/step - loss: 0.0027 - acc: 0.9915 - val_loss: 1.5007 - val_acc: 0.3027\n",
      "Epoch 88/100\n",
      "152/152 [==============================] - 273s 2s/step - loss: 0.0028 - acc: 0.9917 - val_loss: 1.4990 - val_acc: 0.3022\n",
      "Epoch 89/100\n",
      "152/152 [==============================] - 272s 2s/step - loss: 0.0027 - acc: 0.9917 - val_loss: 1.5039 - val_acc: 0.3036\n",
      "Epoch 90/100\n",
      "152/152 [==============================] - 276s 2s/step - loss: 0.0026 - acc: 0.9922 - val_loss: 1.4986 - val_acc: 0.3007\n",
      "Epoch 91/100\n",
      "152/152 [==============================] - 271s 2s/step - loss: 0.0026 - acc: 0.9919 - val_loss: 1.5132 - val_acc: 0.3027\n",
      "Epoch 92/100\n",
      "152/152 [==============================] - 268s 2s/step - loss: 0.0027 - acc: 0.9917 - val_loss: 1.5211 - val_acc: 0.3015\n",
      "Epoch 93/100\n",
      "152/152 [==============================] - 273s 2s/step - loss: 0.0029 - acc: 0.9908 - val_loss: 1.4984 - val_acc: 0.3050\n",
      "Epoch 94/100\n",
      "152/152 [==============================] - 274s 2s/step - loss: 0.0028 - acc: 0.9909 - val_loss: 1.5103 - val_acc: 0.3022\n",
      "Epoch 95/100\n",
      "152/152 [==============================] - 271s 2s/step - loss: 0.0027 - acc: 0.9919 - val_loss: 1.5064 - val_acc: 0.3036\n",
      "Epoch 96/100\n",
      "152/152 [==============================] - 271s 2s/step - loss: 0.0025 - acc: 0.9921 - val_loss: 1.5054 - val_acc: 0.3024\n",
      "Epoch 97/100\n",
      "152/152 [==============================] - 267s 2s/step - loss: 0.0025 - acc: 0.9921 - val_loss: 1.5008 - val_acc: 0.3014\n",
      "Epoch 98/100\n",
      "152/152 [==============================] - 271s 2s/step - loss: 0.0027 - acc: 0.9913 - val_loss: 1.5088 - val_acc: 0.3054\n",
      "Epoch 99/100\n",
      "152/152 [==============================] - 269s 2s/step - loss: 0.0024 - acc: 0.9919 - val_loss: 1.4892 - val_acc: 0.3041\n",
      "Epoch 100/100\n",
      "152/152 [==============================] - 272s 2s/step - loss: 0.0027 - acc: 0.9915 - val_loss: 1.5373 - val_acc: 0.3014\n"
     ]
    }
   ],
   "source": [
    "combined_model, encoder_model, decoder_model = model_seq_to_seq(batch_size=64, epochs=100, mask_zero=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, sep = ' '):\n",
    "    # to obtain the encoder model\n",
    "    # 1- Encode the input to obtain the context vector from the encoder.\n",
    "    encoder_outputs, h, c = encoder_model.predict(input_seq)\n",
    "    states_value = [h,c]\n",
    "    \n",
    "    # Generate empty target sequence of length 1 like this [[0]]\n",
    "    target_seq = np.zeros((1,1))\n",
    "    \n",
    "    # Populate the first character of target sequence with the start character to be like [[5]]\n",
    "    target_seq[0, 0] = target_token_index[st_tok]\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    \n",
    "    \n",
    "    # if you don't find end token\n",
    "    while not stop_condition:\n",
    "        attention_density = []\n",
    "        #feed the predict() with the input to the decoder model which is target_seq and\n",
    "        #the context vector from the encode which is states_value\n",
    "        # output_tokens will hold the last output of the LSTM\n",
    "        \n",
    "        output_tokens, attention, h, c  = decoder_model.predict([target_seq, encoder_outputs] + states_value)\n",
    "        attention_density.append(attention[0][0])# attention is max_sent_len x 1 since we have num_time_steps = 1 for the output        \n",
    "\n",
    "        # Sample a token \n",
    "        # It returns the index of the maximum item in the 1st array last row\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        # return the word as letters from its index\n",
    "        sampled_tok = reverse_target_tok_index[sampled_token_index]\n",
    "        # form the sentence which consists of words and separatoe\n",
    "        decoded_sentence += sep + sampled_tok\n",
    "\n",
    "        # Exit condition: either hit max length which is 64\n",
    "        # or find stop character.\n",
    "        if (sampled_tok == end_tok or len(decoded_sentence) > 64):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1) with the index of the output token.\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states to be fed to the prediction\n",
    "        states_value = [h, c]\n",
    "        \n",
    "        attention_density = np.array(attention_density)\n",
    "    return decoded_sentence, attention_density\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: 0    مرحبا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  hi _END\n",
      "-\n",
      "Input sentence: 1    اركض\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  run _END\n",
      "-\n",
      "Input sentence: 2    اخفض رأسك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  duck _END\n",
      "-\n",
      "Input sentence: 3    اخفضي رأسك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  duck _END\n",
      "-\n",
      "Input sentence: 4    اخفضوا رؤوسكم\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  duck _END\n",
      "-\n",
      "Input sentence: 5    النجده\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  help _END\n",
      "-\n",
      "Input sentence: 6    اقفز\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  jump _END\n",
      "-\n",
      "Input sentence: 7    قف\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  stand up _END\n",
      "-\n",
      "Input sentence: 8    توقف \n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  stop _END\n",
      "-\n",
      "Input sentence: 9    إنتظر\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  wait _END\n",
      "-\n",
      "Input sentence: 10    داوم\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  go on _END\n",
      "-\n",
      "Input sentence: 11    استمر\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  go on _END\n",
      "-\n",
      "Input sentence: 12    مرحبا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  hi _END\n",
      "-\n",
      "Input sentence: 13    تعجل\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  hurry _END\n",
      "-\n",
      "Input sentence: 14    استعجل\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  hurry _END\n",
      "-\n",
      "Input sentence: 15    انا اري\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i see _END\n",
      "-\n",
      "Input sentence: 16    أنا فزت\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i won _END\n",
      "-\n",
      "Input sentence: 17    استرح\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  have a seat _END\n",
      "-\n",
      "Input sentence: 18    ابتسم\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  smile _END\n",
      "-\n",
      "Input sentence: 19    في صحتك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  cheers _END\n",
      "-\n",
      "Input sentence: 20    هل فهمت\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  got it _END\n",
      "-\n",
      "Input sentence: 21    ركض\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he ran _END\n",
      "-\n",
      "Input sentence: 22    أعرف\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i know _END\n",
      "-\n",
      "Input sentence: 23    أعلم ذلك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i know _END\n",
      "-\n",
      "Input sentence: 24    أنا أعلم\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i know _END\n",
      "-\n",
      "Input sentence: 25    أنا في \n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im  _END\n",
      "-\n",
      "Input sentence: 26    أنا بخير\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im ok _END\n",
      "-\n",
      "Input sentence: 27    استمع\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  listen _END\n",
      "-\n",
      "Input sentence: 28    غير معقول\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  no way _END\n",
      "-\n",
      "Input sentence: 29    حقا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  really _END\n",
      "-\n",
      "Input sentence: 30    شكرا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  thanks _END\n",
      "-\n",
      "Input sentence: 31    شكرا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  thanks _END\n",
      "-\n",
      "Input sentence: 32    لماذا أنا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  why me _END\n",
      "-\n",
      "Input sentence: 33    رائع\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  awesome _END\n",
      "-\n",
      "Input sentence: 34    خذ راحتك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  make yourself at home _END\n",
      "-\n",
      "Input sentence: 35    أغرب عن وجهي\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  beat it _END\n",
      "-\n",
      "Input sentence: 36    هاتفني\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  call me _END\n",
      "-\n",
      "Input sentence: 37    اتصل بي\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  call me _END\n",
      "-\n",
      "Input sentence: 38    تفضل بالدخول\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  come inside _END\n",
      "-\n",
      "Input sentence: 39    تعال إلى الداخل\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  come in _END\n",
      "-\n",
      "Input sentence: 40    بالله عليك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  come on _END\n",
      "-\n",
      "Input sentence: 41    هيا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  come on _END\n",
      "-\n",
      "Input sentence: 42    هيا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  come on _END\n",
      "-\n",
      "Input sentence: 43    اخرج من هنا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  get out _END\n",
      "-\n",
      "Input sentence: 44    أخرج\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  get out _END\n",
      "-\n",
      "Input sentence: 45    اخرج\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  get out _END\n",
      "-\n",
      "Input sentence: 46    اتركني و شأني\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  leave me alone _END\n",
      "-\n",
      "Input sentence: 47    اذهب بعيدا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  go away _END\n",
      "-\n",
      "Input sentence: 48    ارحل\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  go away _END\n",
      "-\n",
      "Input sentence: 49    مع السلامه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  take care _END\n",
      "-\n",
      "Input sentence: 50    إلى اللقاء\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  see you again _END\n",
      "-\n",
      "Input sentence: 51    إنتظر\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  wait _END\n",
      "-\n",
      "Input sentence: 52    لقد أتى\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he came _END\n",
      "-\n",
      "Input sentence: 53    هو يجري\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he runs _END\n",
      "-\n",
      "Input sentence: 54    ساعدني\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  help me _END\n",
      "-\n",
      "Input sentence: 55    النجده ساعدني\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  help me _END\n",
      "-\n",
      "Input sentence: 56    ساعدوني\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  help me _END\n",
      "-\n",
      "Input sentence: 57    انتظر\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  wait _END\n",
      "-\n",
      "Input sentence: 58    أنا موافق\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i agree _END\n",
      "-\n",
      "Input sentence: 59    أنا حزين\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im sad _END\n",
      "-\n",
      "Input sentence: 60    أنا أيضا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  me too _END\n",
      "-\n",
      "Input sentence: 61    اخرس\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  shut up _END\n",
      "-\n",
      "Input sentence: 62    اصمت\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  shut up _END\n",
      "-\n",
      "Input sentence: 63    اسكت\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  shut up _END\n",
      "-\n",
      "Input sentence: 64    أغلق فمك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  shut up _END\n",
      "-\n",
      "Input sentence: 65    أوقفه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  stop it _END\n",
      "-\n",
      "Input sentence: 66    خذه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  take it _END\n",
      "-\n",
      "Input sentence: 67    أخبرني\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  tell me _END\n",
      "-\n",
      "Input sentence: 68    توم فاز\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  tom won _END\n",
      "-\n",
      "Input sentence: 69    لقد ربح توم\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  tom won _END\n",
      "-\n",
      "Input sentence: 70    استيقظ\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  wake up _END\n",
      "-\n",
      "Input sentence: 71    أهلا و سهلا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  welcome _END\n",
      "-\n",
      "Input sentence: 72    مرحبا بك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  welcome _END\n",
      "-\n",
      "Input sentence: 73    اهلا وسهلا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  welcome _END\n",
      "-\n",
      "Input sentence: 74    مرحبا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  hi _END\n",
      "-\n",
      "Input sentence: 75    من فاز\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  who won _END\n",
      "-\n",
      "Input sentence: 76    من الذي ربح\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  who won _END\n",
      "-\n",
      "Input sentence: 77    لم لا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  why not _END\n",
      "-\n",
      "Input sentence: 78    لما لا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  why not _END\n",
      "-\n",
      "Input sentence: 79    لا فكره لدي\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  beats me _END\n",
      "-\n",
      "Input sentence: 80    استمتع بوقتك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  have fun _END\n",
      "-\n",
      "Input sentence: 81    أسرعا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  hurry up _END\n",
      "-\n",
      "Input sentence: 82    لقد نسيت\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i forgot _END\n",
      "-\n",
      "Input sentence: 83    فهمته\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i got it _END\n",
      "-\n",
      "Input sentence: 84    فهمتها\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i got it _END\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: 85    فهمت ذلك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i got it _END\n",
      "-\n",
      "Input sentence: 86    أستخدمه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im using it _END\n",
      "-\n",
      "Input sentence: 87    سأدفع أنا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  ill pay _END\n",
      "-\n",
      "Input sentence: 88    أنا مشغول\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im busy _END\n",
      "-\n",
      "Input sentence: 89    إنني مشغول\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im busy _END\n",
      "-\n",
      "Input sentence: 90    أشعر بالبرد\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im freezing _END\n",
      "-\n",
      "Input sentence: 91    أنا حر\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im free _END\n",
      "-\n",
      "Input sentence: 92    أنا هنا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im here _END\n",
      "-\n",
      "Input sentence: 93    لقد عدت إلى البيت\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im home _END\n",
      "-\n",
      "Input sentence: 94    أنا فقير\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im poor _END\n",
      "-\n",
      "Input sentence: 95    أنا ثري\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im rich _END\n",
      "-\n",
      "Input sentence: 96    هذا مؤلم\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  it hurts _END\n",
      "-\n",
      "Input sentence: 97    انها جافه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  its dry _END\n",
      "-\n",
      "Input sentence: 98    الجو حار\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  its hot _END\n",
      "-\n",
      "Input sentence: 99    إنه جديد\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  its new _END\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(100): #[14077,20122,40035,40064, 40056, 40068, 40090, 40095, 40100, 40119, 40131, 40136, 40150, 40153]:\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence, attention = decode_sequence(input_seq, sep = ' ')\n",
    "    print('-')\n",
    "    print('Input sentence:', dataset.Arabic_input[seq_index: seq_index + 1])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: 9700    الحب و الكره شعوران متعاكسان\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  love and hate are opposite emotions _END\n",
      "-\n",
      "Input sentence: 9701    ماتت عمتي منذ سنتين\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  my aunt has been dead for two years _END\n",
      "-\n",
      "Input sentence: 9702    أعجبت عمتي بنجاحي\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  my aunt was pleased with my success _END\n",
      "-\n",
      "Input sentence: 9703    فرحت عمتي بنجاحي\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  my aunt was pleased with my success _END\n",
      "-\n",
      "Input sentence: 9704    مصروفات الدراسه بكليتي غاليه جدا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  my college tuition is too expensive _END\n",
      "-\n",
      "Input sentence: 9705    الدراسه بكليتي مكلفه جدا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  my college tuition is too expensive _END\n",
      "-\n",
      "Input sentence: 9706    سيأتي أولاد عمي بعد بضعه أيام\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  my cousins are coming in a few days _END\n",
      "-\n",
      "Input sentence: 9707    أريد أن أكون رجل إطفاء\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  my dream is to become a firefighter _END\n",
      "-\n",
      "Input sentence: 9708    توفي أبي قبل عامين\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  my father passed away two years ago _END\n",
      "-\n",
      "Input sentence: 9709    هدفي في الحياه هو أن أصبح كاتبا روائيا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  my goal in life is to be a novelist _END\n",
      "-\n",
      "Input sentence: 9710    افتح فمك وأغلق عينيك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  open your mouth and close your eyes _END\n",
      "-\n",
      "Input sentence: 9711    من فضلك مرر إلي الملح و الفلفل\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  pass me the salt and pepper please _END\n",
      "-\n",
      "Input sentence: 9712    لا طعام لمن لا يعمل\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  people who dont work wont be fed _END\n",
      "-\n",
      "Input sentence: 9713    يرجى أن تقفل الباب عند مغادرتك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  please lock the door when you leave _END\n",
      "-\n",
      "Input sentence: 9714    أعد الكتاب في أقرب فرصه ممكنه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  return this book as soon as you can _END\n",
      "-\n",
      "Input sentence: 9715    نصحته ألا يأكل كثيرا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  she advised him not to eat too much _END\n",
      "-\n",
      "Input sentence: 9716    أتتني وسألتني عن اسمي\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  she came up to me and asked my name _END\n",
      "-\n",
      "Input sentence: 9717    أحبت شابا أصغر منها سنا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  she fell in love with a younger man _END\n",
      "-\n",
      "Input sentence: 9718    لديها ابنه تعزف على البيانو\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  she has a daughter who is a pianist _END\n",
      "-\n",
      "Input sentence: 9719    إنها لا تعرف شيئا عن عائلتك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  she knows nothing about your family _END\n",
      "-\n",
      "Input sentence: 9720    إنها ستتزوج العام المقبل\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  shell be getting married next year _END\n",
      "-\n",
      "Input sentence: 9721    استغل كل فرصه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  take advantage of every opportunity _END\n",
      "-\n",
      "Input sentence: 9722    هذا ما حدث هنا يوم الإثنين\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  thats what happened here on monday _END\n",
      "-\n",
      "Input sentence: 9723    وقع الحادث قبل ساعتين\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  the accident happened two hours ago _END\n",
      "-\n",
      "Input sentence: 9724    بالواقع إن الإجابه بسيطه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  the answer is actually quite simple _END\n",
      "-\n",
      "Input sentence: 9725    الرقم الذري للهيدروجين هو واحد\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  the atomic number for hydrogen is  _END\n",
      "-\n",
      "Input sentence: 9726    إقتربت القطه من الفأر بحذر\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  the letter is filled with you _END\n",
      "-\n",
      "Input sentence: 9727    إقتربت القطه ببطأ من الفأر\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  the clock is filled with children _END\n",
      "-\n",
      "Input sentence: 9728    قصفت طائرات العدو المدينه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i got the song at  _END\n",
      "-\n",
      "Input sentence: 9729    التنين حيوان خرافي\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  a mouse is a animal _END\n",
      "-\n",
      "Input sentence: 9730    توقف الحصان و رفض أن يتحرك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  keep out from and am out _END\n",
      "-\n",
      "Input sentence: 9731    أقلعت الطائرات النفاثه واحده تلو الأخرى\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i left a book about the way _END\n",
      "-\n",
      "Input sentence: 9732    عمر السيده أربعون عاما على الأرجح\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i still run all over _END\n",
      "-\n",
      "Input sentence: 9733    وقع الرجل العجوز على الأرض\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  the floor is out of the floor _END\n",
      "-\n",
      "Input sentence: 9734    الاطار الخلفي لعجلتي ضارب\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  the washbasin needs inside _END\n",
      "-\n",
      "Input sentence: 9735    لم يكن المخزن كبيرا أليس كذلك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he didnt allow the job about you _END\n",
      "-\n",
      "Input sentence: 9736    هناك ستون ثانيه في الدقيقه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i had a pain in the garden _END\n",
      "-\n",
      "Input sentence: 9737    في الدقيقه ستون ثانيه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he put up in going in alone _END\n",
      "-\n",
      "Input sentence: 9738    لا يبدو بأن هناك أي مشكله\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i dont eat here be all _END\n",
      "-\n",
      "Input sentence: 9739    لا توجد إجابه لسؤالك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  dont forget what what you did your _END\n",
      "-\n",
      "Input sentence: 9740    لا يوجد شك بخصوص إخلاصه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he couldnt understand the his answer _END\n",
      "-\n",
      "Input sentence: 9741    كانت الغرفه خاليه من الأثاث\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  there were a basket correct _END\n",
      "-\n",
      "Input sentence: 9742    فحصوا مدى نقاوه الماء\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  your water is water _END\n",
      "-\n",
      "Input sentence: 9743    كان يتآمرون ضد الحكومه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  they was very busy _END\n",
      "-\n",
      "Input sentence: 9744    لقد عوقبوا على جرائمهم\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  they ran be one _END\n",
      "-\n",
      "Input sentence: 9745    لقد تم معاقبتهم على جرائمهم\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  it was their left for another _END\n",
      "-\n",
      "Input sentence: 9746    بني هذا الجسر منذ عامين\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  this man has an one ago _END\n",
      "-\n",
      "Input sentence: 9747    هذا أحد كتب توم المفضله\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  that is plane a best around _END\n",
      "-\n",
      "Input sentence: 9748    هنا موقع الحادث\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he worked arrived yesterday _END\n",
      "-\n",
      "Input sentence: 9749    هذه القصه مستمده من قصه واقعيه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  this girl has a want this has got _END\n",
      "-\n",
      "Input sentence: 9750    تتألف هذه النظريه من ثلاثه أجزاء\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  this word is not from one _END\n",
      "-\n",
      "Input sentence: 9751    عثرت على ثلاث حثث في الكهف\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  she has a few school _END\n",
      "-\n",
      "Input sentence: 9752    كنت انا وتوم في بوسطن في شهر أكتوبر\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  tom met mary in  _END\n",
      "-\n",
      "Input sentence: 9753    لا يستطيع توم رؤيه ماري من مكانه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  tom cant wait against his own _END\n",
      "-\n",
      "Input sentence: 9754    توم لا يعرف كيفيه حلب الماعز\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  tom has never know it _END\n",
      "-\n",
      "Input sentence: 9755    أعطى توم الشرطه عنوانا خاطئا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  tom gave me of the can your your well _END\n",
      "-\n",
      "Input sentence: 9756    توم القى نظره سريعه في مرآه الرؤيه الخلفيه \n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  tom is just thinking of the station _END\n",
      "-\n",
      "Input sentence: 9757    يتحدث توم اليابانيه بطلاقه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  tom speaks quickly to the hands _END\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: 9758    توم في الخارج يسقي الأزهار\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  tom has run at the airport _END\n",
      "-\n",
      "Input sentence: 9759    توم هو فقط من يمكنه فعل هذا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  tom is mary but mary isnt _END\n",
      "-\n",
      "Input sentence: 9760    توم فخور جدا بماري أليس كذلك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  tom is pretty but but isnt isnt _END\n",
      "-\n",
      "Input sentence: 9761    ليس توم هو الشخص المناسب للوظيفه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  tom isnt what the sports _END\n",
      "-\n",
      "Input sentence: 9762    توم يحب ماري ولكنها تحب جون\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  tom and mary mary smiles _END\n",
      "-\n",
      "Input sentence: 9763    وعدني توم بأنه سيكون هنا قبل \n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  tom promised to be here at  _END\n",
      "-\n",
      "Input sentence: 9764    توم أخبرني بأن لا أدخل المغاره\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  tom told me to come abroad _END\n",
      "-\n",
      "Input sentence: 9765    اثنان ضد واحد ليس صراعا عادلا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  everyone happens not a friend _END\n",
      "-\n",
      "Input sentence: 9766    تبع المشتبه محققان\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  the good kept up _END\n",
      "-\n",
      "Input sentence: 9767    انتبه هناك حفره كبيره هناك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  look there theres a give me _END\n",
      "-\n",
      "Input sentence: 9768    نحن ذاهبون للتحقق من ذلك علي الفور\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  were just the tip of you _END\n",
      "-\n",
      "Input sentence: 9769    لا نستطيع النوم بسبب الضجيج\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  tom cant coffee to coffee _END\n",
      "-\n",
      "Input sentence: 9770    لا يمكننا النوم بسبب الإزعاج\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  you cant all right but _END\n",
      "-\n",
      "Input sentence: 9771    نحن لا نستطيع النوم بسبب الضجه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  we dont need to come today _END\n",
      "-\n",
      "Input sentence: 9772    لم نرى ما كنا نريد أن نراه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  why are you not to do that _END\n",
      "-\n",
      "Input sentence: 9773    نصنع الزبده و الجبنه من الحليب\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  she job in a lot of carp _END\n",
      "-\n",
      "Input sentence: 9774    كنا نسبح في النهر\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  we were in the office in a park _END\n",
      "-\n",
      "Input sentence: 9775    حصل أغرب من ذلك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  it isnt easy _END\n",
      "-\n",
      "Input sentence: 9776    ماذا يفعلون الناس من اجل المتعه في بوسطن\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  what do you major to many meat _END\n",
      "-\n",
      "Input sentence: 9777    ما اسم تلك السمكه بالإنجليزيه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  what do you do with those english _END\n",
      "-\n",
      "Input sentence: 9778    حين أكبر أريد أن أصبح ملكا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  theres better i understand i use _END\n",
      "-\n",
      "Input sentence: 9779    متى بدأت دراسه اللغه الانجليزيه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  when did you begin playing english _END\n",
      "-\n",
      "Input sentence: 9780    متى تنوي السفر إلى اليابان\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  when did you come to japan _END\n",
      "-\n",
      "Input sentence: 9781    لم لم تقل ذلك مسبقا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  why dont you do that by your day _END\n",
      "-\n",
      "Input sentence: 9782    لم لا يقول الناس ما يعنونه حقا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  why did you really forget so _END\n",
      "-\n",
      "Input sentence: 9783    لم لا نتفق على ألا نتفق فحسب\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  why dont we do not no favor _END\n",
      "-\n",
      "Input sentence: 9784    أتسمح لي بالعزف على البيانو\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  may i play the piano _END\n",
      "-\n",
      "Input sentence: 9785    هل تريديني أن أمشط شعرك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  do you want me to comb your hair _END\n",
      "-\n",
      "Input sentence: 9786    هل تفضل التحدث بالفرنسيه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  do you have a lot to come _END\n",
      "-\n",
      "Input sentence: 9787    يمكنك الذهاب إلى المحطه بالحافله\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  you can go to the station by bus _END\n",
      "-\n",
      "Input sentence: 9788    بإمكانك الوصول إلى المحطه عن طريق الحافله\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  you can go to the station by bus _END\n",
      "-\n",
      "Input sentence: 9789    لا يبدو أنك تهتم بما يحدث\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  dont dont tell you something _END\n",
      "-\n",
      "Input sentence: 9790    ربما قرأت هذا الكتاب أصلا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  this book is book for this book _END\n",
      "-\n",
      "Input sentence: 9791    كأن عليك أن تأتي إلى هنا مبكرا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  i warned you not to come here _END\n",
      "-\n",
      "Input sentence: 9792    عليك أن تعترف بفشلك\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  you must judge for yourself _END\n",
      "-\n",
      "Input sentence: 9793    كان عليك قبول نصيحته\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  you shouldve have told him _END\n",
      "-\n",
      "Input sentence: 9794    كان ينبغي أن تكمله منذ وقت طويل\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  he was his for school _END\n",
      "-\n",
      "Input sentence: 9795    لا تحتاج لفعل ذلك مجددا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  dont forget what to forget it _END\n",
      "-\n",
      "Input sentence: 9796    أنت إما معنا أو ضدنا\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  youre all our visit we _END\n",
      "-\n",
      "Input sentence: 9797    الغريق يتعلق بقشه\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  a mouse is a timid creature _END\n",
      "-\n",
      "Input sentence: 9798    غزى الكثير من السياح الجزيره\n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  a floor is filled with this this _END\n",
      "-\n",
      "Input sentence: 9799    اجتاح العديد من السياح الجزيره \n",
      "Name: Arabic_input, dtype: object\n",
      "Decoded sentence:  im new going is out _END\n"
     ]
    }
   ],
   "source": [
    "decoded_sentence_ls=[]\n",
    "for seq_index in range(9700, 9800): #[14077,20122,40035,40064, 40056, 40068, 40090, 40095, 40100, 40119, 40131, 40136, 40150, 40153]:\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence, attention = decode_sequence(input_seq, sep = ' ')\n",
    "    print('-')\n",
    "    print('Input sentence:', dataset.Arabic_input[seq_index: seq_index + 1])\n",
    "    print('Decoded sentence:', decoded_sentence)\n",
    "    decoded_sentence_ls.append(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
